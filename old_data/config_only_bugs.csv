repo_name,issue_number,issue_title,issue_body,issue_created_at,issue_closed_at,issue_comments_count,issue_url,pr_number,pr_merged_at,pr_url,config_files_changed,app_code_files_changed,other_files_changed,total_files_changed,lines_added,lines_deleted,config_files_lines_changed,app_code_files_lines_changed,resolution_time_hours,labels,has_config_changes,has_code_changes,bug_severity,bug_type,changed_files,services_affected,is_cross_service_bug
GoogleCloudPlatform/microservices-demo,2873,secCompProfile without securityContext enabled leads to errors,"### Describe the bug  when .Values.securityContext is set to 'false' and .Values.seccompProfile.type is set the yaml key ""securityContext"" is not set in the YAML file for all services  ### To Reproduce  1. set .Values.securityContext.enable to 'false' and .Values.seccompProfile.type 2. deploy application| use helm template or something similar 3. see wrong YAML setup  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  persistent issue ",2025-01-20T20:09:02+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2873,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,217.9808333333333,type: bug;priority: p2,True,False,normal,configuration,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
GoogleCloudPlatform/microservices-demo,2872,securityContext opt-in not working for paymentservice + opentelemetry collector,### Describe the bug  when setting .Values.securityContext.enable to 'false' both deployments mentioned above would still try to set uid + gid to 1000 and don't acknowledge the variable passed in the values-file   ### To Reproduce  1. set Values.securityContext.enable to 'false' 2. deploy application| use helm template or sth. similar 3. see wrong uid/gid in paymentservice + opentelemetry-collector pod  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  it appears to be persistent ,2025-01-20T20:06:25+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2872,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,218.02444444444444,type: bug;priority: p2,True,False,normal,security,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
aws-samples/aws-microservices-deploy-options,230,Health checks are failing Fargate deployment pipeline,https://github.com/aws-samples/aws-microservices-deploy-options#deployment-pipeline-fargate-with-aws-codepipeline creates all the CloudFormation templates. But accessing the `ServiceUrl` shows that the service is inaccessible.  Health checks are failing.  @jicowan is investigating,2018-04-14T02:11:47+00:00,2018-04-16T04:04:28+00:00,5,https://github.com/aws-samples/aws-microservices-deploy-options/issues/230,244.0,2018-04-16T04:03:48+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/244,1,0,0,1,1,1,2,0,49.86694444444444,,True,False,normal,security,"[{""filename"": ""apps/ecs/deployment/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
GoogleCloudPlatform/microservices-demo,2677,"GKE v1.29.7-gke.1174000: rpc error: code = Unavailable desc = connection error around ""could not retrieve currencies""","### Describe the bug  On GKE version `v1.29.7-gke.1174000`,  I got an issue while deploying the online-boutique demo with the log below.  ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ### To Reproduce  <!-- Steps to reproduce the behavior: --> Ran the command: ``` kubectl create ns microservices-demo kubectl apply -n microservices-demo -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml ``` Waited for the workloads to  be ready but the load generator pod was crashing due to the `frontend-check` failing to get a successful response. ``` + echo 'Attempt 11: Pinging frontend: frontend:80...' Attempt 11: Pinging frontend: frontend:80... + wget --server-response [http://frontend:80](http://frontend/) + awk '/^  HTTP/{print $2}' Error: Could not reach frontend - Status code: 500 ```  ``` $ k get pods -n microservices-demo NAME                                     READY   STATUS     RESTARTS         AGE adservice-7d7dd69f68-qx4gc               1/1     Running    0                55m cartservice-777f69bb65-rk7xv             1/1     Running    0                55m checkoutservice-bcbf96994-hjntp          1/1     Running    0                55m currencyservice-8557bb8fb-lqrr4          1/1     Running    0                55m emailservice-57cc9b487b-sctq4            1/1     Running    0                55m frontend-6d9488c857-shz52                1/1     Running    0                55m loadgenerator-856dc76c97-m6bhs           0/1     Init:0/1   11 (6m57s ago)   55m paymentservice-759576897b-44dw8          1/1     Running    0                55m productcatalogservice-7b7d78cdbb-n4tsz   1/1     Running    0                55m recommendationservice-769cb665c4-wtjds   1/1     Running    0                55m redis-cart-bf5c68f69-tvvq9               1/1     Running    0                55m shippingservice-5d97bddcdd-zvwbr         1/1     Running    0                55m ```  ### Logs   ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ``` $ k logs loadgenerator-7995465594-k7nph -c frontend-check -n microservices-demo + MAX_RETRIES=12 + RETRY_INTERVAL=10 + seq 1 12 + echo 'Attempt 1: Pinging frontend: frontend:80...' Attempt 1: Pinging frontend: frontend:80... + awk '/^  HTTP/{print $2}' + wget --server-response http://frontend:80 + STATUSCODE=500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' + sleep 10 ... + echo 'Attempt 9: Pinging frontend: frontend:80...' Attempt 9: Pinging frontend: frontend:80... + + awk '/^  HTTP/{print $2}' wget --server-response http://frontend:80 + STATUSCODE=500 Error: Could not reach frontend - Status code: 500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' ```  ### Screenshots  ![online-boutique-gke](https://github.com/user-attachments/assets/b1515906-62be-4bad-9c9c-ce6dc4e0fd24)  ### Environment  GKE version `v1.29.7-gke.1174000`  ### Additional context  <!-- Add any other context about the problem here -->  ### Exposure  <!-- Is the bug intermittent, persistent? Is it widespread, local? --> ",2024-08-13T19:53:42+00:00,2024-08-14T21:36:31+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2677,2429.0,2024-03-18T01:50:38+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2429,4,0,0,4,44,21,65,0,-3570.051111111111,,True,False,critical,configuration,"[{""filename"": ""src/currencyservice/package-lock.json"", ""lines_added"": 16, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""src/currencyservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package-lock.json"", ""lines_added"": 26, ""lines_deleted"": 12, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",src,False
aws-samples/aws-microservices-deploy-options,146,Lambda functions are failing on AWS,"Invoking the Lambda function on AWS is giving the following error:  ``` lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='GreetingApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""}lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='NamesApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""} ``` ",2018-03-30T21:14:38+00:00,2018-03-31T00:10:05+00:00,1,https://github.com/aws-samples/aws-microservices-deploy-options/issues/146,152.0,2018-03-31T00:10:05+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/152,4,0,0,4,82,140,222,0,2.924166666666667,lambda,True,False,normal,database,"[{""filename"": ""services/greeting/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 39, ""file_type"": ""config""}, {""filename"": ""services/name/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 45, ""file_type"": ""config""}, {""filename"": ""services/pom.xml"", ""lines_added"": 82, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""services/webapp/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 49, ""file_type"": ""config""}]",services,False
GoogleCloudPlatform/microservices-demo,2511,Adservice crashing with segfault,"### Describe the bug  Unable to build / deploy on Windows11  ### To Reproduce  ``` git clone https://github.com/GoogleCloudPlatform/microservices-demo cd microservices-demo/ minikube start --cpus=4 --memory 4096 --disk-size 32g skaffold run ``` ### Logs   ``` Waiting for deployments to stabilize...  - deployment/checkoutservice is ready. [10/11 deployment(s) still pending]  - deployment/adservice: container server terminated with exit code 139     - pod/adservice-7cccc9b6fc-m7dvh: container server terminated with exit code 139       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Can not save log file, dump to screen..       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # ...       > [adservice-7cccc9b6fc-m7dvh server] /sys/kernel/mm/transparent_hugepage/defrag (defrag/compaction efforts parameter): always defer defer+madvise [madvise] never       > [adservice-7cccc9b6fc-m7dvh server] Process Memory:       > [adservice-7cccc9b6fc-m7dvh server] Virtual Size: 146444K (peak: 146444K)       > [adservice-7cccc9b6fc-m7dvh server] Resident Set Size: 22436K (peak: 22436K) (anon: 13352K, file: 9084K, shmem: 0K)       > [adservice-7cccc9b6fc-m7dvh server] Swapped out: 0K       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/threads-max (system-wide limit on the number of threads): 126749       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/vm/max_map_count (maximum number of memory map areas a process may have): 262144       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/pid_max (system-wide limit on number of process identifiers): 4194304       > [adservice-7cccc9b6fc-m7dvh server] container (cgroup) information:       > [adservice-7cccc9b6fc-m7dvh server] container_type: cgroupv1       > [adservice-7cccc9b6fc-m7dvh server] cpu_cpuset_cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] cpu_memory_nodes: 0       > [adservice-7cccc9b6fc-m7dvh server] active_processor_count: 1       > [adservice-7cccc9b6fc-m7dvh server] cpu_quota: 30000       > [adservice-7cccc9b6fc-m7dvh server] cpu_period: 100000       > [adservice-7cccc9b6fc-m7dvh server] cpu_shares: 204       > [adservice-7cccc9b6fc-m7dvh server] memory_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_and_swap_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_soft_limit_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] memory_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] memory_max_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_usage_in_bytes: 1352 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_max_usage_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_limit_in_bytes: 1436 k       > [adservice-7cccc9b6fc-m7dvh server] maximum number of tasks: unlimited       > [adservice-7cccc9b6fc-m7dvh server] current number of tasks: 2       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks since vm start: 0       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks percentage since vm start:  0.000       > [adservice-7cccc9b6fc-m7dvh server] CPU: total 8 (initial active 1)       > [adservice-7cccc9b6fc-m7dvh server] CPU Model and flags from /proc/cpuinfo:       > [adservice-7cccc9b6fc-m7dvh server] model name  : 11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz       > [adservice-7cccc9b6fc-m7dvh server] flags               : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear flush_l1d arch_capabilities       > [adservice-7cccc9b6fc-m7dvh server] Online cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] Offline cpus:       > [adservice-7cccc9b6fc-m7dvh server] BIOS frequency limitation: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Frequency switch latency (ns): <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Available cpu frequencies: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Current governor: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Core performance/turbo boost: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Memory: 4k page, physical 307200k(292200k free), swap 4194304k(4194304k free)       > [adservice-7cccc9b6fc-m7dvh server] Page Sizes: 4k       > [adservice-7cccc9b6fc-m7dvh server] vm_info: OpenJDK 64-Bit Server VM (21.0.2+13-LTS) for linux-amd64-musl JRE (21.0.2+13-LTS), built on 2024-01-16T00:00:00Z by ""admin"" with gcc 10.3.1 20211027       > [adservice-7cccc9b6fc-m7dvh server] END.       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  - deployment/adservice failed. Error: container server terminated with exit code 139. I0426 14:29:44.134566   28800 request.go:697] Waited for 1.0548337s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:63092/apis/apps/v1/namespaces/default/replicasets?labelSelector=app%3Dshippingservice 1/11 deployment(s) failed ``` ### Screenshots  ![capture.png](https://github.com/GoogleCloudPlatform/microservices-demo/assets/60648141/de7d71e9-c1ab-42ab-be45-ef6708e2c0ad)  ### Environment  OS: Windows 11 Enterprise 23H2 Kubernetes distribution, version: minikube version: v1.33.0  relevant tool version: Docker Desktop 4.29.0, skaffold: v2.11.0 ",2024-04-26T06:50:08+00:00,2024-05-02T18:14:16+00:00,6,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2511,2568.0,2024-05-30T21:42:57+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2568,0,0,1,1,4,4,0,0,830.8802777777778,type: bug;priority: p1,True,False,critical,ui,"[{""filename"": ""src/cartservice/src/Dockerfile"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}]",src,False
aws-samples/aws-microservices-deploy-options,53,ECS cluster creation failed using CloudFormation template,"Creating an ECS cluster using CloudFormation gives the following error:   ``` Embedded stack arn:aws:cloudformation:us-east-1:091144949931:stack/aws-compute-options-ecs-Infrastructure-AHCL2W4UVW4I/8e090760-2312-11e8-969a-500c288f18d1 was not successfully created: The following resource(s) failed to create: [Route2, ALBPublic, PublicLaunchConfiguration, ALBPrivate, NAT1]. ```  Here is the snapshot:  <img width=""1601"" alt=""screen shot 2018-03-08 at 1 05 40 pm"" src=""https://user-images.githubusercontent.com/113947/37176518-720d7618-22d1-11e8-961d-0184f644f03e.png"">",2018-03-08T21:06:14+00:00,2018-03-09T18:31:18+00:00,4,https://github.com/aws-samples/aws-microservices-deploy-options/issues/53,61.0,2018-03-09T18:06:57+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/61,2,0,0,2,2,2,4,0,21.011944444444445,,True,False,normal,configuration,"[{""filename"": ""apps/ecs/ec2/templates/master.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""apps/ecs/ec2/templates/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
microservices-demo/microservices-demo,416,ECS CloudFormation: the WeaveScope output shouldn't be loadbalanced,"The WeaveScope output goes through an ELB:  <img width=""1535"" alt=""screen shot 2016-11-17 at 12 42 47"" src=""https://cloud.githubusercontent.com/assets/2362916/20388076/794fc438-acc3-11e6-9a50-a5472b2240cb.png"">  This is wrong since it leads to a split brain in the apps, affecting things like terminal spawning:  See: https://github.com/weaveworks/scope/issues/2016#issuecomment-261226026 Identical issue for the integrations repository: https://github.com/weaveworks/integrations/issues/115",2016-11-17T11:49:50+00:00,2017-02-08T12:10:42+00:00,7,https://github.com/microservices-demo/microservices-demo/issues/416,596.0,2017-02-08T12:04:09+00:00,https://github.com/microservices-demo/microservices-demo/pull/596,1,0,1,2,57,14,64,0,1992.2386111111111,bug;deployment,True,False,normal,ui,"[{""filename"": ""deploy/aws-ecs/cloudformation.json"", ""lines_added"": 51, ""lines_deleted"": 13, ""file_type"": ""config""}, {""filename"": ""deploy/aws-ecs/msdemo-cli"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
GoogleCloudPlatform/microservices-demo,1870,"Load testing using Locust failing with 2,000+ users","Hi @NimJay   ### Describe the bug  I tested the Hipster Shop performance on **AWS, GCP** and **Azure** instances using **Locust** on a **minikube cluster.** Earlier I built the Docker images for Arm64 with the `grpc-health-probe` binary used in the Dockerfile and tested the same by deploying the Hipster Shop or performing testing using **Locust**. For **Arm64** I can test the load up to **6500** users without any failures but for **x86_64** I was getting failures from **2000** users.    Now the `grpc-health-probe` binary is no longer necessary because in Kubernetes-1.24 the gRPC health-check probes functionality is built into Kubernetes. I've rebuilt the Arm64 Docker images without the `grpc-health-probe` binary and updated the [kubernetes-manifests.yaml](https://github.com/odidev/microservices-demo/blob/main/release/kubernetes-manifests.yaml) with the images I built. Deployment works successfully, and the UI is accessible. However, I'm getting failures during load testing using **Locust** due to **port forwarding** failing to handle requests from **2000** users on both **Arm64** and **x86_64** architectures.  Due to the latest changes, I am blocked. Could you please share some pointers for the same?    ### To Reproduce  Steps to reproduce the behavior: Tested the locust load by taking a master node and 11 worker nodes to generate the load perfectly by deploying the app on a minikube cluster. 1. Ran command `locust -f locustfile.py --master` and `locust -f locustfile.py --worker --master-host=localhost`   ### Logs   [load test result at 2000 user.pdf](https://github.com/GoogleCloudPlatform/microservices-demo/files/11891962/load.test.result.at.2000.user.pdf)  ### Screenshots  During load testing port forwarding failing to handle requests from 2000 or more users  ![image](https://github.com/GoogleCloudPlatform/microservices-demo/assets/40816837/894b491c-5f51-4f14-8e28-d8296d2f8520)  ### Environment  - OS:  Ubuntu 22.04.2 LTS - Kubernetes distribution, version: minikube - Any relevant tool version: Locust 2.15.1   ",2023-06-28T08:39:48+00:00,2024-04-18T19:24:44+00:00,7,https://github.com/GoogleCloudPlatform/microservices-demo/issues/1870,2844.0,2024-12-31T22:51:35+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2844,0,0,1,1,1,1,0,0,13262.19638888889,type: question;priority: p3,True,False,normal,configuration,"[{""filename"": ""src/shoppingassistantservice/requirements.txt"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
microservices-demo/microservices-demo,416,ECS CloudFormation: the WeaveScope output shouldn't be loadbalanced,"The WeaveScope output goes through an ELB:  <img width=""1535"" alt=""screen shot 2016-11-17 at 12 42 47"" src=""https://cloud.githubusercontent.com/assets/2362916/20388076/794fc438-acc3-11e6-9a50-a5472b2240cb.png"">  This is wrong since it leads to a split brain in the apps, affecting things like terminal spawning:  See: https://github.com/weaveworks/scope/issues/2016#issuecomment-261226026 Identical issue for the integrations repository: https://github.com/weaveworks/integrations/issues/115",2016-11-17T11:49:50+00:00,2017-02-08T12:10:42+00:00,7,https://github.com/microservices-demo/microservices-demo/issues/416,596.0,2017-02-08T12:04:09+00:00,https://github.com/microservices-demo/microservices-demo/pull/596,1,0,1,2,57,14,64,0,1992.2386111111111,bug;deployment,True,False,normal,ui,"[{""filename"": ""deploy/aws-ecs/cloudformation.json"", ""lines_added"": 51, ""lines_deleted"": 13, ""file_type"": ""config""}, {""filename"": ""deploy/aws-ecs/msdemo-cli"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
GoogleCloudPlatform/microservices-demo,2337,skaffold run gives error while checking cache for one service on Linux Droplet.,"root@test-s-4vcpu-8gb-blr1:~/microservices-demo# kubectl get nodes NAME       STATUS   ROLES           AGE    VERSION minikube   Ready    control-plane   138d   v1.27.3 root@test-s-4vcpu-8gb-blr1:~/microservices-demo# skaffold run Generating tags...  - adservice -> adservice:v0.5.0-528-g46aa585-dirty  - cartservice -> cartservice:v0.5.0-528-g46aa585  - checkoutservice -> checkoutservice:v0.5.0-528-g46aa585  - currencyservice -> currencyservice:v0.5.0-528-g46aa585  - emailservice -> emailservice:v0.5.0-528-g46aa585  - frontend -> frontend:v0.5.0-528-g46aa585  - loadgenerator -> loadgenerator:v0.5.0-528-g46aa585  - paymentservice -> paymentservice:v0.5.0-528-g46aa585  - productcatalogservice -> productcatalogservice:v0.5.0-528-g46aa585  - recommendationservice -> recommendationservice:v0.5.0-528-g46aa585  - shippingservice -> shippingservice:v0.5.0-528-g46aa585 ###Checking cache...  ###- adservice: Error checking cache. ###getting hash for artifact ""adservice"": getting dependencies for ""adservice"": file pattern [../build.gradle ../gradlew] must ###match at least one file  I want to deploy microservices-demo on a Digital ocean Linux VM , but @NimJay here it shows a error mentioned above while executing skaffold run, why is that , kindly make me understand and this and share your advice about to proceed further.",2024-01-16T17:40:25+00:00,2024-04-17T21:44:00+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2337,2391.0,2024-02-26T13:02:03+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2391,0,0,2,2,3,3,0,0,979.3605555555556,,True,False,normal,ui,"[{""filename"": ""src/cartservice/src/cartservice.csproj"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/cartservice/tests/cartservice.tests.csproj"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
microservices-demo/microservices-demo,443,"lack of connectivity, even when running load tests","Even when running the load test, Weave Scope often does not show all the expected connections between all the SockShop components/services.",2016-12-05T13:30:41+00:00,2017-01-09T16:58:56+00:00,15,https://github.com/microservices-demo/microservices-demo/issues/443,530.0,2017-01-09T17:00:34+00:00,https://github.com/microservices-demo/microservices-demo/pull/530,1,0,0,1,1,1,2,0,843.4980555555555,bug,True,False,normal,networking,"[{""filename"": ""deploy/kubernetes/manifests/front-end-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,255,Fix docker build on BTRFS,"when running `./mvnw clean install -PbuildDocker -Dmaven.test.skip=true` in my system which uses BTRFS and `Docker version 25.0.3, build 4debf411d1`, the docker build process thats happening under the hood fails with the root cause `layer does not exist`.  A bit of search led me to [this fix](https://stackoverflow.com/a/62409523), and with [this issue](https://github.com/moby/moby/issues/36573) i think I found the root reason why this fails just for me.  This MR changes the dockerfile so that it also works on my machine.",2024-03-04T13:56:57+00:00,2024-03-04T14:22:54+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,255.0,2024-03-04T14:22:54+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,0,0,1,1,8,0,0,0,0.4325,bug,True,False,normal,ui,"[{""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,443,"lack of connectivity, even when running load tests","Even when running the load test, Weave Scope often does not show all the expected connections between all the SockShop components/services.",2016-12-05T13:30:41+00:00,2017-01-09T16:58:56+00:00,15,https://github.com/microservices-demo/microservices-demo/issues/443,530.0,2017-01-09T17:00:34+00:00,https://github.com/microservices-demo/microservices-demo/pull/530,1,0,0,1,1,1,2,0,843.4980555555555,bug,True,False,normal,networking,"[{""filename"": ""deploy/kubernetes/manifests/front-end-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
GoogleCloudPlatform/microservices-demo,2873,secCompProfile without securityContext enabled leads to errors,"### Describe the bug  when .Values.securityContext is set to 'false' and .Values.seccompProfile.type is set the yaml key ""securityContext"" is not set in the YAML file for all services  ### To Reproduce  1. set .Values.securityContext.enable to 'false' and .Values.seccompProfile.type 2. deploy application| use helm template or something similar 3. see wrong YAML setup  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  persistent issue ",2025-01-20T20:09:02+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2873,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,217.9808333333333,type: bug;priority: p2,True,False,normal,configuration,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
GoogleCloudPlatform/microservices-demo,2872,securityContext opt-in not working for paymentservice + opentelemetry collector,### Describe the bug  when setting .Values.securityContext.enable to 'false' both deployments mentioned above would still try to set uid + gid to 1000 and don't acknowledge the variable passed in the values-file   ### To Reproduce  1. set Values.securityContext.enable to 'false' 2. deploy application| use helm template or sth. similar 3. see wrong uid/gid in paymentservice + opentelemetry-collector pod  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  it appears to be persistent ,2025-01-20T20:06:25+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2872,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,218.02444444444444,type: bug;priority: p2,True,False,normal,security,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
GoogleCloudPlatform/microservices-demo,2677,"GKE v1.29.7-gke.1174000: rpc error: code = Unavailable desc = connection error around ""could not retrieve currencies""","### Describe the bug  On GKE version `v1.29.7-gke.1174000`,  I got an issue while deploying the online-boutique demo with the log below.  ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ### To Reproduce  <!-- Steps to reproduce the behavior: --> Ran the command: ``` kubectl create ns microservices-demo kubectl apply -n microservices-demo -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml ``` Waited for the workloads to  be ready but the load generator pod was crashing due to the `frontend-check` failing to get a successful response. ``` + echo 'Attempt 11: Pinging frontend: frontend:80...' Attempt 11: Pinging frontend: frontend:80... + wget --server-response [http://frontend:80](http://frontend/) + awk '/^  HTTP/{print $2}' Error: Could not reach frontend - Status code: 500 ```  ``` $ k get pods -n microservices-demo NAME                                     READY   STATUS     RESTARTS         AGE adservice-7d7dd69f68-qx4gc               1/1     Running    0                55m cartservice-777f69bb65-rk7xv             1/1     Running    0                55m checkoutservice-bcbf96994-hjntp          1/1     Running    0                55m currencyservice-8557bb8fb-lqrr4          1/1     Running    0                55m emailservice-57cc9b487b-sctq4            1/1     Running    0                55m frontend-6d9488c857-shz52                1/1     Running    0                55m loadgenerator-856dc76c97-m6bhs           0/1     Init:0/1   11 (6m57s ago)   55m paymentservice-759576897b-44dw8          1/1     Running    0                55m productcatalogservice-7b7d78cdbb-n4tsz   1/1     Running    0                55m recommendationservice-769cb665c4-wtjds   1/1     Running    0                55m redis-cart-bf5c68f69-tvvq9               1/1     Running    0                55m shippingservice-5d97bddcdd-zvwbr         1/1     Running    0                55m ```  ### Logs   ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ``` $ k logs loadgenerator-7995465594-k7nph -c frontend-check -n microservices-demo + MAX_RETRIES=12 + RETRY_INTERVAL=10 + seq 1 12 + echo 'Attempt 1: Pinging frontend: frontend:80...' Attempt 1: Pinging frontend: frontend:80... + awk '/^  HTTP/{print $2}' + wget --server-response http://frontend:80 + STATUSCODE=500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' + sleep 10 ... + echo 'Attempt 9: Pinging frontend: frontend:80...' Attempt 9: Pinging frontend: frontend:80... + + awk '/^  HTTP/{print $2}' wget --server-response http://frontend:80 + STATUSCODE=500 Error: Could not reach frontend - Status code: 500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' ```  ### Screenshots  ![online-boutique-gke](https://github.com/user-attachments/assets/b1515906-62be-4bad-9c9c-ce6dc4e0fd24)  ### Environment  GKE version `v1.29.7-gke.1174000`  ### Additional context  <!-- Add any other context about the problem here -->  ### Exposure  <!-- Is the bug intermittent, persistent? Is it widespread, local? --> ",2024-08-13T19:53:42+00:00,2024-08-14T21:36:31+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2677,2429.0,2024-03-18T01:50:38+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2429,4,0,0,4,44,21,65,0,-3570.051111111111,,True,False,critical,configuration,"[{""filename"": ""src/currencyservice/package-lock.json"", ""lines_added"": 16, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""src/currencyservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package-lock.json"", ""lines_added"": 26, ""lines_deleted"": 12, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",src,False
GoogleCloudPlatform/microservices-demo,2511,Adservice crashing with segfault,"### Describe the bug  Unable to build / deploy on Windows11  ### To Reproduce  ``` git clone https://github.com/GoogleCloudPlatform/microservices-demo cd microservices-demo/ minikube start --cpus=4 --memory 4096 --disk-size 32g skaffold run ``` ### Logs   ``` Waiting for deployments to stabilize...  - deployment/checkoutservice is ready. [10/11 deployment(s) still pending]  - deployment/adservice: container server terminated with exit code 139     - pod/adservice-7cccc9b6fc-m7dvh: container server terminated with exit code 139       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Can not save log file, dump to screen..       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # ...       > [adservice-7cccc9b6fc-m7dvh server] /sys/kernel/mm/transparent_hugepage/defrag (defrag/compaction efforts parameter): always defer defer+madvise [madvise] never       > [adservice-7cccc9b6fc-m7dvh server] Process Memory:       > [adservice-7cccc9b6fc-m7dvh server] Virtual Size: 146444K (peak: 146444K)       > [adservice-7cccc9b6fc-m7dvh server] Resident Set Size: 22436K (peak: 22436K) (anon: 13352K, file: 9084K, shmem: 0K)       > [adservice-7cccc9b6fc-m7dvh server] Swapped out: 0K       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/threads-max (system-wide limit on the number of threads): 126749       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/vm/max_map_count (maximum number of memory map areas a process may have): 262144       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/pid_max (system-wide limit on number of process identifiers): 4194304       > [adservice-7cccc9b6fc-m7dvh server] container (cgroup) information:       > [adservice-7cccc9b6fc-m7dvh server] container_type: cgroupv1       > [adservice-7cccc9b6fc-m7dvh server] cpu_cpuset_cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] cpu_memory_nodes: 0       > [adservice-7cccc9b6fc-m7dvh server] active_processor_count: 1       > [adservice-7cccc9b6fc-m7dvh server] cpu_quota: 30000       > [adservice-7cccc9b6fc-m7dvh server] cpu_period: 100000       > [adservice-7cccc9b6fc-m7dvh server] cpu_shares: 204       > [adservice-7cccc9b6fc-m7dvh server] memory_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_and_swap_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_soft_limit_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] memory_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] memory_max_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_usage_in_bytes: 1352 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_max_usage_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_limit_in_bytes: 1436 k       > [adservice-7cccc9b6fc-m7dvh server] maximum number of tasks: unlimited       > [adservice-7cccc9b6fc-m7dvh server] current number of tasks: 2       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks since vm start: 0       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks percentage since vm start:  0.000       > [adservice-7cccc9b6fc-m7dvh server] CPU: total 8 (initial active 1)       > [adservice-7cccc9b6fc-m7dvh server] CPU Model and flags from /proc/cpuinfo:       > [adservice-7cccc9b6fc-m7dvh server] model name  : 11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz       > [adservice-7cccc9b6fc-m7dvh server] flags               : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear flush_l1d arch_capabilities       > [adservice-7cccc9b6fc-m7dvh server] Online cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] Offline cpus:       > [adservice-7cccc9b6fc-m7dvh server] BIOS frequency limitation: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Frequency switch latency (ns): <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Available cpu frequencies: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Current governor: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Core performance/turbo boost: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Memory: 4k page, physical 307200k(292200k free), swap 4194304k(4194304k free)       > [adservice-7cccc9b6fc-m7dvh server] Page Sizes: 4k       > [adservice-7cccc9b6fc-m7dvh server] vm_info: OpenJDK 64-Bit Server VM (21.0.2+13-LTS) for linux-amd64-musl JRE (21.0.2+13-LTS), built on 2024-01-16T00:00:00Z by ""admin"" with gcc 10.3.1 20211027       > [adservice-7cccc9b6fc-m7dvh server] END.       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  - deployment/adservice failed. Error: container server terminated with exit code 139. I0426 14:29:44.134566   28800 request.go:697] Waited for 1.0548337s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:63092/apis/apps/v1/namespaces/default/replicasets?labelSelector=app%3Dshippingservice 1/11 deployment(s) failed ``` ### Screenshots  ![capture.png](https://github.com/GoogleCloudPlatform/microservices-demo/assets/60648141/de7d71e9-c1ab-42ab-be45-ef6708e2c0ad)  ### Environment  OS: Windows 11 Enterprise 23H2 Kubernetes distribution, version: minikube version: v1.33.0  relevant tool version: Docker Desktop 4.29.0, skaffold: v2.11.0 ",2024-04-26T06:50:08+00:00,2024-05-02T18:14:16+00:00,6,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2511,2568.0,2024-05-30T21:42:57+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2568,0,0,1,1,4,4,0,0,830.8802777777778,type: bug;priority: p1,True,False,critical,ui,"[{""filename"": ""src/cartservice/src/Dockerfile"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}]",src,False
GoogleCloudPlatform/microservices-demo,1870,"Load testing using Locust failing with 2,000+ users","Hi @NimJay   ### Describe the bug  I tested the Hipster Shop performance on **AWS, GCP** and **Azure** instances using **Locust** on a **minikube cluster.** Earlier I built the Docker images for Arm64 with the `grpc-health-probe` binary used in the Dockerfile and tested the same by deploying the Hipster Shop or performing testing using **Locust**. For **Arm64** I can test the load up to **6500** users without any failures but for **x86_64** I was getting failures from **2000** users.    Now the `grpc-health-probe` binary is no longer necessary because in Kubernetes-1.24 the gRPC health-check probes functionality is built into Kubernetes. I've rebuilt the Arm64 Docker images without the `grpc-health-probe` binary and updated the [kubernetes-manifests.yaml](https://github.com/odidev/microservices-demo/blob/main/release/kubernetes-manifests.yaml) with the images I built. Deployment works successfully, and the UI is accessible. However, I'm getting failures during load testing using **Locust** due to **port forwarding** failing to handle requests from **2000** users on both **Arm64** and **x86_64** architectures.  Due to the latest changes, I am blocked. Could you please share some pointers for the same?    ### To Reproduce  Steps to reproduce the behavior: Tested the locust load by taking a master node and 11 worker nodes to generate the load perfectly by deploying the app on a minikube cluster. 1. Ran command `locust -f locustfile.py --master` and `locust -f locustfile.py --worker --master-host=localhost`   ### Logs   [load test result at 2000 user.pdf](https://github.com/GoogleCloudPlatform/microservices-demo/files/11891962/load.test.result.at.2000.user.pdf)  ### Screenshots  During load testing port forwarding failing to handle requests from 2000 or more users  ![image](https://github.com/GoogleCloudPlatform/microservices-demo/assets/40816837/894b491c-5f51-4f14-8e28-d8296d2f8520)  ### Environment  - OS:  Ubuntu 22.04.2 LTS - Kubernetes distribution, version: minikube - Any relevant tool version: Locust 2.15.1   ",2023-06-28T08:39:48+00:00,2024-04-18T19:24:44+00:00,7,https://github.com/GoogleCloudPlatform/microservices-demo/issues/1870,2844.0,2024-12-31T22:51:35+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2844,0,0,1,1,1,1,0,0,13262.19638888889,type: question;priority: p3,True,False,normal,configuration,"[{""filename"": ""src/shoppingassistantservice/requirements.txt"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
spring-petclinic/spring-petclinic-microservices,127,openzipkin/zipkin startup error,It appears that the openzipkin/zipkin docker image in the docker hub was upgraded to Java 11 and the -XX:+UseCGroupMemoryLimitForHeap is no longer a valid option.  I believe it just needs to be removed.  The startup error is  ``` tracing-server       | Unrecognized VM option 'UseCGroupMemoryLimitForHeap' tracing-server       | Error: Could not create the Java Virtual Machine. tracing-server       | Error: A fatal exception has occurred. Program will exit. ```,2019-04-23T01:25:19+00:00,2019-04-23T05:21:35+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/127,225.0,2023-07-03T18:19:09+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/225,4,0,0,4,48,0,48,0,36784.89722222222,enhancement,True,False,normal,performance,"[{""filename"": ""spring-petclinic-api-gateway/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-customers-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-vets-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-visits-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}]",spring-petclinic-customers-service;spring-petclinic-visits-service;spring-petclinic-vets-service;spring-petclinic-api-gateway,True
GoogleCloudPlatform/microservices-demo,2337,skaffold run gives error while checking cache for one service on Linux Droplet.,"root@test-s-4vcpu-8gb-blr1:~/microservices-demo# kubectl get nodes NAME       STATUS   ROLES           AGE    VERSION minikube   Ready    control-plane   138d   v1.27.3 root@test-s-4vcpu-8gb-blr1:~/microservices-demo# skaffold run Generating tags...  - adservice -> adservice:v0.5.0-528-g46aa585-dirty  - cartservice -> cartservice:v0.5.0-528-g46aa585  - checkoutservice -> checkoutservice:v0.5.0-528-g46aa585  - currencyservice -> currencyservice:v0.5.0-528-g46aa585  - emailservice -> emailservice:v0.5.0-528-g46aa585  - frontend -> frontend:v0.5.0-528-g46aa585  - loadgenerator -> loadgenerator:v0.5.0-528-g46aa585  - paymentservice -> paymentservice:v0.5.0-528-g46aa585  - productcatalogservice -> productcatalogservice:v0.5.0-528-g46aa585  - recommendationservice -> recommendationservice:v0.5.0-528-g46aa585  - shippingservice -> shippingservice:v0.5.0-528-g46aa585 ###Checking cache...  ###- adservice: Error checking cache. ###getting hash for artifact ""adservice"": getting dependencies for ""adservice"": file pattern [../build.gradle ../gradlew] must ###match at least one file  I want to deploy microservices-demo on a Digital ocean Linux VM , but @NimJay here it shows a error mentioned above while executing skaffold run, why is that , kindly make me understand and this and share your advice about to proceed further.",2024-01-16T17:40:25+00:00,2024-04-17T21:44:00+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2337,2391.0,2024-02-26T13:02:03+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2391,0,0,2,2,3,3,0,0,979.3605555555556,,True,False,normal,ui,"[{""filename"": ""src/cartservice/src/cartservice.csproj"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/cartservice/tests/cartservice.tests.csproj"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
microservices-demo/microservices-demo,348,Staging environment is failing because of docker daemon hangs,,2016-10-05T10:59:18+00:00,2016-11-08T16:21:41+00:00,1,https://github.com/microservices-demo/microservices-demo/issues/348,774.0,2018-07-27T08:22:19+00:00,https://github.com/microservices-demo/microservices-demo/pull/774,1,0,0,1,1,0,1,0,15837.38361111111,bug;deployment,True,False,normal,functional,"[{""filename"": ""deploy/kubernetes/manifests/catalogue-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",catalogue-dep.yaml,False
microservices-demo/microservices-demo,348,Staging environment is failing because of docker daemon hangs,,2016-10-05T10:59:18+00:00,2016-11-08T16:21:41+00:00,1,https://github.com/microservices-demo/microservices-demo/issues/348,774.0,2018-07-27T08:22:19+00:00,https://github.com/microservices-demo/microservices-demo/pull/774,1,0,0,1,1,0,1,0,15837.38361111111,bug;deployment,True,False,normal,functional,"[{""filename"": ""deploy/kubernetes/manifests/catalogue-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",catalogue-dep.yaml,False
spring-petclinic/spring-petclinic-microservices,37,Owner details failed when visits-service is down,"We could improve the VisitsServiceClient class by adding a fallback when visits-service is down (maybe with @HystrixCommand). The owner details will not display visits. Here is the stacktrace: ``` 2017-01-14 14:18:52.427 ERROR [api-gateway,,,] 20763 --- [nio-8080-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: No instances available for visits-service] with root cause  java.lang.IllegalStateException: No instances available for visits-service 	at org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient.execute(RibbonLoadBalancerClient.java:90) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:60) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:276) 	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:157) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor.intercept(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.response(TraceRestTemplateInterceptor.java:59) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.intercept(TraceRestTemplateInterceptor.java:53) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.netflix.metrics.MetricsClientHttpRequestInterceptor.intercept(MetricsClientHttpRequestInterceptor.java:68) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.http.client.InterceptingClientHttpRequest.executeInternal(InterceptingClientHttpRequest.java:69) 	at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48) 	at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:53) 	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:619) 	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:580) 	at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:526) 	at org.springframework.web.client.RestTemplate$$FastClassBySpringCGLIB$$aa4e9ed0.invoke(<generated>) 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) 	at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:85) 	at org.springframework.cloud.netflix.metrics.RestTemplateUrlTemplateCapturingAspect.captureUrlTemplate(RestTemplateUrlTemplateCapturingAspect.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:629) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:618) 	at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) 	at org.springframework.web.client.RestTemplate$$EnhancerBySpringCGLIB$$d5d328a8.exchange(<generated>) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.lambda$getVisitsForPets$1(VisitsServiceClient.java:29) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient$$Lambda$13/1446308391.apply(Unknown Source) 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267) 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512) 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) 	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:400) 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:728) 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.getVisitsForPets(VisitsServiceClient.java:31) 	at org.springframework.samples.petclinic.api.boundary.web.ApiGatewayController.getOwnerDetails(ApiGatewayController.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:220) 	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:134) 	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:116) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) 	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) 	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) 	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) 	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) 	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:622) 	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:105) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:89) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:141) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:107) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) 	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:108) 	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472) 	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) 	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) 	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) 	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) 	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:784) 	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) 	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:802) 	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1410) 	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) 	at java.lang.Thread.run(Thread.java:745) ```",2017-01-14T13:21:04+00:00,2017-01-14T15:11:21+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/37,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15332.396666666667,enhancement,True,False,normal,configuration,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
aws-samples/aws-microservices-deploy-options,230,Health checks are failing Fargate deployment pipeline,https://github.com/aws-samples/aws-microservices-deploy-options#deployment-pipeline-fargate-with-aws-codepipeline creates all the CloudFormation templates. But accessing the `ServiceUrl` shows that the service is inaccessible.  Health checks are failing.  @jicowan is investigating,2018-04-14T02:11:47+00:00,2018-04-16T04:04:28+00:00,5,https://github.com/aws-samples/aws-microservices-deploy-options/issues/230,244.0,2018-04-16T04:03:48+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/244,1,0,0,1,1,1,2,0,49.86694444444444,,True,False,normal,security,"[{""filename"": ""apps/ecs/deployment/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
spring-petclinic/spring-petclinic-microservices,31,Services cannot contact with Zipkin server,Zipkin server does not see any traces nor dependencies. Logs are not being pushed to Zipkin.  Reproducible when starting services locally as well as with Docker compose.,2017-01-07T17:25:04+00:00,2017-01-09T07:27:05+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/31,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15496.33,bug,True,False,normal,functional,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,255,Fix docker build on BTRFS,"when running `./mvnw clean install -PbuildDocker -Dmaven.test.skip=true` in my system which uses BTRFS and `Docker version 25.0.3, build 4debf411d1`, the docker build process thats happening under the hood fails with the root cause `layer does not exist`.  A bit of search led me to [this fix](https://stackoverflow.com/a/62409523), and with [this issue](https://github.com/moby/moby/issues/36573) i think I found the root reason why this fails just for me.  This MR changes the dockerfile so that it also works on my machine.",2024-03-04T13:56:57+00:00,2024-03-04T14:22:54+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,255.0,2024-03-04T14:22:54+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,0,0,1,1,8,0,0,0,0.4325,bug,True,False,normal,ui,"[{""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
aws-samples/aws-microservices-deploy-options,146,Lambda functions are failing on AWS,"Invoking the Lambda function on AWS is giving the following error:  ``` lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='GreetingApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""}lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='NamesApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""} ``` ",2018-03-30T21:14:38+00:00,2018-03-31T00:10:05+00:00,1,https://github.com/aws-samples/aws-microservices-deploy-options/issues/146,152.0,2018-03-31T00:10:05+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/152,4,0,0,4,82,140,222,0,2.924166666666667,lambda,True,False,normal,database,"[{""filename"": ""services/greeting/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 39, ""file_type"": ""config""}, {""filename"": ""services/name/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 45, ""file_type"": ""config""}, {""filename"": ""services/pom.xml"", ""lines_added"": 82, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""services/webapp/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 49, ""file_type"": ""config""}]",services,False
microservices-demo/microservices-demo,416,ECS CloudFormation: the WeaveScope output shouldn't be loadbalanced,"The WeaveScope output goes through an ELB:  <img width=""1535"" alt=""screen shot 2016-11-17 at 12 42 47"" src=""https://cloud.githubusercontent.com/assets/2362916/20388076/794fc438-acc3-11e6-9a50-a5472b2240cb.png"">  This is wrong since it leads to a split brain in the apps, affecting things like terminal spawning:  See: https://github.com/weaveworks/scope/issues/2016#issuecomment-261226026 Identical issue for the integrations repository: https://github.com/weaveworks/integrations/issues/115",2016-11-17T11:49:50+00:00,2017-02-08T12:10:42+00:00,7,https://github.com/microservices-demo/microservices-demo/issues/416,596.0,2017-02-08T12:04:09+00:00,https://github.com/microservices-demo/microservices-demo/pull/596,1,0,1,2,57,14,64,0,1992.2386111111111,bug;deployment,True,False,normal,ui,"[{""filename"": ""deploy/aws-ecs/cloudformation.json"", ""lines_added"": 51, ""lines_deleted"": 13, ""file_type"": ""config""}, {""filename"": ""deploy/aws-ecs/msdemo-cli"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
aws-samples/aws-microservices-deploy-options,53,ECS cluster creation failed using CloudFormation template,"Creating an ECS cluster using CloudFormation gives the following error:   ``` Embedded stack arn:aws:cloudformation:us-east-1:091144949931:stack/aws-compute-options-ecs-Infrastructure-AHCL2W4UVW4I/8e090760-2312-11e8-969a-500c288f18d1 was not successfully created: The following resource(s) failed to create: [Route2, ALBPublic, PublicLaunchConfiguration, ALBPrivate, NAT1]. ```  Here is the snapshot:  <img width=""1601"" alt=""screen shot 2018-03-08 at 1 05 40 pm"" src=""https://user-images.githubusercontent.com/113947/37176518-720d7618-22d1-11e8-961d-0184f644f03e.png"">",2018-03-08T21:06:14+00:00,2018-03-09T18:31:18+00:00,4,https://github.com/aws-samples/aws-microservices-deploy-options/issues/53,61.0,2018-03-09T18:06:57+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/61,2,0,0,2,2,2,4,0,21.011944444444445,,True,False,normal,configuration,"[{""filename"": ""apps/ecs/ec2/templates/master.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""apps/ecs/ec2/templates/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
microservices-demo/microservices-demo,443,"lack of connectivity, even when running load tests","Even when running the load test, Weave Scope often does not show all the expected connections between all the SockShop components/services.",2016-12-05T13:30:41+00:00,2017-01-09T16:58:56+00:00,15,https://github.com/microservices-demo/microservices-demo/issues/443,530.0,2017-01-09T17:00:34+00:00,https://github.com/microservices-demo/microservices-demo/pull/530,1,0,0,1,1,1,2,0,843.4980555555555,bug,True,False,normal,networking,"[{""filename"": ""deploy/kubernetes/manifests/front-end-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,127,openzipkin/zipkin startup error,It appears that the openzipkin/zipkin docker image in the docker hub was upgraded to Java 11 and the -XX:+UseCGroupMemoryLimitForHeap is no longer a valid option.  I believe it just needs to be removed.  The startup error is  ``` tracing-server       | Unrecognized VM option 'UseCGroupMemoryLimitForHeap' tracing-server       | Error: Could not create the Java Virtual Machine. tracing-server       | Error: A fatal exception has occurred. Program will exit. ```,2019-04-23T01:25:19+00:00,2019-04-23T05:21:35+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/127,225.0,2023-07-03T18:19:09+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/225,4,0,0,4,48,0,48,0,36784.89722222222,enhancement,True,False,normal,performance,"[{""filename"": ""spring-petclinic-api-gateway/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-customers-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-vets-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-visits-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}]",spring-petclinic-customers-service;spring-petclinic-visits-service;spring-petclinic-vets-service;spring-petclinic-api-gateway,True
spring-petclinic/spring-petclinic-microservices,37,Owner details failed when visits-service is down,"We could improve the VisitsServiceClient class by adding a fallback when visits-service is down (maybe with @HystrixCommand). The owner details will not display visits. Here is the stacktrace: ``` 2017-01-14 14:18:52.427 ERROR [api-gateway,,,] 20763 --- [nio-8080-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: No instances available for visits-service] with root cause  java.lang.IllegalStateException: No instances available for visits-service 	at org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient.execute(RibbonLoadBalancerClient.java:90) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:60) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:276) 	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:157) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor.intercept(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.response(TraceRestTemplateInterceptor.java:59) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.intercept(TraceRestTemplateInterceptor.java:53) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.netflix.metrics.MetricsClientHttpRequestInterceptor.intercept(MetricsClientHttpRequestInterceptor.java:68) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.http.client.InterceptingClientHttpRequest.executeInternal(InterceptingClientHttpRequest.java:69) 	at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48) 	at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:53) 	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:619) 	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:580) 	at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:526) 	at org.springframework.web.client.RestTemplate$$FastClassBySpringCGLIB$$aa4e9ed0.invoke(<generated>) 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) 	at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:85) 	at org.springframework.cloud.netflix.metrics.RestTemplateUrlTemplateCapturingAspect.captureUrlTemplate(RestTemplateUrlTemplateCapturingAspect.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:629) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:618) 	at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) 	at org.springframework.web.client.RestTemplate$$EnhancerBySpringCGLIB$$d5d328a8.exchange(<generated>) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.lambda$getVisitsForPets$1(VisitsServiceClient.java:29) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient$$Lambda$13/1446308391.apply(Unknown Source) 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267) 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512) 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) 	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:400) 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:728) 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.getVisitsForPets(VisitsServiceClient.java:31) 	at org.springframework.samples.petclinic.api.boundary.web.ApiGatewayController.getOwnerDetails(ApiGatewayController.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:220) 	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:134) 	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:116) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) 	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) 	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) 	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) 	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) 	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:622) 	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:105) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:89) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:141) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:107) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) 	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:108) 	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472) 	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) 	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) 	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) 	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) 	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:784) 	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) 	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:802) 	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1410) 	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) 	at java.lang.Thread.run(Thread.java:745) ```",2017-01-14T13:21:04+00:00,2017-01-14T15:11:21+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/37,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15332.396666666667,enhancement,True,False,normal,configuration,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,31,Services cannot contact with Zipkin server,Zipkin server does not see any traces nor dependencies. Logs are not being pushed to Zipkin.  Reproducible when starting services locally as well as with Docker compose.,2017-01-07T17:25:04+00:00,2017-01-09T07:27:05+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/31,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15496.33,bug,True,False,normal,functional,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,255,Fix docker build on BTRFS,"when running `./mvnw clean install -PbuildDocker -Dmaven.test.skip=true` in my system which uses BTRFS and `Docker version 25.0.3, build 4debf411d1`, the docker build process thats happening under the hood fails with the root cause `layer does not exist`.  A bit of search led me to [this fix](https://stackoverflow.com/a/62409523), and with [this issue](https://github.com/moby/moby/issues/36573) i think I found the root reason why this fails just for me.  This MR changes the dockerfile so that it also works on my machine.",2024-03-04T13:56:57+00:00,2024-03-04T14:22:54+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,255.0,2024-03-04T14:22:54+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/255,0,0,1,1,8,0,0,0,0.4325,bug,True,False,normal,ui,"[{""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
GoogleCloudPlatform/microservices-demo,2873,secCompProfile without securityContext enabled leads to errors,"### Describe the bug  when .Values.securityContext is set to 'false' and .Values.seccompProfile.type is set the yaml key ""securityContext"" is not set in the YAML file for all services  ### To Reproduce  1. set .Values.securityContext.enable to 'false' and .Values.seccompProfile.type 2. deploy application| use helm template or something similar 3. see wrong YAML setup  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  persistent issue ",2025-01-20T20:09:02+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2873,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,217.9808333333333,type: bug;priority: p2,True,False,normal,configuration,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
aws-samples/aws-microservices-deploy-options,230,Health checks are failing Fargate deployment pipeline,https://github.com/aws-samples/aws-microservices-deploy-options#deployment-pipeline-fargate-with-aws-codepipeline creates all the CloudFormation templates. But accessing the `ServiceUrl` shows that the service is inaccessible.  Health checks are failing.  @jicowan is investigating,2018-04-14T02:11:47+00:00,2018-04-16T04:04:28+00:00,5,https://github.com/aws-samples/aws-microservices-deploy-options/issues/230,244.0,2018-04-16T04:03:48+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/244,1,0,0,1,1,1,2,0,49.86694444444444,,True,False,normal,security,"[{""filename"": ""apps/ecs/deployment/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
GoogleCloudPlatform/microservices-demo,2872,securityContext opt-in not working for paymentservice + opentelemetry collector,### Describe the bug  when setting .Values.securityContext.enable to 'false' both deployments mentioned above would still try to set uid + gid to 1000 and don't acknowledge the variable passed in the values-file   ### To Reproduce  1. set Values.securityContext.enable to 'false' 2. deploy application| use helm template or sth. similar 3. see wrong uid/gid in paymentservice + opentelemetry-collector pod  ### Logs   n/a  ### Screenshots  n/a  ### Environment  helm version 3.16.4 chart version 0.10.2  ### Additional context  n/a  ### Exposure  it appears to be persistent ,2025-01-20T20:06:25+00:00,2025-01-29T22:07:54+00:00,0,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2872,2874.0,2025-01-29T22:07:53+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2874,12,0,0,12,14,10,24,0,218.02444444444444,type: bug;priority: p2,True,False,normal,security,"[{""filename"": ""helm-chart/templates/adservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/cartservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/checkoutservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/currencyservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/emailservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/frontend.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/loadgenerator.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/opentelemetry-collector.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/paymentservice.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/productcatalogservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/recommendationservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""helm-chart/templates/shippingservice.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
microservices-demo/microservices-demo,348,Staging environment is failing because of docker daemon hangs,,2016-10-05T10:59:18+00:00,2016-11-08T16:21:41+00:00,1,https://github.com/microservices-demo/microservices-demo/issues/348,774.0,2018-07-27T08:22:19+00:00,https://github.com/microservices-demo/microservices-demo/pull/774,1,0,0,1,1,0,1,0,15837.38361111111,bug;deployment,True,False,normal,functional,"[{""filename"": ""deploy/kubernetes/manifests/catalogue-dep.yaml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",catalogue-dep.yaml,False
GoogleCloudPlatform/microservices-demo,2677,"GKE v1.29.7-gke.1174000: rpc error: code = Unavailable desc = connection error around ""could not retrieve currencies""","### Describe the bug  On GKE version `v1.29.7-gke.1174000`,  I got an issue while deploying the online-boutique demo with the log below.  ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ### To Reproduce  <!-- Steps to reproduce the behavior: --> Ran the command: ``` kubectl create ns microservices-demo kubectl apply -n microservices-demo -f https://raw.githubusercontent.com/GoogleCloudPlatform/microservices-demo/main/release/kubernetes-manifests.yaml ``` Waited for the workloads to  be ready but the load generator pod was crashing due to the `frontend-check` failing to get a successful response. ``` + echo 'Attempt 11: Pinging frontend: frontend:80...' Attempt 11: Pinging frontend: frontend:80... + wget --server-response [http://frontend:80](http://frontend/) + awk '/^  HTTP/{print $2}' Error: Could not reach frontend - Status code: 500 ```  ``` $ k get pods -n microservices-demo NAME                                     READY   STATUS     RESTARTS         AGE adservice-7d7dd69f68-qx4gc               1/1     Running    0                55m cartservice-777f69bb65-rk7xv             1/1     Running    0                55m checkoutservice-bcbf96994-hjntp          1/1     Running    0                55m currencyservice-8557bb8fb-lqrr4          1/1     Running    0                55m emailservice-57cc9b487b-sctq4            1/1     Running    0                55m frontend-6d9488c857-shz52                1/1     Running    0                55m loadgenerator-856dc76c97-m6bhs           0/1     Init:0/1   11 (6m57s ago)   55m paymentservice-759576897b-44dw8          1/1     Running    0                55m productcatalogservice-7b7d78cdbb-n4tsz   1/1     Running    0                55m recommendationservice-769cb665c4-wtjds   1/1     Running    0                55m redis-cart-bf5c68f69-tvvq9               1/1     Running    0                55m shippingservice-5d97bddcdd-zvwbr         1/1     Running    0                55m ```  ### Logs   ``` rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing: dial tcp 34.118.227.51:7000: i/o timeout"" could not retrieve currencies main.(*frontendServer).homeHandler 	/src/handlers.go:64 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 github.com/gorilla/mux.(*Router).ServeHTTP 	/go/pkg/mod/github.com/gorilla/mux@v1.8.1/mux.go:212 main.(*logHandler).ServeHTTP 	/src/middleware.go:82 main.main.ensureSessionID.func4 	/src/middleware.go:109 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:218 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1 	/go/pkg/mod/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp@v0.53.0/handler.go:74 net/http.HandlerFunc.ServeHTTP 	/usr/local/go/src/net/http/server.go:2171 net/http.serverHandler.ServeHTTP 	/usr/local/go/src/net/http/server.go:3142 net/http.(*conn).serve 	/usr/local/go/src/net/http/server.go:2044 runtime.goexit 	/usr/local/go/src/runtime/asm_amd64.s:1695 ```  ``` $ k logs loadgenerator-7995465594-k7nph -c frontend-check -n microservices-demo + MAX_RETRIES=12 + RETRY_INTERVAL=10 + seq 1 12 + echo 'Attempt 1: Pinging frontend: frontend:80...' Attempt 1: Pinging frontend: frontend:80... + awk '/^  HTTP/{print $2}' + wget --server-response http://frontend:80 + STATUSCODE=500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' + sleep 10 ... + echo 'Attempt 9: Pinging frontend: frontend:80...' Attempt 9: Pinging frontend: frontend:80... + + awk '/^  HTTP/{print $2}' wget --server-response http://frontend:80 + STATUSCODE=500 Error: Could not reach frontend - Status code: 500 + '[' 500 -eq 200 ] + echo 'Error: Could not reach frontend - Status code: 500' ```  ### Screenshots  ![online-boutique-gke](https://github.com/user-attachments/assets/b1515906-62be-4bad-9c9c-ce6dc4e0fd24)  ### Environment  GKE version `v1.29.7-gke.1174000`  ### Additional context  <!-- Add any other context about the problem here -->  ### Exposure  <!-- Is the bug intermittent, persistent? Is it widespread, local? --> ",2024-08-13T19:53:42+00:00,2024-08-14T21:36:31+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2677,2429.0,2024-03-18T01:50:38+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2429,4,0,0,4,44,21,65,0,-3570.051111111111,,True,False,critical,configuration,"[{""filename"": ""src/currencyservice/package-lock.json"", ""lines_added"": 16, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""src/currencyservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package-lock.json"", ""lines_added"": 26, ""lines_deleted"": 12, ""file_type"": ""config""}, {""filename"": ""src/paymentservice/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",src,False
COVESA/vsomeip,833,[BUG]: <title>ubuntu-latest in workflow will update to ubuntu-24.04,"### vSomeip Version  v3.4.10  ### Boost Version  1.74  ### Environment  Github workflow (Ubuntu 24.04)  ### Describe the bug  This is related to #688   The workflow used has the runner set as ubuntu-latest. The last run automatically used ubuntu 22.04, but will slowly upgrade to ubuntu 24.04 in this rollout: https://github.com/actions/runner-images/issues/10636  We have observed similar failures while building vsome ip in [eclipse-uprotocol](https://github.com/eclipse-uprotocol/up-transport-vsomeip-rust) on our workflow. We have downgraded to ubuntu 22.04 as a temporary fix.  ",2025-01-08T16:46:55+00:00,2025-01-20T15:45:34+00:00,0,https://github.com/COVESA/vsomeip/issues/833,834.0,2025-01-20T15:45:33+00:00,https://github.com/COVESA/vsomeip/pull/834,1,0,2,3,5,2,2,0,286.9772222222222,bug,True,False,normal,ui,"[{""filename"": "".github/workflows/c-cpp.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""Dockerfile"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""README.md"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
aws-samples/aws-microservices-deploy-options,146,Lambda functions are failing on AWS,"Invoking the Lambda function on AWS is giving the following error:  ``` lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='GreetingApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""}lambda $ curl `aws cloudformation \\ >   describe-stacks \\ >   --stack-name aws-microservices-deploy-options-lambda \\ >   --query ""Stacks[].Outputs[?OutputKey=='NamesApiEndpoint'].[OutputValue]"" \\ >   --output text` {""message"": ""Internal server error""} ``` ",2018-03-30T21:14:38+00:00,2018-03-31T00:10:05+00:00,1,https://github.com/aws-samples/aws-microservices-deploy-options/issues/146,152.0,2018-03-31T00:10:05+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/152,4,0,0,4,82,140,222,0,2.924166666666667,lambda,True,False,normal,database,"[{""filename"": ""services/greeting/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 39, ""file_type"": ""config""}, {""filename"": ""services/name/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 45, ""file_type"": ""config""}, {""filename"": ""services/pom.xml"", ""lines_added"": 82, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": ""services/webapp/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 49, ""file_type"": ""config""}]",services,False
spring-petclinic/spring-petclinic-microservices,127,openzipkin/zipkin startup error,It appears that the openzipkin/zipkin docker image in the docker hub was upgraded to Java 11 and the -XX:+UseCGroupMemoryLimitForHeap is no longer a valid option.  I believe it just needs to be removed.  The startup error is  ``` tracing-server       | Unrecognized VM option 'UseCGroupMemoryLimitForHeap' tracing-server       | Error: Could not create the Java Virtual Machine. tracing-server       | Error: A fatal exception has occurred. Program will exit. ```,2019-04-23T01:25:19+00:00,2019-04-23T05:21:35+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/127,225.0,2023-07-03T18:19:09+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/225,4,0,0,4,48,0,48,0,36784.89722222222,enhancement,True,False,normal,performance,"[{""filename"": ""spring-petclinic-api-gateway/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-customers-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-vets-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-petclinic-visits-service/pom.xml"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""config""}]",spring-petclinic-customers-service;spring-petclinic-visits-service;spring-petclinic-vets-service;spring-petclinic-api-gateway,True
GoogleCloudPlatform/microservices-demo,2511,Adservice crashing with segfault,"### Describe the bug  Unable to build / deploy on Windows11  ### To Reproduce  ``` git clone https://github.com/GoogleCloudPlatform/microservices-demo cd microservices-demo/ minikube start --cpus=4 --memory 4096 --disk-size 32g skaffold run ``` ### Logs   ``` Waiting for deployments to stabilize...  - deployment/checkoutservice is ready. [10/11 deployment(s) still pending]  - deployment/adservice: container server terminated with exit code 139     - pod/adservice-7cccc9b6fc-m7dvh: container server terminated with exit code 139       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Can not save log file, dump to screen..       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # A fatal error has been detected by the Java Runtime Environment:       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  SIGSEGV (0xb) at pc=0x00007f0e51906522, pid=1, tid=16       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # JRE version:  (21.0.2+13) (build )       > [adservice-7cccc9b6fc-m7dvh server] # Java VM: OpenJDK 64-Bit Server VM (21.0.2+13-LTS, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, serial gc, linux-amd64)       > [adservice-7cccc9b6fc-m7dvh server] # Problematic frame:       > [adservice-7cccc9b6fc-m7dvh server] # C  [profiler_java_agent.so+0x897522]  std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)+0xc       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # Core dump will be written. Default location: /mnt/wslg/dumps/core.%e.1       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] # ...       > [adservice-7cccc9b6fc-m7dvh server] /sys/kernel/mm/transparent_hugepage/defrag (defrag/compaction efforts parameter): always defer defer+madvise [madvise] never       > [adservice-7cccc9b6fc-m7dvh server] Process Memory:       > [adservice-7cccc9b6fc-m7dvh server] Virtual Size: 146444K (peak: 146444K)       > [adservice-7cccc9b6fc-m7dvh server] Resident Set Size: 22436K (peak: 22436K) (anon: 13352K, file: 9084K, shmem: 0K)       > [adservice-7cccc9b6fc-m7dvh server] Swapped out: 0K       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/threads-max (system-wide limit on the number of threads): 126749       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/vm/max_map_count (maximum number of memory map areas a process may have): 262144       > [adservice-7cccc9b6fc-m7dvh server] /proc/sys/kernel/pid_max (system-wide limit on number of process identifiers): 4194304       > [adservice-7cccc9b6fc-m7dvh server] container (cgroup) information:       > [adservice-7cccc9b6fc-m7dvh server] container_type: cgroupv1       > [adservice-7cccc9b6fc-m7dvh server] cpu_cpuset_cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] cpu_memory_nodes: 0       > [adservice-7cccc9b6fc-m7dvh server] active_processor_count: 1       > [adservice-7cccc9b6fc-m7dvh server] cpu_quota: 30000       > [adservice-7cccc9b6fc-m7dvh server] cpu_period: 100000       > [adservice-7cccc9b6fc-m7dvh server] cpu_shares: 204       > [adservice-7cccc9b6fc-m7dvh server] memory_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_and_swap_limit_in_bytes: 307200 k       > [adservice-7cccc9b6fc-m7dvh server] memory_soft_limit_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] memory_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] memory_max_usage_in_bytes: 15000 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_usage_in_bytes: 1352 k       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_max_usage_in_bytes: unlimited       > [adservice-7cccc9b6fc-m7dvh server] kernel_memory_limit_in_bytes: 1436 k       > [adservice-7cccc9b6fc-m7dvh server] maximum number of tasks: unlimited       > [adservice-7cccc9b6fc-m7dvh server] current number of tasks: 2       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks since vm start: 0       > [adservice-7cccc9b6fc-m7dvh server] Steal ticks percentage since vm start:  0.000       > [adservice-7cccc9b6fc-m7dvh server] CPU: total 8 (initial active 1)       > [adservice-7cccc9b6fc-m7dvh server] CPU Model and flags from /proc/cpuinfo:       > [adservice-7cccc9b6fc-m7dvh server] model name  : 11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz       > [adservice-7cccc9b6fc-m7dvh server] flags               : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid movdiri movdir64b fsrm avx512_vp2intersect md_clear flush_l1d arch_capabilities       > [adservice-7cccc9b6fc-m7dvh server] Online cpus: 0-7       > [adservice-7cccc9b6fc-m7dvh server] Offline cpus:       > [adservice-7cccc9b6fc-m7dvh server] BIOS frequency limitation: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Frequency switch latency (ns): <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Available cpu frequencies: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Current governor: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Core performance/turbo boost: <Not Available>       > [adservice-7cccc9b6fc-m7dvh server] Memory: 4k page, physical 307200k(292200k free), swap 4194304k(4194304k free)       > [adservice-7cccc9b6fc-m7dvh server] Page Sizes: 4k       > [adservice-7cccc9b6fc-m7dvh server] vm_info: OpenJDK 64-Bit Server VM (21.0.2+13-LTS) for linux-amd64-musl JRE (21.0.2+13-LTS), built on 2024-01-16T00:00:00Z by ""admin"" with gcc 10.3.1 20211027       > [adservice-7cccc9b6fc-m7dvh server] END.       > [adservice-7cccc9b6fc-m7dvh server] #       > [adservice-7cccc9b6fc-m7dvh server] #  - deployment/adservice failed. Error: container server terminated with exit code 139. I0426 14:29:44.134566   28800 request.go:697] Waited for 1.0548337s due to client-side throttling, not priority and fairness, request: GET:https://127.0.0.1:63092/apis/apps/v1/namespaces/default/replicasets?labelSelector=app%3Dshippingservice 1/11 deployment(s) failed ``` ### Screenshots  ![capture.png](https://github.com/GoogleCloudPlatform/microservices-demo/assets/60648141/de7d71e9-c1ab-42ab-be45-ef6708e2c0ad)  ### Environment  OS: Windows 11 Enterprise 23H2 Kubernetes distribution, version: minikube version: v1.33.0  relevant tool version: Docker Desktop 4.29.0, skaffold: v2.11.0 ",2024-04-26T06:50:08+00:00,2024-05-02T18:14:16+00:00,6,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2511,2568.0,2024-05-30T21:42:57+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2568,0,0,1,1,4,4,0,0,830.8802777777778,type: bug;priority: p1,True,False,critical,ui,"[{""filename"": ""src/cartservice/src/Dockerfile"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}]",src,False
aws-samples/aws-microservices-deploy-options,53,ECS cluster creation failed using CloudFormation template,"Creating an ECS cluster using CloudFormation gives the following error:   ``` Embedded stack arn:aws:cloudformation:us-east-1:091144949931:stack/aws-compute-options-ecs-Infrastructure-AHCL2W4UVW4I/8e090760-2312-11e8-969a-500c288f18d1 was not successfully created: The following resource(s) failed to create: [Route2, ALBPublic, PublicLaunchConfiguration, ALBPrivate, NAT1]. ```  Here is the snapshot:  <img width=""1601"" alt=""screen shot 2018-03-08 at 1 05 40 pm"" src=""https://user-images.githubusercontent.com/113947/37176518-720d7618-22d1-11e8-961d-0184f644f03e.png"">",2018-03-08T21:06:14+00:00,2018-03-09T18:31:18+00:00,4,https://github.com/aws-samples/aws-microservices-deploy-options/issues/53,61.0,2018-03-09T18:06:57+00:00,https://github.com/aws-samples/aws-microservices-deploy-options/pull/61,2,0,0,2,2,2,4,0,21.011944444444445,,True,False,normal,configuration,"[{""filename"": ""apps/ecs/ec2/templates/master.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""apps/ecs/ec2/templates/webapp.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",apps,False
GoogleCloudPlatform/microservices-demo,1870,"Load testing using Locust failing with 2,000+ users","Hi @NimJay   ### Describe the bug  I tested the Hipster Shop performance on **AWS, GCP** and **Azure** instances using **Locust** on a **minikube cluster.** Earlier I built the Docker images for Arm64 with the `grpc-health-probe` binary used in the Dockerfile and tested the same by deploying the Hipster Shop or performing testing using **Locust**. For **Arm64** I can test the load up to **6500** users without any failures but for **x86_64** I was getting failures from **2000** users.    Now the `grpc-health-probe` binary is no longer necessary because in Kubernetes-1.24 the gRPC health-check probes functionality is built into Kubernetes. I've rebuilt the Arm64 Docker images without the `grpc-health-probe` binary and updated the [kubernetes-manifests.yaml](https://github.com/odidev/microservices-demo/blob/main/release/kubernetes-manifests.yaml) with the images I built. Deployment works successfully, and the UI is accessible. However, I'm getting failures during load testing using **Locust** due to **port forwarding** failing to handle requests from **2000** users on both **Arm64** and **x86_64** architectures.  Due to the latest changes, I am blocked. Could you please share some pointers for the same?    ### To Reproduce  Steps to reproduce the behavior: Tested the locust load by taking a master node and 11 worker nodes to generate the load perfectly by deploying the app on a minikube cluster. 1. Ran command `locust -f locustfile.py --master` and `locust -f locustfile.py --worker --master-host=localhost`   ### Logs   [load test result at 2000 user.pdf](https://github.com/GoogleCloudPlatform/microservices-demo/files/11891962/load.test.result.at.2000.user.pdf)  ### Screenshots  During load testing port forwarding failing to handle requests from 2000 or more users  ![image](https://github.com/GoogleCloudPlatform/microservices-demo/assets/40816837/894b491c-5f51-4f14-8e28-d8296d2f8520)  ### Environment  - OS:  Ubuntu 22.04.2 LTS - Kubernetes distribution, version: minikube - Any relevant tool version: Locust 2.15.1   ",2023-06-28T08:39:48+00:00,2024-04-18T19:24:44+00:00,7,https://github.com/GoogleCloudPlatform/microservices-demo/issues/1870,2844.0,2024-12-31T22:51:35+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2844,0,0,1,1,1,1,0,0,13262.19638888889,type: question;priority: p3,True,False,normal,configuration,"[{""filename"": ""src/shoppingassistantservice/requirements.txt"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
GoogleCloudPlatform/microservices-demo,2337,skaffold run gives error while checking cache for one service on Linux Droplet.,"root@test-s-4vcpu-8gb-blr1:~/microservices-demo# kubectl get nodes NAME       STATUS   ROLES           AGE    VERSION minikube   Ready    control-plane   138d   v1.27.3 root@test-s-4vcpu-8gb-blr1:~/microservices-demo# skaffold run Generating tags...  - adservice -> adservice:v0.5.0-528-g46aa585-dirty  - cartservice -> cartservice:v0.5.0-528-g46aa585  - checkoutservice -> checkoutservice:v0.5.0-528-g46aa585  - currencyservice -> currencyservice:v0.5.0-528-g46aa585  - emailservice -> emailservice:v0.5.0-528-g46aa585  - frontend -> frontend:v0.5.0-528-g46aa585  - loadgenerator -> loadgenerator:v0.5.0-528-g46aa585  - paymentservice -> paymentservice:v0.5.0-528-g46aa585  - productcatalogservice -> productcatalogservice:v0.5.0-528-g46aa585  - recommendationservice -> recommendationservice:v0.5.0-528-g46aa585  - shippingservice -> shippingservice:v0.5.0-528-g46aa585 ###Checking cache...  ###- adservice: Error checking cache. ###getting hash for artifact ""adservice"": getting dependencies for ""adservice"": file pattern [../build.gradle ../gradlew] must ###match at least one file  I want to deploy microservices-demo on a Digital ocean Linux VM , but @NimJay here it shows a error mentioned above while executing skaffold run, why is that , kindly make me understand and this and share your advice about to proceed further.",2024-01-16T17:40:25+00:00,2024-04-17T21:44:00+00:00,2,https://github.com/GoogleCloudPlatform/microservices-demo/issues/2337,2391.0,2024-02-26T13:02:03+00:00,https://github.com/GoogleCloudPlatform/microservices-demo/pull/2391,0,0,2,2,3,3,0,0,979.3605555555556,,True,False,normal,ui,"[{""filename"": ""src/cartservice/src/cartservice.csproj"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/cartservice/tests/cartservice.tests.csproj"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",src,False
spring-petclinic/spring-petclinic-microservices,37,Owner details failed when visits-service is down,"We could improve the VisitsServiceClient class by adding a fallback when visits-service is down (maybe with @HystrixCommand). The owner details will not display visits. Here is the stacktrace: ``` 2017-01-14 14:18:52.427 ERROR [api-gateway,,,] 20763 --- [nio-8080-exec-2] o.a.c.c.C.[.[.[/].[dispatcherServlet]    : Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Request processing failed; nested exception is java.lang.IllegalStateException: No instances available for visits-service] with root cause  java.lang.IllegalStateException: No instances available for visits-service 	at org.springframework.cloud.netflix.ribbon.RibbonLoadBalancerClient.execute(RibbonLoadBalancerClient.java:90) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:60) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor$1.doWithRetry(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:276) 	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:157) 	at org.springframework.cloud.client.loadbalancer.RetryLoadBalancerInterceptor.intercept(RetryLoadBalancerInterceptor.java:48) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.response(TraceRestTemplateInterceptor.java:59) 	at org.springframework.cloud.sleuth.instrument.web.client.TraceRestTemplateInterceptor.intercept(TraceRestTemplateInterceptor.java:53) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.cloud.netflix.metrics.MetricsClientHttpRequestInterceptor.intercept(MetricsClientHttpRequestInterceptor.java:68) 	at org.springframework.http.client.InterceptingClientHttpRequest$InterceptingRequestExecution.execute(InterceptingClientHttpRequest.java:85) 	at org.springframework.http.client.InterceptingClientHttpRequest.executeInternal(InterceptingClientHttpRequest.java:69) 	at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48) 	at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:53) 	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:619) 	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:580) 	at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:526) 	at org.springframework.web.client.RestTemplate$$FastClassBySpringCGLIB$$aa4e9ed0.invoke(<generated>) 	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) 	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) 	at org.springframework.aop.aspectj.MethodInvocationProceedingJoinPoint.proceed(MethodInvocationProceedingJoinPoint.java:85) 	at org.springframework.cloud.netflix.metrics.RestTemplateUrlTemplateCapturingAspect.captureUrlTemplate(RestTemplateUrlTemplateCapturingAspect.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethodWithGivenArgs(AbstractAspectJAdvice.java:629) 	at org.springframework.aop.aspectj.AbstractAspectJAdvice.invokeAdviceMethod(AbstractAspectJAdvice.java:618) 	at org.springframework.aop.aspectj.AspectJAroundAdvice.invoke(AspectJAroundAdvice.java:70) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92) 	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) 	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) 	at org.springframework.web.client.RestTemplate$$EnhancerBySpringCGLIB$$d5d328a8.exchange(<generated>) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.lambda$getVisitsForPets$1(VisitsServiceClient.java:29) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient$$Lambda$13/1446308391.apply(Unknown Source) 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267) 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512) 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:747) 	at java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:721) 	at java.util.stream.AbstractTask.compute(AbstractTask.java:316) 	at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731) 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) 	at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:400) 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:728) 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714) 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233) 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) 	at org.springframework.samples.petclinic.api.application.VisitsServiceClient.getVisitsForPets(VisitsServiceClient.java:31) 	at org.springframework.samples.petclinic.api.boundary.web.ApiGatewayController.getOwnerDetails(ApiGatewayController.java:33) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:220) 	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:134) 	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:116) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) 	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) 	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) 	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) 	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) 	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) 	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:622) 	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:105) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:89) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:141) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:107) 	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:192) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:198) 	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:108) 	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472) 	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:140) 	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) 	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:87) 	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:349) 	at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:784) 	at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:66) 	at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:802) 	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1410) 	at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) 	at java.lang.Thread.run(Thread.java:745) ```",2017-01-14T13:21:04+00:00,2017-01-14T15:11:21+00:00,2,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/37,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15332.396666666667,enhancement,True,False,normal,configuration,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
nestjs/nest,14738,ElasticSearch's ResponseError (and others) gets misinterpreted as custom HTTP exception,"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  ElasticSearch's [Node.js SDK](https://www.npmjs.com/package/@elastic/elasticsearch) [happens to throw errors](https://github.com/elastic/elastic-transport-js/blob/b44dd134cc9bf1fbca5204851ec38030cdfd06e9/src/errors.ts#L162) with `statusCode` getter, which get misinterpreted as HTTP response errors the app generates.  From [the docs](https://docs.nestjs.com/exception-filters):  >  The global exception filter partially supports the http-errors library. Basically, any thrown exception containing the statusCode and message properties will be properly populated and sent back as a response (instead of the default InternalServerErrorException for unrecognized exceptions).  > NOTE: in the docs this section is displayed as a hint and not a warning  As a user of Nest.js this behavior was never intended and not even known about. Docs state that for custom errors docs we should extend `HttpException` class.  #### Note on impact in our case  Instead of throwing a 500 error and notifying developers of a bug, customers of an API received a response with status 400 and ElasticSearch's `ResponseError` internals which they are not supposed to ever see.  ### Minimum reproduction code  see `steps to reproduce`  ### Steps to reproduce  The minimal reproduction will be:  ```js throw { message: ""..."", statusCode: 400 }; ```  ### Expected behavior  There is no way to know for sure without inspecting the code of all dependencies what any specific function could throw in JavaScript.  I suggest that the default exception filter should be modified to treat all caught errors that are not a subclass of `HttpException` as 500 errors.  I know that this would be a breaking change and this could technically be solved with a custom exception filter (or interceptor) but I'm pretty sure that additional safety of most users overweight the needs of those who don't want to subclass `HttpException` class in this case.  ### Package  - [ ] I don't know. Or some 3rd-party package - [x] <code>@nestjs/common</code> - [x] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [ ] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  10.4.x  ### Packages versions  ```json ""@nestjs/common"": ""^10.4.6"", ""@nestjs/core"": ""^10.4.6"", ""@nestjs/microservices"": ""^10.4.6"", ""@nestjs/platform-express"": ""^10.4.6"", ""@nestjs/swagger"": ""^7.4.2"", ""@nestjs/terminus"": ""^10.2.3"", ```   ### Node.js version  20.x  ### In which operating systems have you tested?  - [ ] macOS - [ ] Windows - [x] Linux  ### Other  _No response_",2025-03-03T21:58:37+00:00,2025-03-04T08:17:17+00:00,3,https://github.com/nestjs/nest/issues/14738,13784.0,2024-07-15T08:10:25+00:00,https://github.com/nestjs/nest/pull/13784,2,0,0,2,5,5,10,0,-5557.803333333333,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
spring-petclinic/spring-petclinic-microservices,31,Services cannot contact with Zipkin server,Zipkin server does not see any traces nor dependencies. Logs are not being pushed to Zipkin.  Reproducible when starting services locally as well as with Docker compose.,2017-01-07T17:25:04+00:00,2017-01-09T07:27:05+00:00,0,https://github.com/spring-petclinic/spring-petclinic-microservices/issues/31,107.0,2018-10-15T09:44:52+00:00,https://github.com/spring-petclinic/spring-petclinic-microservices/pull/107,2,0,2,4,27,44,56,0,15496.33,bug,True,False,normal,functional,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""docker-compose.yml"", ""lines_added"": 16, ""lines_deleted"": 38, ""file_type"": ""config""}, {""filename"": ""docker/Dockerfile"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""pom.xml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
nestjs/nest,14735,Dev server runs even if the build fails TSC (with SWC),"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  https://stackoverflow.com/questions/76859548/how-to-force-nestjs-not-to-start-on-the-type-check-error  ### Minimum reproduction code  N/A  ### Steps to reproduce  1. create a starter project 2. switch builder to `swc` and set `typeCheck` as `true` in `nest-cli.json` 3. create a random TS error 4. run `npm run start:dev`  ### Expected behavior  Build fails on TSC error and dev server doesn't start, same behaviour as with the `tsc` builder  ### Package  - [ ] I don't know. Or some 3rd-party package - [ ] <code>@nestjs/common</code> - [x] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [ ] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  _No response_  ### Packages versions  ```json     ""@nestjs/common"": ""^11"",     ""@nestjs/config"": ""^4"",     ""@nestjs/core"": ""^11"",     ""@nestjs/platform-fastify"": ""^11"", ```   ### Node.js version  _No response_  ### In which operating systems have you tested?  - [ ] macOS - [ ] Windows - [x] Linux  ### Other  _No response_",2025-03-03T15:56:08+00:00,2025-03-04T08:07:36+00:00,3,https://github.com/nestjs/nest/issues/14735,13784.0,2024-07-15T08:10:25+00:00,https://github.com/nestjs/nest/pull/13784,2,0,0,2,5,5,10,0,-5551.761944444444,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,14723,[ExceptionsHandler] TypeError: Missing parameter name at 5: https://git.new/pathToRegexpError,"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  I am using MVC pattern with Nestjs but keep getting this issue -   ``` [Nest] 7686  - 03/02/2025, 11:41:37 AM   ERROR [ExceptionsHandler] TypeError: Missing parameter name at 5: https://git.new/pathToRegexpError     at name (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:153:13)     at lexer (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:171:21)     at lexer.next (<anonymous>)     at Iter.peek (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:188:32)     at Iter.tryConsume (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:195:24)     at Iter.text (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:213:26)     at consume (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:285:23)     at parse (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:320:18)     at <anonymous> (/Users/shreymalhotra/Documents/NotEnough/AppBuilder/dev/AppWorld/server/automation-backend/node_modules/path-to-regexp/src/index.ts:503:40)     at Array.map (<anonymous>)  ```   ### Minimum reproduction code  NA  ### Steps to reproduce  ``` ├── @nestjs/common@11.0.11 ├─┬ @nestjs/config@4.0.0 │ └── @nestjs/common@11.0.11 deduped ├─┬ @nestjs/core@11.0.11 │ └── @nestjs/common@11.0.11 deduped ├─┬ @nestjs/jwt@11.0.0 │ └── @nestjs/common@11.0.11 deduped ├─┬ @nestjs/microservices@11.0.11 │ ├── @nestjs/common@11.0.11 deduped │ └── @nestjs/core@11.0.11 deduped ├─┬ @nestjs/passport@11.0.5 │ └── @nestjs/common@11.0.11 deduped ├─┬ @nestjs/platform-express@11.0.11 │ ├── @nestjs/common@11.0.11 deduped │ └── @nestjs/core@11.0.11 deduped ├─┬ @nestjs/serve-static@5.0.3 │ ├── @nestjs/common@11.0.11 deduped │ └── @nestjs/core@11.0.11 deduped ├─┬ @nestjs/swagger@11.0.6 │ ├── @nestjs/common@11.0.11 deduped │ ├── @nestjs/core@11.0.11 deduped │ └─┬ @nestjs/mapped-types@2.1.0 │   └── @nestjs/common@11.0.11 deduped ├─┬ @nestjs/testing@11.0.11 │ ├── @nestjs/common@11.0.11 deduped │ └── @nestjs/core@11.0.11 deduped └─┬ @nestjs/typeorm@11.0.0   ├── @nestjs/common@11.0.11 deduped   └── @nestjs/core@11.0.11 deduped ```  ### Expected behavior  Should be able to navigate routes without pathToRegexpError   ### Package  - [x] I don't know. Or some 3rd-party package - [ ] <code>@nestjs/common</code> - [ ] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [x] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  11.0.x  ### Packages versions  ``` ""dependencies"": {     ""@grpc/grpc-js"": ""^1.12.6"",     ""@grpc/proto-loader"": ""^0.7.13"",     ""@nestjs/common"": ""^11.0.0"",     ""@nestjs/config"": ""^4.0.0"",     ""@nestjs/core"": ""^11.0.0"",     ""@nestjs/jwt"": ""^11.0.0"",     ""@nestjs/passport"": ""^11.0.0"",     ""@nestjs/platform-express"": ""^11.0.0"",     ""@nestjs/serve-static"": ""^5.0.3"",     ""@nestjs/swagger"": ""^11.0.0"",     ""@nestjs/typeorm"": ""^11.0.0"",     ""axios"": ""^1.8.0"",     ""bcrypt"": ""^5.1.1"",     ""class-transformer"": ""^0.5.1"",     ""class-validator"": ""^0.14.1"",     ""cookie-parser"": ""^1.4.6"",     ""dotenv"": ""^16.4.7"",     ""express-session"": ""^1.18.1"",     ""hbs"": ""^4.2.0"",     ""passport"": ""^0.7.0"",     ""passport-jwt"": ""^4.0.1"",     ""passport-local"": ""^1.0.0"",     ""pg"": ""^8.13.3"",     ""reflect-metadata"": ""^0.2.2"",     ""rxjs"": ""^7.8.1"",     ""typeorm"": ""^0.3.20""   },   ""devDependencies"": {     ""@nestjs/cli"": ""^11.0.0"",     ""@nestjs/microservices"": ""^11.0.0"",     ""@nestjs/schematics"": ""^11.0.0"",     ""@nestjs/testing"": ""^11.0.0"",     ""@types/cookie-parser"": ""^1.4.3"",     ""@types/express"": ""^5.0.0"",     ""@types/express-session"": ""^1.18.1"",     ""@types/jest"": ""^29.5.2"",     ""@types/node"": ""^20.3.1"",     ""@types/passport-local"": ""^1.0.38"",     ""@types/supertest"": ""^6.0.0"",     ""@typescript-eslint/eslint-plugin"": ""^8.1.0"",     ""@typescript-eslint/parser"": ""^8.1.0"",     ""eslint"": ""^8.1.0"",     ""eslint-config-prettier"": ""^9.0.0"",     ""eslint-plugin-prettier"": ""^5.0.0"",     ""grpc_tools_node_protoc_ts"": ""^5.3.3"",     ""grpc-tools"": ""^1.13.0"",     ""jest"": ""^29.5.0"",     ""prettier"": ""^3.0.0"",     ""source-map-support"": ""^0.5.21"",     ""supertest"": ""^7.0.0"",     ""ts-jest"": ""^29.1.0"",     ""ts-loader"": ""^9.4.3"",     ""ts-node"": ""^10.9.1"",     ""ts-proto"": ""^2.6.1"",     ""tsconfig-paths"": ""^4.2.0""   }, ```   ### Node.js version  22.14.0  ### In which operating systems have you tested?  - [x] macOS - [ ] Windows - [ ] Linux  ### Other  Maybe related https://github.com/expressjs/express/issues/6038",2025-03-02T06:17:53+00:00,2025-03-03T09:03:36+00:00,2,https://github.com/nestjs/nest/issues/14723,13768.0,,https://github.com/nestjs/nest/pull/13768,2,0,0,2,5,5,10,0,26.761944444444445,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,14724,NestJS fails to resolve npm workspace packages with TypeScript project references,"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  I'm experiencing an issue with NestJS not properly resolving TypeScript project references when using npm workspaces. While `tsc -b` works correctly, running `nest start` fails to resolve the workspace package.  ### Minimum reproduction code  https://github.com/itizarsa/nestjs-repro  ### Steps to reproduce  1. npm install 2. npm run dev --service=@arshath/connect  ### Expected behavior  NestJS should properly resolve the TypeScript project references, similar to how `tsc -b` does.  ### Package  - [ ] I don't know. Or some 3rd-party package - [x] <code>@nestjs/common</code> - [x] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [ ] <code>@nestjs/platform-express</code> - [x] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  11.0.10  ### Packages versions  ```json  { 	""@nestjs/cli"": ""^11.0.5"", 	""@nestjs/common"": ""^11.0.10"", 	""@nestjs/core"": ""^11.0.10"", 	""@nestjs/platform-fastify"": ""^11.0.10"", 	""reflect-metadata"": ""^0.2.2"", 	""@types/node"": ""^22.13.5"", 	""typescript"": ""^5.7.3"" }   ```   ### Node.js version  22.14.0  ### In which operating systems have you tested?  - [x] macOS - [ ] Windows - [ ] Linux  ### Other  ``` src/app.controller.ts:1:24 - error TS2307: Cannot find module '@arshath/util' or its corresponding type declarations.  1 import { Result } from ""@arshath/util""; ```",2025-03-02T07:13:37+00:00,2025-03-02T11:03:58+00:00,2,https://github.com/nestjs/nest/issues/14724,13784.0,2024-07-15T08:10:25+00:00,https://github.com/nestjs/nest/pull/13784,2,0,0,2,5,5,10,0,-5519.053333333333,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,14646,Running error using bcrypt,"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  ```ts import { Injectable } from '@nestjs/common'; // 这是一个处理密码加密盐HASH和验证密码的库 import * as bcrypt from 'bcrypt';  @Injectable() export class UtilityService {   async hashPassword(password: string): Promise<string> {     // 生成一个盐值,用于增强哈希的案例性     const salt = await bcrypt.genSalt();     // 使用生成的盐值对密码进行哈希,并返回哈希结果     return bcrypt.hash(password, salt);   }    // 验证密码   async comparePassword(password: string, hash: string): Promise<boolean> {     // 比较密码和hash值是否匹配, true || false     return bcrypt.compare(password, hash);   } } ```  ```shell [16:12:07] Starting compilation in watch mode...  [16:12:10] Found 0 errors. Watching for file changes.  node:internal/modules/cjs/loader:1247   throw err;   ^  Error: Cannot find module 'C:\\studyFiles\\nest-handlebars-study\\node_modules\\.pnpm\\bcrypt@5.1.1\\node_modules\\bcrypt\\lib\\binding\\napi-v3\\bcrypt_lib.node' Require stack: - C:\\studyFiles\\nest-handlebars-study\\node_modules\\.pnpm\\bcrypt@5.1.1\\node_modules\\bcrypt\\bcrypt.js - C:\\studyFiles\\nest-handlebars-study\\dist\\shared\\services\\utility.service.js - C:\\studyFiles\\nest-handlebars-study\\dist\\admin\\controller\\user.controller.js - C:\\studyFiles\\nest-handlebars-study\\dist\\admin\\admin.module.js - C:\\studyFiles\\nest-handlebars-study\\dist\\app.module.js - C:\\studyFiles\\nest-handlebars-study\\dist\\main.js     at Function._resolveFilename (node:internal/modules/cjs/loader:1244:15)     at Function._load (node:internal/modules/cjs/loader:1070:27)     at TracingChannel.traceSync (node:diagnostics_channel:322:14)     at wrapModuleLoad (node:internal/modules/cjs/loader:217:24)     at Module.require (node:internal/modules/cjs/loader:1335:12)     at require (node:internal/modules/helpers:136:16)     at Object.<anonymous> (C:\\studyFiles\\nest-handlebars-study\\node_modules\\.pnpm\\bcrypt@5.1.1\\node_modules\\bcrypt\\bcrypt.js:6:16)     at Module._compile (node:internal/modules/cjs/loader:1562:14)     at Object..js (node:internal/modules/cjs/loader:1699:10)     at Module.load (node:internal/modules/cjs/loader:1313:32) {   code: 'MODULE_NOT_FOUND',   requireStack: [     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\node_modules\\\\.pnpm\\\\bcrypt@5.1.1\\\\node_modules\\\\bcrypt\\\\bcrypt.js',     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\dist\\\\shared\\\\services\\\\utility.service.js',     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\dist\\\\admin\\\\controller\\\\user.controller.js',     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\dist\\\\admin\\\\admin.module.js',     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\dist\\\\app.module.js',     'C:\\\\studyFiles\\\\nest-handlebars-study\\\\dist\\\\main.js'   ] }  Node.js v22.13.0  进程已结束，退出代码为 0 ```  ### Minimum reproduction code  https://github.com/MC-YCY/nestjs-hbs/blob/master/src/shared/services/utility.service.ts  ### Steps to reproduce  ```ts import { Injectable } from '@nestjs/common'; // 这是一个处理密码加密盐HASH和验证密码的库 import * as bcrypt from 'bcrypt';  @Injectable() export class UtilityService {   async hashPassword(password: string): Promise<string> {     // 生成一个盐值,用于增强哈希的案例性     const salt = await bcrypt.genSalt();     // 使用生成的盐值对密码进行哈希,并返回哈希结果     return bcrypt.hash(password, salt);   }    // 验证密码   async comparePassword(password: string, hash: string): Promise<boolean> {     // 比较密码和hash值是否匹配, true || false     return bcrypt.compare(password, hash);   } } ```  ### Expected behavior  正常运行  ### Package  - [ ] I don't know. Or some 3rd-party package - [x] <code>@nestjs/common</code> - [x] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [x] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [x] Other (see below)  ### Other package  bcrypt  ### NestJS version  10.0.0  ### Packages versions  ```json {   ""name"": ""handlebars"",   ""version"": ""0.0.1"",   ""description"": """",   ""author"": """",   ""private"": true,   ""license"": ""UNLICENSED"",   ""scripts"": {     ""build"": ""nest build"",     ""format"": ""prettier --write \\""src/**/*.ts\\"" \\""test/**/*.ts\\"""",     ""start"": ""nest start"",     ""start:dev"": ""nest start --watch"",     ""start:debug"": ""nest start --debug --watch"",     ""start:prod"": ""node dist/main"",     ""lint"": ""eslint \\""{src,apps,libs,test}/**/*.ts\\"" --fix"",     ""test"": ""jest"",     ""test:watch"": ""jest --watch"",     ""test:cov"": ""jest --coverage"",     ""test:debug"": ""node --inspect-brk -r tsconfig-paths/register -r ts-node/register node_modules/.bin/jest --runInBand"",     ""test:e2e"": ""jest --config ./test/jest-e2e.json""   },   ""dependencies"": {     ""@nestjs/common"": ""^10.0.0"",     ""@nestjs/config"": ""^3.3.0"",     ""@nestjs/core"": ""^10.0.0"",     ""@nestjs/mapped-types"": ""^2.1.0"",     ""@nestjs/platform-express"": ""^10.4.15"",     ""@nestjs/swagger"": ""^11.0.3"",     ""@nestjs/typeorm"": ""^10.0.2"",     ""bcrypt"": ""^5.1.1"",     ""class-transformer"": ""^0.5.1"",     ""class-validator"": ""^0.14.1"",     ""cookie-parser"": ""^1.4.7"",     ""express"": ""^4.21.2"",     ""express-handlebars"": ""^8.0.1"",     ""express-session"": ""^1.18.1"",     ""handlebars"": ""^4.7.8"",     ""moment"": ""^2.30.1"",     ""mysql2"": ""^3.12.0"",     ""nestjs-i18n"": ""^10.5.0"",     ""nodemon"": ""^3.1.9"",     ""reflect-metadata"": ""^0.2.2"",     ""rxjs"": ""^7.8.1"",     ""typeorm"": ""^0.3.20""   },   ""devDependencies"": {     ""@nestjs/cli"": ""^10.0.0"",     ""@nestjs/schematics"": ""^10.0.0"",     ""@nestjs/testing"": ""^10.0.0"",     ""@types/bcrypt"": ""^5.0.2"",     ""@types/cookie-parser"": ""^1.4.8"",     ""@types/express"": ""^5.0.0"",     ""@types/jest"": ""^29.5.2"",     ""@types/node"": ""^20.3.1"",     ""@types/supertest"": ""^6.0.0"",     ""@typescript-eslint/eslint-plugin"": ""^8.0.0"",     ""@typescript-eslint/parser"": ""^8.0.0"",     ""eslint"": ""^8.0.0"",     ""eslint-config-prettier"": ""^9.0.0"",     ""eslint-plugin-prettier"": ""^5.0.0"",     ""jest"": ""^29.5.0"",     ""prettier"": ""^3.0.0"",     ""source-map-support"": ""^0.5.21"",     ""supertest"": ""^7.0.0"",     ""ts-jest"": ""^29.1.0"",     ""ts-loader"": ""^9.4.3"",     ""ts-node"": ""^10.9.1"",     ""tsconfig-paths"": ""^4.2.0"",     ""typescript"": ""^5.1.3""   },   ""jest"": {     ""moduleFileExtensions"": [       ""js"",       ""json"",       ""ts""     ],     ""rootDir"": ""src"",     ""testRegex"": "".*\\\\.spec\\\\.ts$"",     ""transform"": {       ""^.+\\\\.(t|j)s$"": ""ts-jest""     },     ""collectCoverageFrom"": [       ""**/*.(t|j)s""     ],     ""coverageDirectory"": ""../coverage"",     ""testEnvironment"": ""node""   } } ```   ### Node.js version  22.13.0  ### In which operating systems have you tested?  - [ ] macOS - [x] Windows - [ ] Linux  ### Other  _No response_",2025-02-17T08:19:33+00:00,2025-02-17T08:42:23+00:00,1,https://github.com/nestjs/nest/issues/14646,13694.0,2024-06-18T10:51:07+00:00,https://github.com/nestjs/nest/pull/13694,2,0,0,2,5,5,10,0,-5853.473888888889,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,14450,NestJS v11 `@nestjs/platform-fastify` has wrong dependency,"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  When install NestJS v11 with `@nestjs/platform-fastify`, it tries to install `@nestjs/core@10.0.0`, and fails by dependency confliction.  The problem comes from the `@nestjs/platform-fastify`'s `package.json` file. Looking at the `package.json` file of Github repository, there is no problem. However, if I install it through `npm install --force` option and open the `package.json` file from the `node_modules/@nestjs/platform-fastify/package.json` file, its actual `peerDependencies` has a critical problem, depending on the previous major version like below.  Here is the Github repository's `peerDependencies`. As you can see, it is proper.  https://github.com/nestjs/nest/blob/e63a2c1f7c5c552c7cfb0af43500e02780087e8e/packages/platform-fastify/package.json#L29-L34  However, the NPM published `peerDependencies` is not proper. It requires previous major version of `@nestjs/core`.  ```json {   ""peerDependencies"": {     ""@fastify/static"": ""^8.0.0"",     ""@fastify/view"": ""^10.0.0"",     ""@nestjs/common"": ""^10.0.0"",     ""@nestjs/core"": ""^10.0.0""   } } ```  ### Minimum reproduction code  https://github.com/samchon/nestjs-v11-platform-fastify-dependency-bug  ### Steps to reproduce  ```bash git clone https://github.com/samchon/nestjs-v11-app-use-fastify-multer-bug npm install npm run build  # WORKING PROPERLY npm run setup:v10 npm run test  # ERROR OCCURES npm run setup:v11 npm run test ```  ### Expected behavior  This bug caused by wrong-publishing.  The `peerDependencies` archived in the Github repository is proper, but NPM module is not proper.  Therefore, `@nestjs/platform-fastify` must be re-published.  ### Package  - [ ] I don't know. Or some 3rd-party package - [x] <code>@nestjs/common</code> - [x] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [ ] <code>@nestjs/platform-express</code> - [x] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  11.0.1  ### Packages versions  ```json {   ""@nestjs/common"": ""^11.0.11"",   ""@nestjs/core"": ""^11.0.1"",   ""@nestjs/platform-fastify"": ""^11.0.1"" } ```   ### Node.js version  20.10.0  ### In which operating systems have you tested?  - [ ] macOS - [x] Windows - [ ] Linux  ### Other  _No response_",2025-01-18T12:55:24+00:00,2025-01-20T08:15:11+00:00,7,https://github.com/nestjs/nest/issues/14450,12940.0,2024-11-27T08:27:18+00:00,https://github.com/nestjs/nest/pull/12940,4,0,0,4,208,204,412,0,-1252.4683333333332,needs triage,True,False,critical,configuration,"[{""filename"": ""sample/06-mongoose/package-lock.json"", ""lines_added"": 103, ""lines_deleted"": 101, ""file_type"": ""config""}, {""filename"": ""sample/06-mongoose/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""sample/14-mongoose-base/package-lock.json"", ""lines_added"": 103, ""lines_deleted"": 101, ""file_type"": ""config""}, {""filename"": ""sample/14-mongoose-base/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,4900,"eslint: Parsing error: ""parserOptions.project"" has been set for @typescript-eslint/parser.","# Here is the fix  https://github.com/nestjs/nest/issues/4900#issuecomment-669743374  ## Bug Report I installed nest and start new project. and there is a problem with eslint: in .eslint.js I got error  ``` Parsing error: ""parserOptions.project"" has been set for @typescript-eslint/parser. The file does not match your project config: .eslintrc.js. The file must be included in at least one of the projects provided. ``` eslint in code also not working, and command `npm run lint` doesn't find any problems.  ## Current behavior  ## Input Code .eslintrc.js  ``` module.exports = {   parser: '@typescript-eslint/parser',   parserOptions: {     sourceType: 'module',     project: 'tsconfig.json'   },   plugins: ['@typescript-eslint/eslint-plugin'],   extends: [     'plugin:@typescript-eslint/eslint-recommended',     'plugin:@typescript-eslint/recommended',     'prettier',     'prettier/@typescript-eslint',   ], ... }; ``` I didn't change anything actually.   ## Environment  <pre><code> Nest version: 7.2.0   For Tooling issues: - Node version: 13.10.1 - Platform:  Linux mint  webstorm, npm 6.13.7. </code></pre>   is it reproduced or is it just me?  ",2020-06-12T14:47:18+00:00,2020-06-15T07:51:06+00:00,23,https://github.com/nestjs/nest/issues/4900,13897.0,2024-08-22T06:41:00+00:00,https://github.com/nestjs/nest/pull/13897,2,0,0,2,5,5,10,0,36759.895,needs triage,True,False,normal,configuration,"[{""filename"": ""package-lock.json"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
nestjs/nest,14348,BaseExceptionFilter applicationRef.isHeadersSent is undefined,"### Is there an existing issue for this?  - [X] I have searched the existing issues  ### Current behavior  In `handleUnknownError`  is `applicationRef` but in catch is `const applicationRef = this.applicationRef ||             (this.httpAdapterHost && this.httpAdapterHost.httpAdapter);`.   And I get `applicationRef.isHeadersSent` is undefined in modified express adapter but it is in `applicationRef._httpAdapter.isHeadersSent`  ``` class BaseExceptionFilter {     constructor(applicationRef) {         this.applicationRef = applicationRef;     }     catch(exception, host) {         const applicationRef = this.applicationRef ||             (this.httpAdapterHost && this.httpAdapterHost.httpAdapter);         if (!(exception instanceof common_1.HttpException)) {             return this.handleUnknownError(exception, host, applicationRef);         }         const res = exception.getResponse();         const message = (0, shared_utils_1.isObject)(res)             ? res             : {                 statusCode: exception.getStatus(),                 message: res,             };         const response = host.getArgByIndex(1);         if (!applicationRef.isHeadersSent(response)) {             applicationRef.reply(response, message, exception.getStatus());         }         else {             applicationRef.end(response);         }     }     handleUnknownError(exception, host, applicationRef) {         console.log(applicationRef)           const body = this.isHttpError(exception)             ? {                 statusCode: exception.statusCode,                 message: exception.message,             }             : {                 statusCode: common_1.HttpStatus.INTERNAL_SERVER_ERROR,                 message: constants_1.MESSAGES.UNKNOWN_EXCEPTION_MESSAGE,             };         const response = host.getArgByIndex(1);         if (!applicationRef.isHeadersSent(response)) {             applicationRef.reply(response, body, body.statusCode);         }         else {             applicationRef.end(response);         }         if (this.isExceptionObject(exception)) {             return BaseExceptionFilter.logger.error(exception.message, exception.stack);         }         return BaseExceptionFilter.logger.error(exception);     }     isExceptionObject(err) {         return (0, shared_utils_1.isObject)(err) && !!err.message;     }     /**      * Checks if the thrown error comes from the ""http-errors"" library.      * @param err error object      */     isHttpError(err) {         return err?.statusCode && err?.message;     } } ```  ### Minimum reproduction code  https://github.com/nestjs  ### Steps to reproduce  _No response_  ### Expected behavior  -  ### Package  - [ ] I don't know. Or some 3rd-party package - [X] <code>@nestjs/common</code> - [X] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [X] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  10  ### Packages versions  --  ### Node.js version  _No response_  ### In which operating systems have you tested?  - [ ] macOS - [ ] Windows - [X] Linux  ### Other  _No response_",2024-12-20T17:12:23+00:00,2024-12-21T08:57:47+00:00,6,https://github.com/nestjs/nest/issues/14348,12940.0,2024-11-27T08:27:18+00:00,https://github.com/nestjs/nest/pull/12940,4,0,0,4,208,204,412,0,-560.7513888888889,needs triage,True,False,normal,functional,"[{""filename"": ""sample/06-mongoose/package-lock.json"", ""lines_added"": 103, ""lines_deleted"": 101, ""file_type"": ""config""}, {""filename"": ""sample/06-mongoose/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""sample/14-mongoose-base/package-lock.json"", ""lines_added"": 103, ""lines_deleted"": 101, ""file_type"": ""config""}, {""filename"": ""sample/14-mongoose-base/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
NVIDIA/GenerativeAIExamples,21,Error message has incorrect model engine name of nemo-infer instead of ai-playground,"RetrievalAugmentedGeneration.common.utils.get_llm() reports the incorrect model_engine name in the last line of the function in error string. Says ""Supported engines are triton-trt-llm and nemo-infer"", but should say ""Supported engines are triton-trt-llm and ai-playground"".  In the config.yaml file, we must specify llm.model_engine as one of [triton-trt-llm, ai-playground]. ",2024-01-02T22:17:32+00:00,2024-04-23T04:29:02+00:00,4,https://github.com/NVIDIA/GenerativeAIExamples/issues/21,217.0,2024-10-14T16:39:41+00:00,https://github.com/NVIDIA/GenerativeAIExamples/pull/217,2,0,0,2,0,10,10,0,6858.369166666666,question,True,False,normal,configuration,"[{""filename"": ""nemo/retriever-synthetic-data-generation/scripts/conf/config-fiqa.yaml"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""config""}, {""filename"": ""nemo/retriever-synthetic-data-generation/scripts/conf/config-nq.yaml"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""config""}]",,False
hashicorp/consul-terraform-sync,475,Set CGO_ENABLED=0,Fixes build for docker images flagged in #474. This doesn't fix the 0.4.0 docker images yet though.,2021-11-01T15:56:09+00:00,2021-11-01T19:21:06+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/475,475.0,2021-11-01T19:21:06+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/475,1,0,0,1,1,0,1,0,3.415833333333333,bug;backport/0.4,True,False,normal,ui,"[{""filename"": "".github/workflows/build.yml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
hashicorp/consul-terraform-sync,270,Adopt new pgp key,"This adopts the new HashiCorp PGP key, as described at https://discuss.hashicorp.com/t/hcsec-2021-12-codecov-security-event-and-hashicorp-gpg-key-exposure/23512/2 and https://hashicorp.com/security.  Please merge after reviewing 🙏 ",2021-04-25T20:33:26+00:00,2021-04-26T14:51:14+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/270,270.0,2021-04-26T14:51:14+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/270,0,0,1,1,1,1,0,0,18.296666666666667,security;backport/0.1,True,False,normal,security,"[{""filename"": ""docker/Dockerfile"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
alibaba/spring-cloud-alibaba,2001,EDAS 应用开启健康检查后应用一直报503 ，NacosException: failed to req API:/api//nacos/v1/ns/service/list,"sca version: 2.2.1  according to the following log, it is always called after health check is enabled, NamingProxy.getServiceList(), causing the server-side throttling to be triggered. Check whether the health check on config maintains a variable. Can this be changed to a similar method?  sca版本：2.2.1 根据下面日志发现开启健康检查后会一直调用，NamingProxy.getServiceList()，导致触发server端限流。查看了config那边的健康检查是维持一个变量。这个是否可以改成类似的方式？ ![image](https://user-images.githubusercontent.com/26211950/111597933-a9377800-8809-11eb-92e5-e403fc733087.png)  错误日志： 2021-03-18 07:46:14.899 ERROR 2319 --- [http-nio-8080-exec-7] c.a.c.n.discovery.NacosDiscoveryClient   : get service name from nacos server fail, com.alibaba.nacos.api.exception.NacosException: failed to req API:/api//nacos/v1/ns/service/list after all servers([100.100.21.44, 100.100.0.46]) tried: <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN""> <html> <head><title>503 Service Temporarily Unavailable</title></head> <body bgcolor=""white""> <h1>503 Service Temporarily Unavailable</h1> <p>The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. Sorry for the inconvenience.<br/> Please report this message and include the following information to us.<br/> Thank you very much!</p> <table> <tr> <td>URL:</td> <td>http://100.100.0.46/nacos/v1/ns/service/list?app=610b7e82-533c-4b36-a8a8-d30a643e0913&groupName=scissor_group&data=1616024774882&namespaceId=xxxxxx-39a4fa97f6b2&signature=xxxxx&pageNo=1&pageSize=2147483647&ak=xxxxx&encoding=UTF-8</td> </tr> <tr> <td>Server:</td> <td>sz.edas.vipserver-011197238145.st3</td> </tr> <tr> <td>Date:</td> <td>2021/03/18 07:46:14</td> </tr> </table> <hr/>Powered by Tengine</body> </html> at com.alibaba.nacos.client.naming.net.NamingProxy.reqAPI(NamingProxy.java:496) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.nacos.client.naming.net.NamingProxy.reqAPI(NamingProxy.java:401) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.nacos.client.naming.net.NamingProxy.reqAPI(NamingProxy.java:397) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.nacos.client.naming.net.NamingProxy.getServiceList(NamingProxy.java:385) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.nacos.client.naming.NacosNamingService.getServicesOfServer(NacosNamingService.java:458) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.nacos.client.naming.NacosNamingService.getServicesOfServer(NacosNamingService.java:447) ~[nacos-client-1.2.1.jar!/:na] at com.alibaba.cloud.nacos.discovery.NacosServiceDiscovery.getServices(NacosServiceDiscovery.java:64) ~[spring-cloud-starter-alibaba-nacos-discovery-2.2.1.RELEASE.jar!/:2.2.1.RELEASE] at com.alibaba.cloud.nacos.discovery.NacosDiscoveryClient.getServices(NacosDiscoveryClient.java:67) ~[spring-cloud-starter-alibaba-nacos-discovery-2.2.1.RELEASE.jar!/:2.2.1.RELEASE] at org.springframework.cloud.client.discovery.composite.CompositeDiscoveryClient.getServices(CompositeDiscoveryClient.java:67) [spring-cloud-commons-2.2.5.RELEASE.jar!/:2.2.5.RELEASE] at org.springframework.cloud.client.discovery.health.DiscoveryClientHealthIndicator.health(DiscoveryClientHealthIndicator.java:69) [spring-cloud-commons-2.2.5.RELEASE.jar!/:2.2.5.RELEASE] at org.springframework.cloud.client.discovery.health.DiscoveryCompositeHealthContributor.lambda$asHealthIndicator$0(DiscoveryCompositeHealthContributor.java:77) [spring-cloud-commons-2.2.5.RELEASE.jar!/:2.2.5.RELEASE] at org.springframework.boot.actuate.health.HealthIndicator.getHealth(HealthIndicator.java:37) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointWebExtension.getHealth(HealthEndpointWebExtension.java:85) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointWebExtension.getHealth(HealthEndpointWebExtension.java:44) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getContribution(HealthEndpointSupport.java:99) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getAggregateHealth(HealthEndpointSupport.java:110) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getContribution(HealthEndpointSupport.java:96) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getAggregateHealth(HealthEndpointSupport.java:110) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getContribution(HealthEndpointSupport.java:96) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getHealth(HealthEndpointSupport.java:74) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointSupport.getHealth(HealthEndpointSupport.java:61) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointWebExtension.health(HealthEndpointWebExtension.java:71) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.health.HealthEndpointWebExtension.health(HealthEndpointWebExtension.java:60) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at sun.reflect.GeneratedMethodAccessor95.invoke(Unknown Source) ~[na:na] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_282] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_282] at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:282) ~[spring-core-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.boot.actuate.endpoint.invoke.reflect.ReflectiveOperationInvoker.invoke(ReflectiveOperationInvoker.java:77) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.endpoint.annotation.AbstractDiscoveredOperation.invoke(AbstractDiscoveredOperation.java:60) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$ServletWebOperationAdapter.handle(AbstractWebMvcEndpointHandlerMapping.java:305) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(AbstractWebMvcEndpointHandlerMapping.java:388) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at sun.reflect.GeneratedMethodAccessor94.invoke(Unknown Source) ~[na:na] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_282] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_282] at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:190) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:138) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:105) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:878) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:792) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1040) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:943) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:898) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:626) ~[tomcat-embed-core-9.0.37.jar!/:4.0.FR] at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883) ~[spring-webmvc-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at javax.servlet.http.HttpServlet.service(HttpServlet.java:733) ~[tomcat-embed-core-9.0.37.jar!/:4.0.FR] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-embed-websocket-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.springframework.boot.actuate.metrics.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:93) ~[spring-boot-actuator-2.3.3.RELEASE.jar!/:2.3.3.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) ~[spring-web-5.2.8.RELEASE.jar!/:5.2.8.RELEASE] at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:202) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:96) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:541) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:139) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:373) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:868) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1589) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_282] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_282] at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) ~[tomcat-embed-core-9.0.37.jar!/:9.0.37] at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_282] 2021-03-18 12:03:47.967 ERROR 2319 --- [http-nio-8080-exec-5] com.alibaba.nacos.client.naming          : request: /nacos/v1/ns/service/list failed, servers: [100.100.21.44, 100.100.0.46], code: 503, msg: <!DOCTYPE HTML PUBLIC ""-//IETF//DTD HTML 2.0//EN""> <html> <head><title>503 Service Temporarily Unavailable</title></head> <body bgcolor=""white""> <h1>503 Service Temporarily Unavailable</h1> <p>The server is temporarily unable to service your request due to maintenance downtime or capacity problems. Please try again later. Sorry for the inconvenience.<br/> Please report this message and include the following information to us.<br/> Thank you very much!</p>  ",2021-03-18T08:48:33+00:00,2024-12-19T18:52:23+00:00,3,https://github.com/alibaba/spring-cloud-alibaba/issues/2001,480.0,,https://github.com/alibaba/spring-cloud-alibaba/pull/480,67,0,0,67,158,158,316,0,32938.063888888886,stale;wait-for-feedback,True,False,normal,configuration,"[{""filename"": ""pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-dependencies/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-docs/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-dubbo/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/acm-example/acm-local-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/ans-example/ans-consumer-feign-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/ans-example/ans-consumer-ribbon-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/ans-example/ans-provider-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/fescar-example/account-service/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/fescar-example/business-service/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/fescar-example/order-service/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/fescar-example/storage-service/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-config-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-discovery-example/nacos-discovery-consumer-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-discovery-example/nacos-discovery-provider-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-discovery-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-gateway-example/nacos-gateway-discovery-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-gateway-example/nacos-gateway-provider-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/nacos-example/nacos-gateway-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/oss-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/rocketmq-example/rocketmq-consume-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/rocketmq-example/rocketmq-produce-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/schedulerx-example/schedulerx-simple-task-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/sentinel-example/sentinel-core-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/sentinel-example/sentinel-dubbo-example/sentinel-dubbo-api/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/sentinel-example/sentinel-dubbo-example/sentinel-dubbo-consumer-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/sentinel-example/sentinel-dubbo-example/sentinel-dubbo-provider-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/sms-example/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-dubbo-examples/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-dubbo-examples/spring-cloud-dubbo-consumer-sample/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-dubbo-examples/spring-cloud-dubbo-provider-sample/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-dubbo-examples/spring-cloud-dubbo-provider-web-sample/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-dubbo-examples/spring-cloud-dubbo-sample-api/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-bus-rocketmq-example/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-fescar/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-nacos-config-server/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-nacos-config/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-nacos-discovery/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-sentinel-datasource/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-sentinel-zuul/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-sentinel/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-test/core-support/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-test/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-test/sentinel-test-support/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-acm/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-ans/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-context/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-oss/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-schedulerx/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alicloud-sms/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-alibaba-fescar/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-alibaba-nacos-config-server/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-alibaba-nacos-config/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-alibaba-nacos-discovery/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-alibaba-sentinel/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-bus-rocketmq/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-dubbo/pom.xml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alibaba/spring-cloud-starter-stream-rocketmq/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/spring-cloud-starter-alicloud-acm/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/spring-cloud-starter-alicloud-ans/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/spring-cloud-starter-alicloud-oss/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/spring-cloud-starter-alicloud-schedulerx/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-starter-alicloud/spring-cloud-starter-alicloud-sms/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""spring-cloud-stream-binder-rocketmq/pom.xml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}]",sentinel-dubbo-example;spring-cloud-alibaba-dubbo-examples;spring-cloud-alibaba-nacos-config-server;fescar-example;spring-cloud-starter-alibaba,True
alibaba/spring-cloud-alibaba,3257,spring-cloud-alibaba-sidecar-nacos-example test failed in 2022.x,"**Which Component** integrated-example spring-cloud-alibaba-sidecar-nacos-example  **Describe the bug** In starting the example of `node - service. js heterogeneous service` and `DemoApplication`, after visiting `http://127.0.0.1:8070/node-service/health.json` no results.And the browser reports a 503 exception. ![image](https://user-images.githubusercontent.com/77964041/230772504-2766227e-9bb1-4815-9f18-01713ed24a3b.png) The check URL for the heterogeneous service is available. ![image](https://user-images.githubusercontent.com/77964041/230772569-daab1b19-1f36-4976-b4ca-4b485539d339.png)   **Simplest demo** https://github.com/alibaba/spring-cloud-alibaba/tree/2022.x/spring-cloud-alibaba-examples/spring-cloud-alibaba-sidecar-examples  **To Reproduce** Steps to reproduce the behavior: Refer to the [readme](https://github.com/alibaba/spring-cloud-alibaba/blob/2022.x/spring-cloud-alibaba-examples/spring-cloud-alibaba-sidecar-examples/readme-zh.md) documentation  **Screenshots** Service successfully registered with nacos. The heterogeneous service is started successfully. ![image](https://user-images.githubusercontent.com/77964041/230772370-18e0e7d7-defc-4313-85f3-c3e4945c14b3.png) ![image](https://user-images.githubusercontent.com/77964041/230772357-35c525bf-cad0-46d0-82f9-d8d79f765b76.png)  **Additional context** Spring Cloud Alibaba 2022.x nacos 2.2.x, win10, node.js16.16.0 ",2023-04-09T12:30:42+00:00,2023-04-19T06:09:53+00:00,9,https://github.com/alibaba/spring-cloud-alibaba/issues/3257,3266.0,2023-04-19T06:10:27+00:00,https://github.com/alibaba/spring-cloud-alibaba/pull/3266,2,0,0,2,9,0,9,0,233.6625,,True,False,normal,configuration,"[{""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-sidecar-examples/spring-cloud-alibaba-sidecar-nacos-example/pom.xml"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-cloud-alibaba-examples/spring-cloud-alibaba-sidecar-examples/spring-cloud-alibaba-sidecar-nacos-example/src/main/resources/application.yml"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""config""}]",resources,False
hashicorp/consul-terraform-sync,47,Fix TF logging,"tfexec mutes TF logging and does not enable configuring the log level, through API and env `TF_LOG` ([source](https://github.com/hashicorp/terraform-exec/blob/21c9bef3c185aca9d1b550704f8b1fcc5eb18307/tfexec/terraform_cmd.go#L27)). This is a work-around to allow users to see the Terraform output from the various commands executed (init, workspace, plan, apply, etc.) to be included in the Consul NIA logs.  This PR also adds the option to write TF logs on TRACE level to a file on disk. Running it locally, it's quite large and appends for each command. It's useful to see exactly what Terraform is doing per task.  Additions: * Config option `driver.terraform.log` (bool) to enable Terraform output to be included in the Consul NIA log * Config option `driver.terraform.persist_log` (bool) to enable trace logging to disk per task.  Fixes: * Removed config option `driver.terraform.log_level` that does nothing.  <details> <summary>Example logs with Terraform output included</summary>  ``` 2020/09/02 20:03:01.498582 [INFO] (controller.readwrite) init work 2020/09/02 20:03:01.498856 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform init -no-color -force-copy -input=false -lock-timeout=0s -backend=true -get=true -get-plugins=true -lock=true -upgrade=false -verify-plugins=true Initializing modules...  Initializing the backend...  Initializing provider plugins... - Using previously-installed hashicorp/local v1.4.0  The following providers do not have any version constraints in configuration, so the latest version was installed.  To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below.  * hashicorp/local: version = ""~> 1.4.0""  Terraform has been successfully initialized! 2020/09/02 20:03:01.972030 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform workspace new -no-color print Workspace ""print"" already exists 2020/09/02 20:03:02.157733 [DEBUG] (client.terraformcli) workspace already exists: 'print' 2020/09/02 20:03:02.157778 [INFO] (controller.readwrite) apply work 2020/09/02 20:03:02.157891 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform apply -no-color -auto-approve -input=false -var-file=/Users/kngo/dev/hashicorp/consul-nia/test/output/test.tfvars -var-file=/Users/kngo/dev/hashicorp/consul-nia/test/output/override.tfvars -var-file=terraform.tfvars -lock=true -parallelism=10 -refresh=true module.print.local_file.services[""api""]: Refreshing state... [id=400fa10de12186c911b0fe39db69b3bbd4bfab9d] module.print.local_file.all_addresses: Refreshing state... [id=6e247fcff44778b54f54b8848df24aa15a0e534f] module.print.local_file.services[""web""]: Refreshing state... [id=4b84b15bff6ee5796152495a230e45e3d7e947d9] module.print.local_file.variables: Refreshing state... [id=68fbf5de80e5e0e07127ae9232cad0ff015966ab] ```  </details>  <details> <summary>Example logs with Terraform output excluded</summary>  ``` 2020/09/02 20:03:01.498582 [INFO] (controller.readwrite) init work 2020/09/02 20:03:01.498856 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform init -no-color -force-copy -input=false -lock-timeout=0s -backend=true -get=true -get-plugins=true -lock=true -upgrade=false -verify-plugins=true 2020/09/02 20:03:01.972030 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform workspace new -no-color print 2020/09/02 20:03:02.157733 [DEBUG] (client.terraformcli) workspace already exists: 'print' 2020/09/02 20:03:02.157778 [INFO] (controller.readwrite) apply work 2020/09/02 20:03:02.157891 [INFO] running Terraform command: /Users/kngo/dev/hashicorp/consul-nia/terraform apply -no-color -auto-approve -input=false -var-file=/Users/kngo/dev/hashicorp/consul-nia/test/output/test.tfvars -var-file=/Users/kngo/dev/hashicorp/consul-nia/test/output/override.tfvars -var-file=terraform.tfvars -lock=true -parallelism=10 -refresh=true ```  </details>  Resolves #45",2020-09-02T20:25:02+00:00,2020-09-03T14:31:47+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/47,475.0,2021-11-01T19:21:06+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/475,1,0,0,1,1,0,1,0,10198.934444444443,bug,True,False,major,configuration,"[{""filename"": "".github/workflows/build.yml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
dapr/dapr,7816,Pubsub bug during graceful shutdown (continuation),"## In what area(s)?  /area runtime    ## What version of Dapr?  1.13.x  ## Expected Behavior - Messages should be correctly completed during the ""block shutdown"" period. - explicit shutdown call should continue with sidecar shutdown procedure immediately (even if it's now inside the block shutdown period"")  ## Actual Behavior In 1.13.2 the behaviour of ""block-shutdown-period"" was fixed.... almost: https://github.com/dapr/dapr/commit/121f5e4aa048e17a00455939b8067d06c4eff284  Here's the problem: - if we explicitly call  the ""shutdown"" api to stop the sidecar, it all works correctly... except that it says ""App health probes stopping"" and then it remains blocked for the rest of the ""block period"", nothing you can do about it (since probes stopped,  no way to ""unblock"" now).  - if we DON'T explicitly call  call the ""shutdown"" api and just exit the main container - health probes will unlock the sidecar and it continues with graceful shutdown... except that it considers the last pubsub message as not being ""complete"" so it will return it to the queue, which is not quite what we'd want.  ## Steps to Reproduce the Problem  Try to initiate a shutdown while the app is consuming a message via pubsub; attempt to use ""block-shutdown-duration"" annotation to allow for graceful shutdown (to allow message to be gracefully <completed>).   ## Release Note  <!-- How should the fix for this issue be communicated in our release notes? It can be populated later. --> <!-- Keep it as a single line. Examples: -->  <!-- RELEASE NOTE: **ADD** New feature in Dapr. --> <!-- RELEASE NOTE: **FIX** Bug in runtime. --> <!-- RELEASE NOTE: **UPDATE** Runtime dependency. -->  RELEASE NOTE:  **FIX** Bug in runtime.  ",2024-06-17T13:05:29+00:00,2024-11-05T21:27:51+00:00,6,https://github.com/dapr/dapr/issues/7816,4909.0,2022-07-15T00:21:54+00:00,https://github.com/dapr/dapr/pull/4909,1,0,0,1,18,0,18,0,-16884.72638888889,kind/bug;stale,True,False,normal,ui,"[{""filename"": "".github/workflows/dapr-test.yml"", ""lines_added"": 18, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
go-kratos/kratos,3480,Server start may fail and cause Register error,"<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened:  If server failed to start, it will cause Register fail too.  #### What you expected to happen: Return server error, instead register error if server error happens. `{""level"":""warn"",""ts"":""2024-12-05T19:02:16.607470+0800"",""logger"":""etcd-client"",""caller"":""v3@v3.5.17/retry_interceptor.go:63"",""msg"":""retrying of unary invoker failed"",""target"":""etcd-endpoints://0xc0003e4f00/127.0.0.1:2379"",""attempt"":0,""error"":""rpc error: code = Canceled desc = context canceled""}`  #### How to reproduce it (as minimally and precisely as possible): Start a program which listen port A, use kratos to listen port A too.  #### Anything else we need to know?: The ctx will be cancled since the latst error, you guys did't handle it.  #### Environment: - Kratos version (use `kratos -v`): v2.8.2 - Go version (use `go version`): go version go1.23.4 linux/amd64 - OS (e.g: `cat /etc/os-release`): ``` NAME=""Arch Linux"" PRETTY_NAME=""Arch Linux"" ID=arch BUILD_ID=rolling ANSI_COLOR=""38;2;23;147;209"" HOME_URL=""https://archlinux.org/"" DOCUMENTATION_URL=""https://wiki.archlinux.org/"" SUPPORT_URL=""https://bbs.archlinux.org/"" BUG_REPORT_URL=""https://gitlab.archlinux.org/groups/archlinux/-/issues"" PRIVACY_POLICY_URL=""https://terms.archlinux.org/docs/privacy-policy/"" LOGO=archlinux-logo  ``` - Others: None",2024-12-05T11:03:22+00:00,2025-03-07T03:33:09+00:00,1,https://github.com/go-kratos/kratos/issues/3480,1755.0,2022-01-08T12:14:54+00:00,https://github.com/go-kratos/kratos/pull/1755,1,0,0,1,3,1,4,0,-25486.807777777776,bug,True,False,normal,ui,"[{""filename"": "".github/workflows/gitee-sync.yml"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
apache/pulsar,24039,[Bug] Unable to start cluster in compose (4.0.2),"### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  OS: macOs (14.2.1 (23C71)) Pulsar: 4.0.2 docker-compose: 1.29.2, build 5becea4c  ### Minimal reproduce step  ``` version: '3'  services:   # Start zookeeper   zookeeper:     image: apachepulsar/pulsar:4.0.2     container_name: zookeeper     restart: on-failure     networks:       - pulsar     volumes:       - ./data/zookeeper:/pulsar/data/zookeeper     environment:       - metadataStoreUrl=zk:zookeeper:2181       - PULSAR_MEM=-Xms256m -Xmx256m -XX:MaxDirectMemorySize=256m     command: >       bash -c ""bin/apply-config-from-env.py conf/zookeeper.conf && \\              bin/generate-zookeeper-config.sh conf/zookeeper.conf && \\              exec bin/pulsar zookeeper""     healthcheck:       test: [ ""CMD"", ""bin/pulsar-zookeeper-ruok.sh"" ]       interval: 10s       timeout: 5s       retries: 30    # Init cluster metadata   pulsar-init:     container_name: pulsar-init     hostname: pulsar-init     image: apachepulsar/pulsar:4.0.2     networks:       - pulsar     command: >       bash -c ""bin/pulsar initialize-cluster-metadata \\       --cluster cluster-a \\       --zookeeper zookeeper:2181 \\       --configuration-store zookeeper:2181 \\       --web-service-url http://broker:8080 \\       --broker-service-url pulsar://broker:6650""     depends_on:       zookeeper:         condition: service_healthy    # Start bookie   bookie:     image: apachepulsar/pulsar:4.0.2     container_name: bookie     restart: on-failure     networks:       - pulsar     environment:       - clusterName=cluster-a       - PULSAR_PREFIX_clusterName=cluster-a       - PULSAR_PREFIX_webServiceUrl=pulsar://broker:6650       - zkServers=zookeeper:2181       - metadataServiceUri=metadata-store:zk:zookeeper:2181       # otherwise every time we run docker compose uo or down we fail to start due to Cookie       # See: https://github.com/apache/bookkeeper/blob/405e72acf42bb1104296447ea8840d805094c787/bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Cookie.java#L57-68       - advertisedAddress=bookie       - BOOKIE_MEM=-Xms512m -Xmx512m -XX:MaxDirectMemorySize=256m     depends_on:       zookeeper:         condition: service_healthy       pulsar-init:         condition: service_completed_successfully     # Map the local directory to the container to avoid bookie startup failure due to insufficient container disks.     volumes:       - ./data/bookkeeper:/pulsar/data/bookkeeper     command: bash -c ""bin/apply-config-from-env.py conf/bookkeeper.conf && exec bin/pulsar bookie""    # Start broker   broker:     image: apachepulsar/pulsar:4.0.2     container_name: broker     hostname: broker     restart: on-failure     networks:       - pulsar     environment:       - metadataStoreUrl=zk:zookeeper:2181       - zookeeperServers=zookeeper:2181       - clusterName=cluster-a       - managedLedgerDefaultEnsembleSize=1       - managedLedgerDefaultWriteQuorum=1       - managedLedgerDefaultAckQuorum=1       - advertisedAddress=broker       - advertisedListeners=external:pulsar://127.0.0.1:6650       - PULSAR_MEM=-Xms512m -Xmx512m -XX:MaxDirectMemorySize=256m     depends_on:       zookeeper:         condition: service_healthy       bookie:         condition: service_started     ports:       - ""6650:6650""       - ""8080:8080""     command: bash -c ""bin/apply-config-from-env.py conf/broker.conf && exec bin/pulsar broker""  networks:   pulsar:     driver: bridge  ```  ### What did you expect to see?  A running cluster composed from 4 images based on `apachepulsar/pulsar:4.0.2`  ### What did you see instead?  ``` ERROR: for bookie  Container ""19983ee60365"" exited with code 1. ERROR: Encountered errors while bringing up the project. nryanov@air apache-pulsar % docker logs 19983ee60365 picocli.CommandLine$MissingParameterException: Missing required options: '--cluster=<cluster>', '--web-service-url=<clusterWebServiceUrl>' ```  ### Anything else?  If i switch image version from 4.0.2 to 3.3.4 (e.g.) then everything start well. Also i've tried to add in a bookie container ENVs like cluster & webServiceUrl but it didn't work.  ### Are you willing to submit a PR?  - [ ] I'm willing to submit a PR!",2025-02-28T19:09:57+00:00,2025-02-28T21:03:34+00:00,4,https://github.com/apache/pulsar/issues/24039,24040.0,2025-03-03T19:05:58+00:00,https://github.com/apache/pulsar/pull/24040,1,0,1,2,32,108,128,0,71.9336111111111,type/bug,True,False,normal,configuration,"[{""filename"": ""docker-compose/kitchen-sink/README.MD"", ""lines_added"": 1, ""lines_deleted"": 11, ""file_type"": ""other""}, {""filename"": ""docker-compose/kitchen-sink/docker-compose.yml"", ""lines_added"": 31, ""lines_deleted"": 97, ""file_type"": ""config""}]",,False
dapr/dapr,8078,Workflow API: No Activity State Found,"## In what area(s)?  > /area runtime  > /area operator  > /area placement  > /area docs  > /area test-and-release  ## What version of Dapr?  <!-- Delete all but your choice -->  > 1.1.x > 1.0.x > edge: output of `git describe --dirty`  ## Expected Behavior  Workflow to execute successfully both with and without scheduler and not throw errors about `no activity state found`.  ## Actual Behavior  Currently user is seeing the following w/ v 1.14.2-rc.3 w/o Scheduler: ``` Error executing reminder for internal actor <run-activity>: no activity state found, scope: dapr.runtime.actor error invoking reminder on actor <activity actor>: no activity state found, scope: dapr.runtime.actor ``` With Scheduler enabled it appears to be a similar err: ``` failed to invoke scheduled actor reminder named run-activity due to: no activity state found Error executing reminder for internal actor <run-activity>: no activity state found, scope: dapr.runtime.actor failed to invoke scheduled actor reminder named: run-activity due to: no activity state found, scope: dapr.runtime.scheduler ```  ## Steps to Reproduce the Problem  Use Workflows without Scheduler, also seems to impact if using Scheduler so once root issue is fixed it _should_ auto work with Scheduler.  ## Release Note  <!-- How should the fix for this issue be communicated in our release notes? It can be populated later. --> <!-- Keep it as a single line. Examples: -->  <!-- RELEASE NOTE: **ADD** New feature in Dapr. --> <!-- RELEASE NOTE: **FIX** Bug in runtime. --> <!-- RELEASE NOTE: **UPDATE** Runtime dependency. -->  RELEASE NOTE: FIXED Workflow API no activity state found ",2024-09-10T21:14:33+00:00,2024-10-02T22:35:14+00:00,1,https://github.com/dapr/dapr/issues/8078,5004.0,2022-08-12T13:10:08+00:00,https://github.com/dapr/dapr/pull/5004,1,0,0,1,7,0,7,0,-18248.07361111111,kind/bug;P0,True,False,critical,functional,"[{""filename"": "".github/workflows/dapr-3rdparty-images.yaml"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
go-kratos/kratos,3333,HTTPClient  proto: syntax error (line 1:1): unexpected token 404,"## kratos version: v2.7.3  ## proto file  ```proto syntax = ""proto3"";  package api.rabbit.hook;  import ""google/api/annotations.proto"";  option go_package = ""github.com/aide-cloud/moon/api/rabbit/hook;hook""; option java_multiple_files = true; option java_package = ""api.rabbit.hook"";  // 用于接受外界需要推送的消息 service Hook {   // 发送消息, 用于接受http数据   rpc SendMsg (SendMsgRequest) returns (SendMsgReply) {     option (google.api.http) = {       post: ""/v1_rabbit_send_msg""       body: ""*""     };   } }  message SendMsgRequest {   // 用于接收外界的数据， 兼容所有json格式   string json_data = 1;   // 用于匹配该数据发送给谁   string route = 2; } message SendMsgReply {   // 发送的结果   string msg = 1;   // 状态码   int32 code = 2;   // 发送时间   string time = 3; } ```  ## HTTPClient  ```go hookapi.NewHookHTTPClient(l.httpClient).SendMsg(ctx, in, httpOpts...) ```  ## error info  ```bash error: code = 404 reason =  message =  metadata = map[] cause = proto: syntax error (line 1:1): unexpected token 404 ```",2024-05-29T14:25:44+00:00,2024-09-04T16:06:14+00:00,3,https://github.com/go-kratos/kratos/issues/3333,3500.0,,https://github.com/go-kratos/kratos/pull/3500,0,0,2,2,17,16,0,0,2353.675,bug,True,False,normal,configuration,"[{""filename"": ""contrib/config/consul/go.mod"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""contrib/config/consul/go.sum"", ""lines_added"": 13, ""lines_deleted"": 12, ""file_type"": ""other""}]",,False
go-kratos/kratos,2899,httputil.DumpRequestOut return error when dump request,"``` if it, ok := transport.FromServerContext(ctx); ok {     if tr, ok := it.(*http.Transport); ok {         dump, err := httputil.DumpRequestOut(tr.Request(), true)         // err not nil     } } ```  The problem is that the data in `request.URL` is invalid，No Schema, no Host",2023-07-03T03:17:24+00:00,2023-11-24T16:02:27+00:00,4,https://github.com/go-kratos/kratos/issues/2899,2056.0,2022-05-28T09:33:54+00:00,https://github.com/go-kratos/kratos/pull/2056,0,0,2,2,3,5,0,0,-9617.725,bug,True,False,normal,database,"[{""filename"": ""contrib/config/kubernetes/go.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""contrib/config/kubernetes/go.sum"", ""lines_added"": 2, ""lines_deleted"": 4, ""file_type"": ""other""}]",,False
eventuate-local/eventuate-local,50,NullPointerException when deleting events from events table,eventuateLocalVersion=0.15.0.RELEASE CDC docker image version 0.18.0.RELEASE  After deleting some events:  ``` DELETE FROM `eventuate`.`events` WHERE  `event_id`=' ``` https://github.com/eventuate-local/eventuate-local/blob/0.18.0.RELEASE/eventuate-local-java-embedded-cdc/src/main/java/io/eventuate/local/cdc/debezium/MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.java#L146  ``` Caused by: java.lang.RuntimeException: Engine through exceptionStopping connector after error in the application's handler method: null at io.eventuate.local.cdc.debezium.MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.lambda$startCapturingChanges$0(MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.java:96) ~[eventuate-local-java-embedded-cdc-0.15.0-SNAPSHOT.jar!/:na] at io.debezium.embedded.EmbeddedEngine.run(EmbeddedEngine.java:742) ~[debezium-embedded-0.3.6.jar!/:0.3.6] at io.eventuate.local.cdc.debezium.MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.lambda$startCapturingChanges$1(MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.java:105) ~[eventuate-local-java-embedded-cdc-0.15.0-SNAPSHOT.jar!/:na] ... 3 common frames omitted Caused by: java.lang.NullPointerException: null at io.eventuate.local.cdc.debezium.MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.receiveEvent(MySqlBinLogBasedEventTableChangesToAggregateTopicRelay.java:146) ~[eventuate-local-java-embedded-cdc-0.15.0-SNAPSHOT.jar!/:na] at io.debezium.embedded.EmbeddedEngine.run(EmbeddedEngine.java:662) ~[debezium-embedded-0.3.6.jar!/:0.3.6] ```,2018-07-05T15:21:01+00:00,2018-07-06T22:13:04+00:00,1,https://github.com/eventuate-local/eventuate-local/issues/50,232.0,,https://github.com/eventuate-local/eventuate-local/pull/232,1,0,1,2,194,5,2,0,30.8675,bug,True,False,normal,database,"[{""filename"": ""eventuate-local-console/eventuate-local-console-server/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""eventuate-local-console/eventuate-local-console-server/yarn.lock"", ""lines_added"": 193, ""lines_deleted"": 4, ""file_type"": ""other""}]",eventuate-local-console,False
eventuate-local/eventuate-local,7,cdc-service fails when there exists database with `-` in their name,"The same as #6. I have described the problem wrongly there, it's database rather than table.  It is fixed in 0.3.6 of debezium: https://github.com/debezium/debezium/blob/v0.3.6/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/SnapshotReader.java#L260  P.S. I was not able to make current tests pass on my local machine, so I'm not doing a PR. Docs on how to properly run tests would be helpful. :-)",2017-06-15T08:44:22+00:00,2017-06-24T23:09:51+00:00,5,https://github.com/eventuate-local/eventuate-local/issues/7,221.0,,https://github.com/eventuate-local/eventuate-local/pull/221,1,0,1,2,221,8,2,0,230.4247222222222,bug,True,False,normal,database,"[{""filename"": ""eventuate-local-console/eventuate-local-console-server/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""eventuate-local-console/eventuate-local-console-server/yarn.lock"", ""lines_added"": 220, ""lines_deleted"": 7, ""file_type"": ""other""}]",eventuate-local-console,False
apache/rocketmq,8982,[Bug] E2E golang pipeline fails due to Go version incompatibility,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  GitHub Action  ### RocketMQ version  xxx  ### JDK Version  xxx  ### Describe the Bug  The e2e-golang pipeline fails due to Go version incompatibility. The pipeline currently installs Go 1.22.6, which does not meet the minimum version requirement (>= 1.22.7) of certain dependencies, such as google.golang.org/grpc@v1.68.0.  ![image](https://github.com/user-attachments/assets/0ef091da-7359-4406-b89a-e15058972da7)   ### Steps to Reproduce  1. Run the e2e-golang pipeline with the current script. 2. Observe the Go installation step installs Go 1.22.6. 3. Proceed to the testing phase and observe dependency version issues causing the pipeline to fail.  ### What Did You Expect to See?  The pipeline should complete successfully without any version conflicts for Go dependencies.  ### What Did You See Instead?  The pipeline fails due to Go version incompatibility during dependency resolution.  ### Additional Context  _No response_",2024-11-25T10:39:39+00:00,2024-11-26T03:37:47+00:00,0,https://github.com/apache/rocketmq/issues/8982,8985.0,2024-11-26T03:37:46+00:00,https://github.com/apache/rocketmq/pull/8985,2,0,0,2,6,4,10,0,16.968611111111112,,True,False,normal,ui,"[{""filename"": "".github/workflows/pr-e2e-test.yml"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": "".github/workflows/push-ci.yml"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""config""}]",,False
WasmEdge/WasmEdge,2061,AOT bug,"## Description  ### Current State run a wasm is ok, but after aot, it will return error with `[error] execution failed: out of bounds memory access, Code: 0x88`  ### Expected   ## Environment   - Hardware Architecture:  - Operating system: Ubuntu 18  The following information is optional. Please provide them only if you have built from source. - C++ Compiler version: - CMake version: - CMake flags: `-DCMAKE_BUILD_TYPE=Release -DWASMEDGE_BUILD_AOT_RUNTIME=ON` - Boost version:  ## Steps to Reproduce https://github.com/second-state/wasmedge_asyncify/blob/preview/examples/aot/src/main.rs ",2022-11-07T08:46:06+00:00,2025-03-07T05:12:26+00:00,6,https://github.com/WasmEdge/WasmEdge/issues/2061,640.0,,https://github.com/WasmEdge/WasmEdge/pull/640,0,0,2,2,0,4,0,0,20420.43888888889,bug;help wanted;priority:low;c-AOT,True,False,normal,security,"[{""filename"": ""utils/docker/Dockerfile.appdev_aarch64"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""utils/docker/Dockerfile.appdev_x86_64"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""other""}]",docker,False
WasmEdge/WasmEdge,3757,bug: metal backend of the stable diffusion plugin should be enabled by default on macOS arm64,"### Summary  Please check the screenshot. The default assets of the stable diffusion plugin on macOS arm64 don't enable Metal support. And it also provide a standalone asset with Metal support, which is not the expected behavior.  * Remove the metal-disabled version on the macOS arm64 platform * Use the metal-enabled version for the macOS arm64 asset  ### Current State  We have both metal-disabled and metal-enabled assets.  ### Expected State  We should only provide one metal-enabled asset for the macOS arm64 platform.  ### Reproduction steps  Check the 0.14.1-rc.4 release assets list.  ### Screenshots  ![image](https://github.com/user-attachments/assets/89d8834c-c9fc-4b19-9bc2-7a08a6973062)   ### Any logs you want to share for showing the specific issue  _No response_  ### Components  Others  ### WasmEdge Version or Commit you used  0.14.1-rc.4  ### Operating system information  macOS  ### Hardware Architecture  arm64  ### Compiler flags and options  None.",2024-09-11T17:57:15+00:00,2024-09-13T07:01:03+00:00,0,https://github.com/WasmEdge/WasmEdge/issues/3757,4028.0,2025-02-26T11:46:06+00:00,https://github.com/WasmEdge/WasmEdge/pull/4028,4,0,2,6,41,44,64,0,4025.8141666666666,bug,True,False,normal,ui,"[{""filename"": "".github/extensions.paths-filter.yml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": "".github/workflows/matrix-extensions.json"", ""lines_added"": 0, ""lines_deleted"": 10, ""file_type"": ""config""}, {""filename"": "".github/workflows/reusable-build-extensions-on-macos.yml"", ""lines_added"": 28, ""lines_deleted"": 13, ""file_type"": ""config""}, {""filename"": "".github/workflows/reusable-build-extensions.yml"", ""lines_added"": 6, ""lines_deleted"": 6, ""file_type"": ""config""}, {""filename"": ""CMakeLists.txt"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""plugins/wasmedge_stablediffusion/CMakeLists.txt"", ""lines_added"": 5, ""lines_deleted"": 14, ""file_type"": ""other""}]",,False
easegress-io/easegress,284,Metrics are not following a same principle,"In the calculation of metrics like `request count`, `status code count`, `min duration`, `max duration`, all data are weighted equally. In the calculation of metrics like `m1`, `m5`, the moving average algorithm is used, which gives more weight to recent data. Metrics like `duration percentiles` are actually only using the recent data for calculation.  These metrics should follow the same principle to weight data by default, exceptional cases should be handled carefully.",2021-09-29T03:27:26+00:00,2024-01-28T03:21:44+00:00,1,https://github.com/easegress-io/easegress/issues/284,560.0,2022-04-02T01:05:29+00:00,https://github.com/easegress-io/easegress/pull/560,4,0,0,4,9,9,18,0,4437.634166666667,bug,True,False,normal,database,"[{""filename"": "".github/workflows/code.analysis.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/license-checker.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/release.yml"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": "".github/workflows/test.yml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}]",,False
easegress-io/easegress,1066,"fix goreleaser build error, add egbuilder to release",- Add `egbuilder` to release. - Update dockerfile to fix gorelease build error,2023-08-16T06:18:52+00:00,2023-08-16T06:37:56+00:00,0,https://github.com/easegress-io/easegress/pull/1066,1066.0,2023-08-16T06:37:56+00:00,https://github.com/easegress-io/easegress/pull/1066,1,0,2,3,18,3,17,0,0.3177777777777777,,True,False,normal,ui,"[{""filename"": "".goreleaser.yml"", ""lines_added"": 16, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""build/package/Dockerfile"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""build/package/Dockerfile.goreleaser"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
stefanprodan/podinfo,21,"exec user process caused ""exec format error""","I'm following https://medium.com/@stefanprodan/running-kubernetes-on-scaleway-bare-metal-with-terraform-and-kubeadm-1cf18aae32d5 and I'm in the process of updating everything to the most recent versions of the involved software (terraform, metrics-server versions, kubernetes versions, etc.)  I'm almost done, but I'm trying out the podinfo deploy but I have just an error message in pod logs when starting the container this way:  ``` $ kubectl --kubeconfig ./$(terraform output kubectl_config) \\ >   apply -f https://raw.githubusercontent.com/stefanprodan/k8s-podinfo/master/deploy/auto-scaling/podinfo-svc-nodeport.yaml service/podinfo-nodeport created  kubectl --kubeconfig ./$(terraform output kubectl_config) \\ >   apply -f https://raw.githubusercontent.com/stefanprodan/k8s-podinfo/master/deploy/auto-scaling/podinfo-dep.yaml deployment.apps/podinfo created ```  ``` standard_init_linux.go:211: exec user process caused ""exec format error"" ``` ![Screen Shot 2019-08-04 at 01 00 27](https://user-images.githubusercontent.com/12844/62417580-48b6ef80-b653-11e9-82fa-a1091554ae52.png) ![Screen Shot 2019-08-04 at 01 01 01](https://user-images.githubusercontent.com/12844/62417584-65ebbe00-b653-11e9-81a4-937ab59237f0.png)  ",2019-08-03T23:01:43+00:00,2019-08-04T02:02:51+00:00,2,https://github.com/stefanprodan/podinfo/issues/21,289.0,2023-08-10T12:08:37+00:00,https://github.com/stefanprodan/podinfo/pull/289,2,0,3,5,5,5,4,0,35221.115,,True,False,normal,configuration,"[{""filename"": "".github/workflows/release.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/test.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""Dockerfile"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""Dockerfile.base"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""Dockerfile.xx"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
stefanprodan/podinfo,188,"""error"":""dial tcp: missing address"",""server"":""podinfo-redis:6379""","## Issue ## Attempting to test some redis monitoring and the podinfo main application can't seem to connect to the redis pods,  values.yml ``` cache: ""podinfo-redis:6379"" # Redis deployment redis:   enabled: true   repository: redis   tag: 6.0.8 ```  deployment logs ``` ❯ kubectl logs -n podinfo deployment/podinfo Found 2 pods, using pod/podinfo-65d549885d-f8d2j {""level"":""info"",""ts"":""2022-04-11T23:02:22.027Z"",""caller"":""podinfo/main.go:150"",""msg"":""Starting podinfo"",""version"":""6.1.2"",""revision"":"""",""port"":""9898""} {""level"":""info"",""ts"":""2022-04-11T23:02:22.032Z"",""caller"":""api/server.go:265"",""msg"":""Starting HTTP Server."",""addr"":"":9898""} {""level"":""warn"",""ts"":""2022-04-11T23:02:22.032Z"",""caller"":""api/cache.go:159"",""msg"":""cache server is offline"",""error"":""dial tcp: missing address"",""server"":""podinfo-redis:6379""} {""level"":""warn"",""ts"":""2022-04-11T23:02:52.032Z"",""caller"":""api/cache.go:159"",""msg"":""cache server is offline"",""error"":""dial tcp: missing address"",""server"":""podinfo-redis:6379""} {""level"":""warn"",""ts"":""2022-04-11T23:03:22.036Z"",""caller"":""api/cache.go:159"",""msg"":""cache server is offline"",""error"":""dial tcp: missing address"",""server"":""podinfo-redis:6379""} {""level"":""warn"",""ts"":""2022-04-11T23:03:52.032Z"",""caller"":""api/cache.go:159"",""msg"":""cache server is offline"",""error"":""dial tcp: missing address"",""server"":""podinfo-redis:6379""} ```  I'm able to `nslookup` the podinfo-redis service which returns the correct address. I also spun up a test `redis-cli` container and was able to connect successfully.  redis-cli test ``` root@redis-cli:/data# redis-cli -h podinfo-redis -p 6379 podinfo-redis:6379> exit root@redis-cli:/data# redis-cli -h podinfo-redis -p 6379 --stat ------- data ------ --------------------- load -------------------- - child - keys       mem      clients blocked requests            connections           1          866.85K  2       0       7556 (+0)           641          1          866.85K  2       0       7569 (+13)          642          1          866.85K  2       0       7573 (+4)           642          1          866.85K  2       0       7580 (+7)           642          1          866.85K  2       0       7584 (+4)           642          1          866.85K  2       0       7588 (+4)           642          1          866.85K  2       0       7592 (+4)           642          1          866.85K  2       0       7596 (+4)           642   ```",2022-04-11T23:09:34+00:00,2022-04-13T08:42:55+00:00,2,https://github.com/stefanprodan/podinfo/issues/188,370.0,2024-06-14T21:28:42+00:00,https://github.com/stefanprodan/podinfo/pull/370,2,0,0,2,2,2,4,0,19078.318888888887,bug,True,False,normal,networking,"[{""filename"": ""deploy/bases/backend/deployment.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""deploy/bases/frontend/deployment.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
stefanprodan/podinfo,14,Setting a port on the PodSpec results in a HTTP server crash,"``` {""level"":""fatal"",""ts"":""2019-03-21T17:48:59.820Z"",""caller"":""api/server.go:127"",""msg"":""HTTP server crashed"",""error"":""listen tcp: address :tcp://10.111.55.252:9898: too many colons in address"",""stacktrace"":""github.com/stefanprodan/k8s-podinfo/pkg/api.(*Server).ListenAndServe.func1\\n\\t/go/src/github.com/stefanprodan/k8s-podinfo/pkg/api/server.go:127""} ```",2019-03-21T17:54:41+00:00,2019-03-21T19:06:10+00:00,8,https://github.com/stefanprodan/podinfo/issues/14,305.0,2023-10-02T17:41:06+00:00,https://github.com/stefanprodan/podinfo/pull/305,1,0,11,12,95,91,2,0,39743.77361111111,,True,False,critical,functional,"[{""filename"": "".github/workflows/release.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""README.md"", ""lines_added"": 13, ""lines_deleted"": 16, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/cue.mod/pkg/timoni.sh/core/v1alpha1/image.cue"", ""lines_added"": 32, ""lines_deleted"": 7, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/cue.mod/pkg/timoni.sh/core/v1alpha1/metadata.cue"", ""lines_added"": 12, ""lines_deleted"": 15, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/cue.mod/pkg/timoni.sh/core/v1alpha1/selector.cue"", ""lines_added"": 21, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/config.cue"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/deployment.cue"", ""lines_added"": 4, ""lines_deleted"": 11, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/hpa.cue"", ""lines_added"": 1, ""lines_deleted"": 8, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/ingress.cue"", ""lines_added"": 1, ""lines_deleted"": 6, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/service.cue"", ""lines_added"": 2, ""lines_deleted"": 7, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/serviceaccount.cue"", ""lines_added"": 1, ""lines_deleted"": 8, ""file_type"": ""other""}, {""filename"": ""timoni/podinfo/templates/servicemonitor.cue"", ""lines_added"": 3, ""lines_deleted"": 10, ""file_type"": ""other""}]",templates,False
stefanprodan/podinfo,9,localhost 8080 error,hi stefan  i was getting while installing   helm install sp/podinfo --name webhook  --namespace=test Error: Get http://localhost:8080/version: dial tcp [::1]:8080: connect: connection refused,2018-12-01T01:30:15+00:00,2018-12-04T05:28:21+00:00,2,https://github.com/stefanprodan/podinfo/issues/9,372.0,2024-06-23T12:09:43+00:00,https://github.com/stefanprodan/podinfo/pull/372,1,0,1,2,2,1,2,0,48754.65777777778,,True,False,normal,networking,"[{""filename"": "".github/workflows/test.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""timoni/podinfo/cue.mod/module.cue"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
permitio/opal,606,BasePolicyWatcherTask: Signal stop if broadcaster fails to connect,"This is blocked by merging that PR - https://github.com/permitio/fastapi_websocket_pubsub/pull/77 And releasing `fastapi_websocket_pubsub` 0.3.8  Until now `EventBroadcaster` wasn't raising an error if listening `Broadcaster` channel failed to connect. Now that it does (see related fix in `fastapi_websocket_pubsub`), we should also detect it in the `BasePolicyWatcherTask` and end that long lived task so the opal-server worker process closes and restarts (rather than remaining in a static erroneous limbo state)  A more fancy implementation would be retrying the failed connection (to avoid unnecessarily restarting the worker), but that could be later improved.",2024-06-26T14:51:46+00:00,2024-07-02T13:52:53+00:00,3,https://github.com/permitio/opal/pull/606,718.0,2024-12-24T15:56:43+00:00,https://github.com/permitio/opal/pull/718,1,0,0,1,18,11,29,0,4345.0825,,True,False,normal,networking,"[{""filename"": ""documentation/package-lock.json"", ""lines_added"": 18, ""lines_deleted"": 11, ""file_type"": ""config""}]",,False
permitio/opal,616,Remove buggy build of cedar-client for testing,This is buggy because has the same step id. Also unnecessary because we don't use that image on test. Also remove standalone client which isn't used in tests.,2024-07-13T11:19:32+00:00,2024-07-13T11:24:42+00:00,1,https://github.com/permitio/opal/pull/616,616.0,2024-07-13T11:24:42+00:00,https://github.com/permitio/opal/pull/616,1,0,0,1,0,26,26,0,0.0861111111111111,,True,False,normal,ui,"[{""filename"": "".github/workflows/on_release.yml"", ""lines_added"": 0, ""lines_deleted"": 26, ""file_type"": ""config""}]",,False
permitio/opal,576,Doc - one doc says only git can be a source for policies but another says they can be sourced from an API server that exposes tar bundles,"On https://docs.opal.ac/tutorials/configure_external_data_sources/#general-background-about-opal_data_config_sources it says the source of data for Policies (and static data) is `Git repository (we are thinking about expanding this to more sources in the near future).` This appears to be outdated because https://www.opal.ac/tutorials/track_an_api_bundle_server/ says `This document describes how to use OPAL for syncing policy (code & static data) sourced from an API server that exposes tar bundles (rather than from a git repo, which is the default policy source).`",2024-04-26T13:14:41+00:00,2024-04-27T06:22:22+00:00,5,https://github.com/permitio/opal/issues/576,659.0,2024-12-02T18:10:17+00:00,https://github.com/permitio/opal/pull/659,1,0,0,1,57,43,100,0,5284.926666666666,bug,True,False,normal,configuration,"[{""filename"": ""documentation/package-lock.json"", ""lines_added"": 57, ""lines_deleted"": 43, ""file_type"": ""config""}]",,False
micronaut-projects/micronaut-core,727,POST multipart/form-data with declarative HTTP client fails," ### Steps to Reproduce  Execute following test:  ``` @Client(""https://api.elasticemail.com/v2/email/"") interface ElasticEmailClient {      @Post(value = ""send"", produces = [MediaType.MULTIPART_FORM_DATA])     fun send(             apiKey: String,             to: String,             subject: String,             from: String? = null,             bodyText: String? = null     ): Mono<Map<String, Any>> }  class ElasticEmailTest {      @Test     fun testSendEmail() {         val ctx = ApplicationContext.build().start()         val client = ctx.getBean(ElasticEmailClient::class.java)         val result = client.send(                 apiKey = ""xxxxx"",                 to = ""xxxx@nmote.com"",                 from = ""xxxxx@nmote.com"",                 subject = ""Test email"",                 bodyText = ""Test 1 2 3""         ).block()     } ```  ### Expected Behaviour  Declarative HTTP client should make a POST request with parameters from send method call mapped to POST request parameters.  ### Actual Behaviour  Fails with following exception:  ``` io.micronaut.http.multipart.MultipartException: The type java.util.LinkedHashMap is not a supported type for a multipart request body  	at io.micronaut.http.client.DefaultHttpClient.buildMultipartRequest(DefaultHttpClient.java:1846) 	at io.micronaut.http.client.DefaultHttpClient.buildNettyRequest(DefaultHttpClient.java:1393) 	at io.micronaut.http.client.DefaultHttpClient.sendRequestThroughChannel(DefaultHttpClient.java:1512) 	at io.micronaut.http.client.DefaultHttpClient.lambda$null$23(DefaultHttpClient.java:1011) 	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:511) 	at io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:504) 	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:483) 	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:424) 	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:103) 	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84) 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:306) 	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:616) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:528) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:482) 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442) 	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 	at java.base/java.lang.Thread.run(Thread.java:834) 	Suppressed: java.lang.Exception: #block terminated with an error 		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:93) 		at reactor.core.publisher.Mono.block(Mono.java:1474) 		at com.nmote.elasticemail.ElasticEmailTest.testSendEmail(ElasticEmailTest.kt:40) 		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 		at java.base/java.lang.reflect.Method.invoke(Method.java:566) 		at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:515) 		at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:115) 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$6(TestMethodTestDescriptor.java:171) 		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72) 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:167) 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:114) 		at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:59) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:105) 		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:95) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:71) 		at java.base/java.util.ArrayList.forEach(ArrayList.java:1540) 		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:110) 		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:95) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:71) 		at java.base/java.util.ArrayList.forEach(ArrayList.java:1540) 		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:38) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$5(NodeTestTask.java:110) 		at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:72) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:95) 		at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:71) 		at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:32) 		at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) 		at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:51) 		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220) 		at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188) 		at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202) 		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181) 		at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128) 		at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:74) 		at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) 		at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) 		at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) ```  ### Environment Information  - **Operating System**: macOS 10.14 - **Micronaut Version:** 1.0.RC2 - **JDK Version:** 1.8 and 11 ",2018-10-13T15:20:24+00:00,2019-02-14T06:47:41+00:00,7,https://github.com/micronaut-projects/micronaut-core/issues/727,11377.0,2024-12-02T14:54:59+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11377,0,0,1,1,1,1,0,0,53807.57638888889,status: under consideration;type: improvement;relates-to: http-client,True,False,normal,database,"[{""filename"": ""gradle/libs.versions.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
micronaut-projects/micronaut-core,416,Response body of HttpClientResponseException is empty(),"- When using the Declarative HTTP Client and a HttpClientResponseException is thrown, the response is always empty() - even when there is a response body.  - The response body is printed in the log output of DefaultHttpClient - But later it says ""Full HTTP response received an empty body""  ``` 2018-07-19 14:42:05 TRACE DefaultHttpClient:1321 - HTTP Client Response Received for Request: POST / .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1322 - Status Code: 400 Bad Request .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Server: Server .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Date: Thu, 19 Jul 2018 12:42:05 GMT .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Content-Type: application/json .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Content-Length: 41 .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Connection: close .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1498 - Vary: Accept-Encoding,User-Agent .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1477 - Response Body .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1478 - ----   2018-07-19 14:42:05 TRACE DefaultHttpClient:1479 - {""message"":null,""reason"":""Unregistered""} .  2018-07-19 14:42:05 TRACE DefaultHttpClient:1480 - ----   2018-07-19 14:42:05 TRACE DefaultHttpClient:195 - Full HTTP response received an empty body .  2018-07-19 14:42:05 TRACE DefaultHttpClient:182 - Unable to convert response body to target type  class .   ```  ",2018-07-19T12:48:13+00:00,2018-08-09T10:40:22+00:00,10,https://github.com/micronaut-projects/micronaut-core/issues/416,11419.0,2024-12-12T16:22:03+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11419,0,0,1,1,1,1,0,0,56115.563888888886,status: in progress;type: improvement;status: acknowledged,True,False,normal,configuration,"[{""filename"": ""gradle/libs.versions.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
line/armeria,5695,Test failure: `com.linecorp.armeria.common.stream.InputStreamStreamMessageTest.thrownInputStream()`,``` org.opentest4j.AssertionFailedError:  Expecting value to be false but was true 	at java.base@17.0.10/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 	at java.base@17.0.10/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77) 	at java.base@17.0.10/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 	at java.base@17.0.10/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) 	at app//com.linecorp.armeria.common.stream.InputStreamStreamMessageTest.thrownInputStream(InputStreamStreamMessageTest.java:450) 	at java.base@17.0.10/java.lang.reflect.Method.invoke(Method.java:568) 	at java.base@17.0.10/java.util.ArrayList.forEach(ArrayList.java:1511) 	at java.base@17.0.10/java.util.ArrayList.forEach(ArrayList.java:1511)  ``` ,2024-05-23T04:54:32+00:00,2024-05-27T02:06:58+00:00,1,https://github.com/line/armeria/issues/5695,5903.0,,https://github.com/line/armeria/pull/5903,1,0,0,1,718,206,924,0,93.20722222222224,,True,False,normal,security,"[{""filename"": ""site/package-lock.json"", ""lines_added"": 718, ""lines_deleted"": 206, ""file_type"": ""config""}]",,False
helidon-io/helidon,9735,4.x: ConsoleHandler misconfigured,"## Environment Details * Helidon Version: 4.1.6 ----------  ## Problem Description  Projects generated with the archetype use the following logging.properties ```properties handlers=java.util.logging.ConsoleHandler java.util.logging.SimpleFormatter.format=%1$tY.%1$tm.%1$td %1$tH:%1$tM:%1$tS.%1$tL %5$s%6$s%n ```  However the `java.util.logging.ConsoleHandler` applies filtering on its own so one needs to add this:  ```properties java.util.logging.ConsoleHandler.level=ALL ```  This means that out of the box the logging.properties does not work as documented. Should we add the property above or should use Helidon's console handler:  ```properties handlers=io.helidon.logging.jul.HelidonConsoleHandler ```  ## Steps to reproduce  Add this to the logging.properties: ```properties io.helidon.webserver.http1.Http1LoggingConnectionListener.level=ALL ```  Make a request, nothing is logged. Expecting something like this:  ``` 2025.02.05 15:55:30.016 [0x296a818b 0x4f757cb8] recv prologue: HttpPrologue[protocol=HTTP, protocolVersion=1.1, method=GET, uriPath=/greet, query=, fragment=] 2025.02.05 15:55:30.021 [0x296a818b 0x4f757cb8] recv headers:  Accept: */* Host: localhost:8080 User-Agent: curl/8.7.1  2025.02.05 15:55:30.045 [0x296a818b 0x4f757cb8] send status: 200 OK 2025.02.05 15:55:30.045 [0x296a818b 0x4f757cb8] send headers:  Content-Length: 26 Content-Type: application/json  2025.02.05 15:55:30.050 [0x296a818b 0x4f757cb8] send data: +--------+-------------------------------------------------+----------------+ |   index|  0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f |            data| +--------+-------------------------------------------------+----------------+ |00000000| 48 54 54 50 2f 31 2e 31 20 32 30 30 20 4f 4b 0d |HTTP/1.1 200 OK.| |00000010| 0a 44 61 74 65 3a 20 57 65 64 2c 20 35 20 46 65 |.Date: Wed, 5 Fe| |00000020| 62 20 32 30 32 35 20 31 35 3a 35 35 3a 33 30 20 |b 2025 15:55:30 | |00000030| 2d 30 38 30 30 0d 0a 43 6f 6e 6e 65 63 74 69 6f |-0800..Connectio| |00000040| 6e 3a 20 6b 65 65 70 2d 61 6c 69 76 65 0d 0a 43 |n: keep-alive..C| |00000050| 6f 6e 74 65 6e 74 2d 4c 65 6e 67 74 68 3a 20 32 |ontent-Length: 2| |00000060| 36 0d 0a 43 6f 6e 74 65 6e 74 2d 54 79 70 65 3a |6..Content-Type:| |00000070| 20 61 70 70 6c 69 63 61 74 69 6f 6e 2f 6a 73 6f | application/jso| |00000080| 6e 0d 0a 0d 0a 7b 22 6d 65 73 73 61 67 65 22 3a |n....{""message"":| |00000090| 22 48 65 6c 6c 6f 20 57 6f 72 6c 64 21 22 7d    |""Hello World!""} | +--------+-------------------------------------------------+----------------+ ```  ",2025-02-05T23:56:12+00:00,2025-02-26T08:14:37+00:00,0,https://github.com/helidon-io/helidon/issues/9735,9830.0,2025-02-26T08:14:36+00:00,https://github.com/helidon-io/helidon/pull/9830,0,0,1,1,3,3,0,0,488.3066666666667,bug;P2;archetypes;logging;4.x,True,False,minor,configuration,"[{""filename"": ""archetypes/archetypes/src/main/archetype/se/common/files/src/main/resources/logging.properties.mustache"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
line/armeria,5690,Test failure: `com.linecorp.armeria.server.Http1ServerDelayedCloseConnectionTest.shouldDelayDisconnectByServerSideIfClientDoesNotHandleConnectionClose()`,"``` java.lang.AssertionError:  Expecting code not to raise a throwable but caught   ""java.net.BindException: Address already in use 	at java.base/sun.nio.ch.Net.bind0(Native Method) 	at java.base/sun.nio.ch.Net.bind(Net.java:555) 	at java.base/sun.nio.ch.Net.bind(Net.java:544) 	at java.base/sun.nio.ch.NioSocketImpl.bind(NioSocketImpl.java:648) 	at java.base/java.net.DelegatingSocketImpl.bind(DelegatingSocketImpl.java:94) 	at java.base/java.net.Socket.bind(Socket.java:682) 	at com.linecorp.armeria.server.Http1ServerDelayedCloseConnectionTest.lambda$shouldDelayDisconnectByServerSideIfClientDoesNotHandleConnectionClose$0(Http1ServerDelayedCloseConnectionTest.java:95) 	at org.assertj.core.api.ThrowableAssert.catchThrowable(ThrowableAssert.java:63) 	at org.assertj.core.api.AssertionsForClassTypes.catchThrowable(AssertionsForClassTypes.java:892) 	at org.assertj.core.api.AssertionsForClassTypes.assertThatCode(AssertionsForClassTypes.java:863) 	at org.assertj.core.api.Assertions.assertThatCode(Assertions.java:1286) 	at com.linecorp.armeria.server.Http1ServerDelayedCloseConnectionTest.shouldDelayDisconnectByServerSideIfClientDoesNotHandleConnectionClose(Http1ServerDelayedCloseConnectionTest.java:95) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:568) 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:728) 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) 	at org.junit.jupiter.engine.extension.SameThreadTimeoutInvocation.proceed(SameThreadTimeoutInvocation.java:45) 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:218) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:214) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:139) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:69) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask ``` ",2024-05-21T01:09:35+00:00,2024-05-22T10:30:04+00:00,0,https://github.com/line/armeria/issues/5690,5903.0,,https://github.com/line/armeria/pull/5903,1,0,0,1,718,206,924,0,33.341388888888886,,True,False,normal,networking,"[{""filename"": ""site/package-lock.json"", ""lines_added"": 718, ""lines_deleted"": 206, ""file_type"": ""config""}]",,False
spring-cloud/spring-cloud-kubernetes,1813,SSLPeerUnverifiedException Hostname fd33:1a73:fa8f::1 not verified after upgrade to Boot 3.4.0 and Cloud 2024.0.0,"**Describe the bug** After the upgrade from Spring Boot 3.3.6 to 3.4.0 and Spring Cloud 2023.0.4 to 2024.0.0 we're experiencing issues regarding hostname verification which read like  ``` javax.net.ssl.SSLPeerUnverifiedException: Hostname fd33:1a73:fa8f::1 not verified:     certificate: sha256/bLcj0Q+HP/EF+4njk0xrQvqb/KtOHnZa2xf+rl9ldkc=     DN: CN=kube-apiserver     subjectAltNames: [fd33:1a73:fa8f:0:0:0:0:1, 2a05:d014:396:cd05:0:0:0:e781, 172.16.98.175, 55c2d4e83b3377534d8c22d619c3cb94.gr7.eu-central-1.eks.amazonaws.com, ip-172-16-98-175.eu-central-1.compute.internal, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local] 	at okhttp3.internal.connection.RealConnection.connectTls(RealConnection.java:334) 	at okhttp3.internal.connection.RealConnection.establishProtocol(RealConnection.java:284) 	at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:169)         [...] 	at java.lang.Thread.run(Thread.java:1570)  Wrapped by: java.io.IOException: Hostname fd33:1a73:fa8f::1 not verified:     certificate: sha256/bLcj0Q+HP/EF+4njk0xrQvqb/KtOHnZa2xf+rl9ldkc=      DN: CN=kube-apiserver     subjectAltNames: [fd33:1a73:fa8f:0:0:0:0:1, 2a05:d014:396:cd05:0:0:0:e781, 172.16.98.175, 55c2d4e83b3377534d8c22d619c3cb94.gr7.eu-central-1.eks.amazonaws.com, ip-172-16-98-175.eu-central-1.compute.internal, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local]  	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:504) 	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:524) 	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleGet(OperationSupport.java:467) 	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleGet(BaseOperation.java:792) 	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.requireFromServer(BaseOperation.java:193) 	... 20 common frames omitted  Wrapped by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Pod]  with name: [offer-attribute-assignor-6778d89688-pdm8h]  in namespace: [offer-attribute-assignor]  failed. 	at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:159) 	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.requireFromServer(BaseOperation.java:195) 	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.get(BaseOperation.java:149) 	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.isReady(BaseOperation.java:919) 	... 5 frames excluded 	... 13 common frames omitted  Wrapped by: org.springframework.context.ApplicationContextException: Failed to start bean 'leaderInitiator' 	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:326) 	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:510) 	at java.lang.Iterable.forEach(Iterable.java:75) 	... 10 frames excluded 	at de.idealo.orca.attribute.ApplicationKt.main(Application.kt:24) ```  There are various bug reports similar to this one available but they all date years back and are already closed. I suspect the crucial change anywhere within `io.fabric8:kubernetes-client` between versions 6.9.2 (Cloud 2023.0.4) and 6.13.4 (Cloud 2024.0.0).  From my understanding the address `fd33:1a73:fa8f::1` should be fine regarding verification. Did we miss something? ",2024-12-04T15:40:53+00:00,2025-02-14T17:25:49+00:00,19,https://github.com/spring-cloud/spring-cloud-kubernetes/issues/1813,1875.0,2025-02-14T17:25:06+00:00,https://github.com/spring-cloud/spring-cloud-kubernetes/pull/1875,2,0,0,2,11,5,16,0,1729.7369444444444,feedback-provided,True,False,normal,networking,"[{""filename"": ""spring-cloud-kubernetes-fabric8-autoconfig/pom.xml"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spring-cloud-kubernetes-fabric8-leader/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""config""}]",,False
camunda/camunda,27919,[Backport stable/8.4] ci: remove broken benchmark measurement support,# Description Backport of #27911 to `stable/8.4`.  relates to  original author: @lenaschoenburg,2025-02-10T14:26:26+00:00,2025-03-14T11:44:06+00:00,2,https://github.com/camunda/camunda/pull/27919,27919.0,2025-03-14T11:44:06+00:00,https://github.com/camunda/camunda/pull/27919,2,0,0,2,0,30,30,0,765.2944444444445,,True,False,critical,security,"[{""filename"": "".github/workflows/zeebe-benchmark.yml"", ""lines_added"": 0, ""lines_deleted"": 29, ""file_type"": ""config""}, {""filename"": "".github/workflows/zeebe-pr-benchmark.yaml"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
camunda/camunda,27918,[Backport stable/8.3] ci: remove broken benchmark measurement support,# Description Backport of #27911 to `stable/8.3`.  relates to  original author: @lenaschoenburg,2025-02-10T14:26:19+00:00,2025-03-14T10:13:46+00:00,3,https://github.com/camunda/camunda/pull/27918,27918.0,2025-03-14T10:13:46+00:00,https://github.com/camunda/camunda/pull/27918,2,0,0,2,0,30,30,0,763.7908333333334,,True,False,critical,security,"[{""filename"": "".github/workflows/zeebe-benchmark.yml"", ""lines_added"": 0, ""lines_deleted"": 29, ""file_type"": ""config""}, {""filename"": "".github/workflows/zeebe-pr-benchmark.yaml"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
camunda/camunda,27920,[Backport stable/8.5] ci: remove broken benchmark measurement support,# Description Backport of #27911 to `stable/8.5`.  relates to  original author: @lenaschoenburg,2025-02-10T14:26:33+00:00,2025-03-14T09:51:05+00:00,2,https://github.com/camunda/camunda/pull/27920,27920.0,2025-03-14T09:51:05+00:00,https://github.com/camunda/camunda/pull/27920,3,0,0,3,0,32,32,0,763.4088888888889,component/zeebe,True,False,critical,security,"[{""filename"": "".github/workflows/zeebe-benchmark.yml"", ""lines_added"": 0, ""lines_deleted"": 29, ""file_type"": ""config""}, {""filename"": "".github/workflows/zeebe-medic-benchmarks.yml"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": "".github/workflows/zeebe-pr-benchmark.yaml"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
camunda/camunda,29004,Optimize UpdateIndexStepResumesReindexOperationsIT is failing for OpenSearch,"Engineering DRI: @oleksandr-kriuchenko-lohika  ### Describe the bug  <!-- A clear and concise description of what the observation is and the problem it causes. Screenshots and recordings can be used here to aid this description --> When Optimize CI is run on a branch or PR, there is a consistent failure in `UpdateIndexStepResumesReindexOperationsIT` when running `Upgrade (opensearch)` The test does not give any errors, but rather times out and the test will fail  ### Expected behaviour  <!-- A description of the behaviour that you would expect --> `UpdateIndexStepResumesReindexOperationsIT` does not time out and the test passes  ### To Reproduce  <!-- Clear steps to reproduce the behavior. This will be later used to review the fix --> Open a PR and run Optimize CI  ### Environment observed:  - Optimize mode(s): - Optimize version(s): - Database(s) (Elasticsearch/OpenSearch):  ### Links  <!-- Add links to related SUPPORT/SEC tickets or other issues  -->  ### Breakdown  <!-- A breakdown of tasks that need to be completed in order for this to be ready for review. --> <!-- - [ ] #123 - [ ] Step X -->  ```[tasklist] ### Pull Requests - [ ] https://github.com/camunda/camunda/pull/29165 ```  ## Bug Lifecycle  For managing the issue lifecycle, please use the workflow commands. You can see the available commands by writing `/help` as a comment on this issue.  ### QA Verification  Is this bug reproducible?  - [ ] Yes. If so:   - [ ] The bug description is clear   - [ ] The steps to reproduce are clear   - [ ] The environments observed are correct and complete - [ ] No  ### Review  #### Engineering Review  - [ ] All code targeting the main branch has been reviewed by at least one Engineer - [ ] The PR targeting the main branch has been approved by the reviewing Engineer - [ ] If the API has changed, the API documentation is updated - [ ] All other PRs (docs, controller etc.) in the breakdown have been approved  #### QA Review  - [ ] The change is implemented as described on all target environments/versions/modes - [ ] The documentation changes are as expected  ### Completion  - [ ] All Review stages are successfully completed - [ ] All associated PRs are merged to the main branch(es) and stable branches - [ ] The correct version labels are applied to the issue",2025-02-28T16:19:58+00:00,2025-03-12T23:54:25+00:00,13,https://github.com/camunda/camunda/issues/29004,29165.0,2025-03-12T23:54:24+00:00,https://github.com/camunda/camunda/pull/29165,2,0,0,2,7,1,8,0,295.5738888888889,kind/bug;component/optimize;qa/pendingVerification;version:8.6.11,True,False,normal,database,"[{""filename"": "".github/renovate.json"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""parent/pom.xml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
camunda/camunda,27989,fix: CVE-2024-57699 ,(fix: CVE-2024-57699)  Present in all stable versions of Tasklist 8.3 to 8.6  ,2025-02-11T11:41:46+00:00,2025-02-11T17:41:55+00:00,0,https://github.com/camunda/camunda/issues/27989,27995.0,2025-02-12T15:37:12+00:00,https://github.com/camunda/camunda/pull/27995,2,0,0,2,6,1,7,0,27.92388888888889,version:8.8.0-alpha2,True,False,normal,functional,"[{""filename"": "".github/renovate.json"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""parent/pom.xml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
radius-project/radius,8918,Scheduled long running test failed - Run ID: 13951165402,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13951165402).  [AB#15067](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15067)",2025-03-19T16:14:39+00:00,2025-03-19T19:07:47+00:00,0,https://github.com/radius-project/radius/issues/8918,7636.0,2024-05-28T17:27:17+00:00,https://github.com/radius-project/radius/pull/7636,6,0,0,6,3532,4263,7795,0,-7078.789444444445,test-failure,True,False,major,networking,"[{""filename"": "".github/workflows/lint.yaml"", ""lines_added"": 8, ""lines_deleted"": 8, ""file_type"": ""config""}, {""filename"": "".github/workflows/publish-docs.yaml"", ""lines_added"": 10, ""lines_deleted"": 9, ""file_type"": ""config""}, {""filename"": ""hack/bicep-types-radius/src/autorest.bicep/package-lock.json"", ""lines_added"": 2650, ""lines_deleted"": 3512, ""file_type"": ""config""}, {""filename"": ""hack/bicep-types-radius/src/autorest.bicep/package.json"", ""lines_added"": 16, ""lines_deleted"": 16, ""file_type"": ""config""}, {""filename"": ""hack/bicep-types-radius/src/generator/package-lock.json"", ""lines_added"": 832, ""lines_deleted"": 702, ""file_type"": ""config""}, {""filename"": ""hack/bicep-types-radius/src/generator/package.json"", ""lines_added"": 16, ""lines_deleted"": 16, ""file_type"": ""config""}]",,False
radius-project/radius,8915,Scheduled long running test failed - Run ID: 13948455888,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13948455888).  [AB#15066](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15066)",2025-03-19T14:11:51+00:00,2025-03-19T19:07:46+00:00,0,https://github.com/radius-project/radius/issues/8915,7634.0,2024-05-24T20:48:56+00:00,https://github.com/radius-project/radius/pull/7634,1,0,1,2,88,2,4,0,-7169.381944444444,test-failure,True,False,major,networking,"[{""filename"": ""docs/release-notes/v0.34.0.md"", ""lines_added"": 86, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""versions.yaml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}]",,False
radius-project/radius,8914,Scheduled long running test failed - Run ID: 13946045211,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13946045211).  [AB#15065](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15065)",2025-03-19T12:15:34+00:00,2025-03-19T19:07:46+00:00,0,https://github.com/radius-project/radius/issues/8914,7611.0,,https://github.com/radius-project/radius/pull/7611,3,0,0,3,43,25,68,0,6.87,test-failure,True,False,major,networking,"[{""filename"": "".github/workflows/build.yaml"", ""lines_added"": 25, ""lines_deleted"": 19, ""file_type"": ""config""}, {""filename"": "".github/workflows/long-running-azure.yaml"", ""lines_added"": 11, ""lines_deleted"": 5, ""file_type"": ""config""}, {""filename"": "".github/workflows/purge-test-resources.yaml"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
radius-project/radius,8912,Scheduled long running test failed - Run ID: 13941613186,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13941613186).  [AB#15063](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15063)",2025-03-19T08:14:00+00:00,2025-03-19T19:07:45+00:00,0,https://github.com/radius-project/radius/issues/8912,7616.0,2024-05-20T18:48:25+00:00,https://github.com/radius-project/radius/pull/7616,1,0,0,1,1,0,1,0,-7261.426388888889,test-failure,True,False,major,networking,"[{""filename"": "".github/workflows/purge-test-resources.yaml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
jina-ai/serve,5945,ci: debug with python dev mode,"<!---Decomposing the complex issue into subtasks can help you build it step-by-step. Thanks for your pull request! :rocket: ---> <!---We know that dev life is hectic, but **please provide a (brief) description** of what your PR does, and how it does it. **Otherwise, your PR cannot be reviewed!** ---> <!---This policy was agreed upon in a past company retro, and makes everyone's life a little easier. Thanks for your collaboration!--->  **Goals:** <!---https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword--->  - resolves #ISSUE-NUMBER - ... - ...  - ... - ... - [ ] check and update documentation. See [guide](https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md#-contributing-documentation) and ask the team. ",2023-06-30T12:31:18+00:00,2023-07-05T07:15:19+00:00,1,https://github.com/jina-ai/serve/pull/5945,5945.0,,https://github.com/jina-ai/serve/pull/5945,1,0,0,1,1,1,2,0,114.73361111111112,area/housekeeping;area/cicd;size/XS,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
project-oak/oak,5000,fix bug in provenance generation script,Change-Id: Ie3df48c899c4b90e39af4f599f8c7a9b05cfda17  https://github.com/project-oak/oak/pull/4995 broke this step,2024-04-05T16:17:33+00:00,2024-04-05T16:22:26+00:00,0,https://github.com/project-oak/oak/pull/5000,5000.0,2024-04-05T16:22:26+00:00,https://github.com/project-oak/oak/pull/5000,1,0,0,1,1,1,2,0,0.0813888888888888,,True,False,normal,functional,"[{""filename"": "".github/workflows/reusable_provenance.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openclarity/openclarity,908,fix(helm): installation error with externalPostgresql,"Fix the following error due to extra curly brackets:  │     * Deployment.apps ""openclarity-apiserver"" is invalid: [spec.template.spec.containers[0].env[3].valueFrom.secretKeyRef.name: Invalid value: ""openclarity }}"": a lowercase RFC 1123 subdomain must consist of l │  ## Description  <!-- Please provide a meaningful description of what this change will do, or is for. Bonus points for including links to related issues, other PRs, or technical references and some before/after screenshots if the change affects the UI.  | Before | After | | :---: | :---: | | 🖼️ | 🖼️ |  Note that by _not_ including a description, you are asking reviewers to do extra work to understand the context of this change, which may lead to your PR taking much longer to review, or result in it not being reviewed at all. -->  ## Type of Change  - [x] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [x] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [x] Existing issues have been referenced (where applicable) - [x] I have verified this change is not present in other open pull requests - [x] Functionality is documented - [x] All code style checks pass - [x] New code contribution is covered by automated tests - [x] All new and existing tests pass ",2024-10-31T21:26:25+00:00,2024-11-04T08:59:53+00:00,0,https://github.com/openclarity/openclarity/pull/908,908.0,2024-11-04T08:59:53+00:00,https://github.com/openclarity/openclarity/pull/908,1,0,0,1,3,3,6,0,83.55777777777777,size/XS,True,False,normal,database,"[{""filename"": ""installation/kubernetes/helm/openclarity/templates/apiserver/deployment.yaml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}]",templates,False
openclarity/openclarity,888,fix(helm): external postgresql port quotes,"## Description  <!-- Please provide a meaningful description of what this change will do, or is for. Bonus points for including links to related issues, other PRs, or technical references and some before/after screenshots if the change affects the UI.  | Before | After | | :---: | :---: | | 🖼️ | 🖼️ |  Note that by _not_ including a description, you are asking reviewers to do extra work to understand the context of this change, which may lead to your PR taking much longer to review, or result in it not being reviewed at all. -->  Fixes #884  ## Type of Change  - [X] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [X] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [X] Existing issues have been referenced (where applicable) - [X] I have verified this change is not present in other open pull requests - [ ] Functionality is documented - [x] All code style checks pass - [ ] New code contribution is covered by automated tests - [x] All new and existing tests pass ",2024-10-17T13:30:44+00:00,2024-10-21T09:17:49+00:00,0,https://github.com/openclarity/openclarity/pull/888,888.0,2024-10-21T09:17:49+00:00,https://github.com/openclarity/openclarity/pull/888,1,0,0,1,1,1,2,0,91.78472222222224,bug;size/XS,True,False,normal,database,"[{""filename"": ""installation/kubernetes/helm/openclarity/templates/apiserver/deployment.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",templates,False
openclarity/openclarity,850,fix(ci): PR size labeler,"## Description  Fixes https://github.com/openclarity/openclarity/issues/834  <!-- Please provide a meaningful description of what this change will do, or is for. Bonus points for including links to related issues, other PRs, or technical references and some before/after screenshots if the change affects the UI.  | Before | After | | :---: | :---: | | 🖼️ | 🖼️ |  Note that by _not_ including a description, you are asking reviewers to do extra work to understand the context of this change, which may lead to your PR taking much longer to review, or result in it not being reviewed at all. -->  ## Type of Change  - [ ] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [ ] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [ ] Existing issues have been referenced (where applicable) - [ ] I have verified this change is not present in other open pull requests - [ ] Functionality is documented - [ ] All code style checks pass - [ ] New code contribution is covered by automated tests - [ ] All new and existing tests pass ",2024-09-10T07:24:49+00:00,2024-09-10T08:37:01+00:00,0,https://github.com/openclarity/openclarity/pull/850,850.0,2024-09-10T08:37:01+00:00,https://github.com/openclarity/openclarity/pull/850,1,0,0,1,2,0,2,0,1.2033333333333334,github_actions;kind/bug,True,False,normal,ui,"[{""filename"": "".github/workflows/pr.yml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}]",,False
kanisterio/kanister,2988,deps(github): bump tj-actions/changed-files from 44.5.5 to 44.5.6,"Bumps [tj-actions/changed-files](https://github.com/tj-actions/changed-files) from 44.5.5 to 44.5.6. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/tj-actions/changed-files/releases"">tj-actions/changed-files's releases</a>.</em></p> <blockquote> <h2>v44.5.6</h2> <h2>What's Changed</h2> <ul> <li>chore(deps): update typescript-eslint monorepo to v7.14.1 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2169"">tj-actions/changed-files#2169</a></li> <li>Upgraded to v44.5.5 by <a href=""https://github.com/tj-actions-bot""><code>@​tj-actions-bot</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2168"">tj-actions/changed-files#2168</a></li> <li>chore(deps): update dependency <code>@​types/node</code> to v20.14.9 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2170"">tj-actions/changed-files#2170</a></li> <li>chore(deps): update dependency <code>@​types/micromatch</code> to v4.0.8 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2171"">tj-actions/changed-files#2171</a></li> <li>chore(deps): update dependency <code>@​types/lodash</code> to v4.17.6 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2172"">tj-actions/changed-files#2172</a></li> <li>chore(deps): update actions/checkout action to v3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2173"">tj-actions/changed-files#2173</a></li> <li>chore(deps): update actions/checkout action to v4 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2174"">tj-actions/changed-files#2174</a></li> <li>chore(deps): update dependency <code>@​types/micromatch</code> to v4.0.9 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2175"">tj-actions/changed-files#2175</a></li> <li>chore(deps-dev): bump <code>@​typescript-eslint/parser</code> from 7.14.1 to 7.15.0 by <a href=""https://github.com/dependabot""><code>@​dependabot</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2181"">tj-actions/changed-files#2181</a></li> <li>chore(deps): update dependency <code>@​typescript-eslint/eslint-plugin</code> to v7.15.0 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2178"">tj-actions/changed-files#2178</a></li> <li>chore(deps): update dependency <code>@​types/node</code> to v20.14.10 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2182"">tj-actions/changed-files#2182</a></li> <li>chore(deps): update dependency ts-jest to v29.2.0 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2184"">tj-actions/changed-files#2184</a></li> <li>chore(deps): update typescript-eslint monorepo to v7.16.0 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2185"">tj-actions/changed-files#2185</a></li> <li>chore(deps): update actions/setup-node action to v4.0.3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2188"">tj-actions/changed-files#2188</a></li> <li>chore(deps): update dependency ts-jest to v29.2.1 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2189"">tj-actions/changed-files#2189</a></li> <li>chore(deps): update dependency ts-jest to v29.2.2 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2190"">tj-actions/changed-files#2190</a></li> <li>chore(deps): update codacy/codacy-analysis-cli-action action to v4.4.2 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2191"">tj-actions/changed-files#2191</a></li> <li>chore(deps): update codacy/codacy-analysis-cli-action action to v4.4.4 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2192"">tj-actions/changed-files#2192</a></li> <li>chore(deps): update codacy/codacy-analysis-cli-action action to v4.4.5 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2193"">tj-actions/changed-files#2193</a></li> <li>chore(deps): update dependency prettier to v3.3.3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2194"">tj-actions/changed-files#2194</a></li> <li>chore(deps): update typescript-eslint monorepo to v7.16.1 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2196"">tj-actions/changed-files#2196</a></li> <li>chore(deps): update dependency <code>@​types/lodash</code> to v4.17.7 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2197"">tj-actions/changed-files#2197</a></li> <li>chore(deps): update dependency <code>@​types/node</code> to v20.14.11 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2198"">tj-actions/changed-files#2198</a></li> <li>fix(deps): update dependency <code>@​octokit/rest</code> to v21.0.1 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2199"">tj-actions/changed-files#2199</a></li> <li>chore(deps): update dependency eslint-plugin-prettier to v5.2.1 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2200"">tj-actions/changed-files#2200</a></li> <li>chore(deps): update dependency ts-jest to v29.2.3 by <a href=""https://github.com/renovate""><code>@​renovate</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2203"">tj-actions/changed-files#2203</a></li> <li>fix: remove unused code by <a href=""https://github.com/jackton1""><code>@​jackton1</code></a> in <a href=""https://redirect.github.com/tj-actions/changed-files/pull/2202"">tj-actions/changed-files#2202</a></li> </ul> <p><strong>Full Changelog</strong>: <a href=""https://github.com/tj-actions/changed-files/compare/v44...v44.5.6"">https://github.com/tj-actions/changed-files/compare/v44...v44.5.6</a></p> </blockquote> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/tj-actions/changed-files/commit/6b2903bdce6310cfbddd87c418f253cf29b2dec9""><code>6b2903b</code></a> fix: remove unused code (<a href=""https://redirect.github.com/tj-actions/changed-files/issues/2202"">#2202</a>)</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/dc82617b247f7d2e5734aa9c0a0e5b800e58c1d8""><code>dc82617</code></a> chore(deps): update dependency ts-jest to v29.2.3</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/1797e9a1ea6596c1ae24e354e3d27b60332efcda""><code>1797e9a</code></a> chore(deps): update dependency eslint-plugin-prettier to v5.2.1</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/56b5e13a32a8099659d9f0de4b06bc1fe9b60d3b""><code>56b5e13</code></a> fix(deps): update dependency <code>@​octokit/rest</code> to v21.0.1</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/9e63ef55ed3d63a5d2a83c405c415505f18fc6ea""><code>9e63ef5</code></a> chore(deps): update dependency <code>@​types/node</code> to v20.14.11</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/f1afcaced6b0126bdec60471b68db60065446b50""><code>f1afcac</code></a> chore(deps): update dependency <code>@​types/lodash</code> to v4.17.7</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/e42a3d1b0ac3775d2029e7f164e9969b2d0b70a3""><code>e42a3d1</code></a> chore(deps): update typescript-eslint monorepo to v7.16.1</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/f0eedff3627914fdcdf6458fad3b3d27fdff3f4a""><code>f0eedff</code></a> chore(deps): update dependency prettier to v3.3.3</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/f6d7b721fdaf52c025f2ffecd8ed077ef7dbb8a8""><code>f6d7b72</code></a> chore(deps): update codacy/codacy-analysis-cli-action action to v4.4.5</li> <li><a href=""https://github.com/tj-actions/changed-files/commit/6f59461624f15e6b5588a64c5e2e6116135e331b""><code>6f59461</code></a> chore(deps): update codacy/codacy-analysis-cli-action action to v4.4.4</li> <li>Additional commits viewable in <a href=""https://github.com/tj-actions/changed-files/compare/cc733854b1f224978ef800d29e4709d5ee2883e4...6b2903bdce6310cfbddd87c418f253cf29b2dec9"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tj-actions/changed-files&package-manager=github_actions&previous-version=44.5.5&new-version=44.5.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-07-18T22:50:43+00:00,2024-07-19T07:03:13+00:00,2,https://github.com/kanisterio/kanister/pull/2988,2988.0,,https://github.com/kanisterio/kanister/pull/2988,1,0,0,1,1,1,2,0,8.208333333333334,dependencies;security;github_actions,True,False,major,security,"[{""filename"": "".github/workflows/atlas-image-build.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
lfn-cnti/testsuite,2215,[BUG] Some spec tests make use of environment_cleanup function that does nothing,"**Describe the bug** This is another artifact of #2120 that was missed. Some spec test files used before_all/after_each constructs that ran the `ShellCmd.environment_cleanup()` function, the issue is that this function makes use of the now deleted cleanup function, effectively making this do nothing at present.  I am not sure what the intention of `environemnt_cleanup` was, something leads me to believe that it should have uninstalled both the CNF and other testsuite tools (`clustertools`, etc.), but this seems unnecessary and would only cause the spec tests to run longer. After reviewing the affected spec tests I came to the conclusion that:  1. Some occurrences of `envinronment_cleanup` are completely unnecessary. 2. Other can be replaced by just using `cnf_uninstall`.  This leads me to believe that we can safely remove/replace this function as it currently does ""nothing"". ",2025-01-28T09:06:08+00:00,2025-02-05T22:46:00+00:00,0,https://github.com/lfn-cnti/testsuite/issues/2215,2216.0,2025-02-05T15:20:36+00:00,https://github.com/lfn-cnti/testsuite/pull/2216,0,0,9,9,6,32,0,0,198.2411111111111,bug,True,False,normal,ui,"[{""filename"": ""spec/cluster_setup_spec.cr"", ""lines_added"": 5, ""lines_deleted"": 7, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""spec/platform/hardware_and_scheduler_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""spec/platform/platform_spec.cr"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""spec/setup_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""spec/spec_helper.cr"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""spec/utils/cnf_manager_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""spec/workload/configuration_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""spec/workload/installability_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,2206,[BUG] Workload test helm_chart_valid (helm lint) fails,"**Describe the bug** Workload test helm_chart_valid fails with CNF testsuite 1.3.3. The helm lint test is successfully when executed from CLI.  **To Reproduce** Steps to reproduce the behavior:  1. Deploy AUT with local Helm chart tarball 2. Run cnf-testsuite helm_chart_valid   **Expected behavior** Workload test helm_chart_valid is successful.  Directory content: ``` ~/.cnf-testsuite # tree cnti cnti ├── cnf-testsuite.yml ├── config-input.yaml ├── eks-values.yaml ├── microk8s-values.yaml ├── prod-dns24-24.09.0.tgz <- Helm chart └── values.yaml ```  Config file: ``` ~/.cnf-testsuite # cat cnti/cnf-testsuite.yml --- allowlist_helm_chart_container_names: [] # [LIST_OF_CONTAINERS_ALLOWED_TO_RUN_PRIVLIDGED] helm_directory: prod-dns24-24.09.0.tgz #helm_directory: chart helm_install_namespace: prod-system release_name: prod-dnstest service_name: prod-dnssig-cnti helm_values: '-f cnti/microk8s-values.yaml -f cnti/config-input.yaml -f cnti/values.yaml' container_names: - name: prod-dnssig-cnti   rolling_update_test_tag: 24.09.1-SNAPSHOT   rolling_downgrade_test_tag: 24.09.99   rolling_version_change_test_tag: 24.09.1-SNAPSHOT   rollback_from_tag: 24.09.1-SNAPSHOT ```  Offline Helm lint test: ``` ~/.cnf-testsuite # helm version version.BuildInfo{Version:""v3.16.4"", GitCommit:""7877b45b63f95635153b29a42c0c2f4273ec45ca"", GitTreeState:""clean"", GoVersion:""go1.22.7""} ~/.cnf-testsuite # helm lint -f cnti/microk8s-values.yaml -f cnti/config-input.yaml -f cnti/values.yaml cnti/prod-dns24-24.09.0.tgz ==> Linting cnti/prod-dns24-24.09.0.tgz [INFO] Chart.yaml: icon is recommended 1 chart(s) linted, 0 chart(s) failed <- SUCCESS ```  AUT deployment: ``` ~/.cnf-testsuite # ./test-aut.sh dns CNF configuration validated 📋 Successfully created directories for cnf-testsuite KUBECONFIG is already set. cnf-testsuite namespace already exists on the Kubernetes cluster ClusterTools installed cnf setup start Waiting for resource availability, timeout for each resource is 1800 seconds All CNF resources are up! Successfully setup prod-dnstest cnf setup complete AUT deployed.  ~/.cnf-testsuite # kubectl get pods -A | grep dnssig-cnti prod-system      prod-dnssig-cnti-0                 1/1     Running   0          82s ```  CNF test: ``` ~/.cnf-testsuite # cnf-testsuite helm_chart_valid Successfully created directories for cnf-testsuite 🎬 Testing: [helm_chart_valid] ✖️  FAILED: [helm_chart_valid] Helm Chart prod-dns24-24.09.0.tgz Lint Failed ⎈📝☑ ```  CNF test with debug info: ``` ~/.cnf-testsuite # cnf-testsuite helm_chart_valid -l debug Successfully created directories for cnf-testsuite D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: helm_v3?: Regex::MatchData(""BuildInfo{Version:\\""v3.16.4\\"", GitCommit:\\""7877b45b63f95635153b29a42c0c2f4273ec45ca\\"", GitTreeState:\\""clean\\"", GoVersion:\\""go1.22.7\\"""" 1:""v3.16.4"" 2:""16."") I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: Globally installed helm satisfies required version. Skipping local helm install. I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_runner args: #<Sam::Args:0x7747de594a00 @arr=[], @named_args={}> D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: Results.file created: results/cnf-testsuite-results-20250109-192604-138.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_config_list I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: find response: [""cnfs/prod-dnstest/cnf-testsuite.yml""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_installed?  true I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: check_cnf_config args: #<Sam::Args:0x7747de594a00 @arr=[], @named_args={}> I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: check_cnf_config cnf:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_config_list I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" 🎬 Testing: [helm_chart_valid] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: find response: [""cnfs/prod-dnstest/cnf-testsuite.yml""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: CNF configs found: 1 D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: single_task_runner args: #<Sam::Args:0x7747de594360 @arr=[], @named_args={""cnf-config"" => ""cnfs/prod-dnstest/cnf-testsuite.yml""}> D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: parse_config_yml config_yml_path: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: generate_and_set_release_name I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: generate_and_set_release_name config_yml_path: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: parsed_config_file: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: src_helm_directory: prod-dns24-24.09.0.tgz D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: predefined_release_name: prod-dnstest I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: parsed_config_file: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_installation_method I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_installation_method config: #<Totem::Config:0x7747de4b9c80> I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_installation_method config: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: directory_parameter_split : prod-dns24-24.09.0.tgz I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: directory_parameter_split : prod-dns24-24.09.0.tgz I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: directory : prod-dns24-24.09.0.tgz parameters:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: release_name: prod-dnstest I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: helm_directory: prod-dns24-24.09.0.tgz I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: manifest_directory:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: Building helm_directory and manifest_directory full paths I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: full_helm_directory: /root/.cnf-testsuite/cnfs/prod-dnstest/prod-dns24-24.09.0.tgz exists? false I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: full_manifest_directory: /root/.cnf-testsuite/cnfs/prod-dnstest/ exists? true D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: install_type: helm_chart not found in cnfs/prod-dnstest/cnf-testsuite.yml D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: install type count install_type: helm_directory D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: install_type: manifest_directory not found in cnfs/prod-dnstest/cnf-testsuite.yml D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: installation_type_count: 1 I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: helm_directory not empty, using: /root/.cnf-testsuite/cnfs/prod-dnstest/prod-dns24-24.09.0.tgz I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_destination_dir config_file: cnfs/prod-dnstest/cnf-testsuite.yml I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: parsed_config_file: cnfs/prod-dnstest/cnf-testsuite.yml D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: cnf_destination_dir parsed_config_file config: #<Totem::Config:0x7747de4b9be0> I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: release_name: prod-dnstest I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf destination dir: /root/.cnf-testsuite/cnfs/prod-dnstest I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: NOT USING EXPORTED CHART PATH I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite-helm_chart_valid: Starting test D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite-helm_chart_valid: cnf_config: #<CNFManager::Config:0x7747de4ba8c0> I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: parsed_config_file: cnfs/prod-dnstest/cnf-testsuite.yml D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite-helm_chart_valid: current dir: /root/.cnf-testsuite D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: helm_v3?: Regex::MatchData(""BuildInfo{Version:\\""v3.16.4\\"", GitCommit:\\""7877b45b63f95635153b29a42c0c2f4273ec45ca\\"", GitTreeState:\\""clean\\"", GoVersion:\\""go1.22.7\\"""" 1:""v3.16.4"" 2:""16."") I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf_destination_dir config_file: cnfs/prod-dnstest/ I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: parsed_config_file: cnfs/prod-dnstest//cnf-testsuite.yml D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: cnf_destination_dir parsed_config_file config: #<Totem::Config:0x7747de4b9aa0> I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: release_name: prod-dnstest I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cnf destination dir: /root/.cnf-testsuite/cnfs/prod-dnstest ✖️  FAILED: [helm_chart_valid] Helm Chart prod-dns24-24.09.0.tgz Lint Failed ⎈📝☑ D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: task helm_chart_valid emoji: ⎈📝☑ I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type_by_task I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: points: {""name"" => ""helm_chart_valid"", ""emoji"" => ""⎈📝☑"", ""tags"" => [""compatibility"", ""dynamic"", ""workload"", ""normal""]} D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: points tags: [""compatibility"", ""dynamic"", ""workload"", ""normal""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: resp: [""compatibility"", ""dynamic"", ""workload"", ""normal""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: compatibility acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: dynamic acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: workload acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: normal acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type: normal I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: cmd: /root/.cnf-testsuite/cnf-testsuite helm_chart_valid I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite-helm_chart_valid: task_runtime=00:00:00.115519760; start_time=2025-01-09 19:26:04 UTC; end_time:2025-01-09 19:26:04 UTC I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type_by_task I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: points: {""name"" => ""helm_chart_valid"", ""emoji"" => ""⎈📝☑"", ""tags"" => [""compatibility"", ""dynamic"", ""workload"", ""normal""]} D, [2025-01-09 19:26:04 UTC #12719] DEBUG -- cnf-testsuite: points tags: [""compatibility"", ""dynamic"", ""workload"", ""normal""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: resp: [""compatibility"", ""dynamic"", ""workload"", ""normal""] I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: compatibility acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: dynamic acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: workload acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type x: normal acc:  I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: task_type: normal I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: upsert_task: task: helm_chart_valid has status: failed and is awarded: 0 points. Runtime: 00:00:00.115519760 I, [2025-01-09 19:26:04 UTC #12719]  INFO -- cnf-testsuite: results yaml: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""v1.3.3"", ""status"" => nil, ""command"" => ""/root/.cnf-testsuite/cnf-testsuite helm_chart_valid"", ""points"" => nil, ""exit_code"" => 0, ""items"" => [{""name"" => ""helm_chart_valid"", ""status"" => ""failed"", ""type"" => ""normal"", ""points"" => 0}]} ```    ",2025-01-09T19:41:15+00:00,2025-02-05T09:32:55+00:00,6,https://github.com/lfn-cnti/testsuite/issues/2206,2214.0,2025-02-05T07:56:12+00:00,https://github.com/lfn-cnti/testsuite/pull/2214,6,0,2,8,107,12,49,0,636.2491666666666,bug,True,False,normal,configuration,"[{""filename"": ""sample-cnfs/sample_conditional_values_file/chart/Chart.yaml"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample_conditional_values_file/chart/templates/deployment.yaml"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample_conditional_values_file/chart/values.schema.json"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample_conditional_values_file/chart/values.yaml"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample_conditional_values_file/cnf-testsuite.yml"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample_conditional_values_file/test-values.yaml"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""spec/workload/installability_spec.cr"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/compatibility.cr"", ""lines_added"": 49, ""lines_deleted"": 12, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,1973,[BUG] pod_memory_hog spec test failing in github actions when PR merged to `main`,"**Describe the bug** pod_memory_hog continues to fail in github actions:  ``` I, [2024-04-15 16:45:39 +00:00 #10118]  INFO -- cnf-testsuite: upsert_task: task: pod_memory_hog has status: failed and is awarded: 0 points. Runtime: 96 seconds I, [2024-04-15 16:45:39 +00:00 #10118]  INFO -- cnf-testsuite: results yaml: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""main-2024-04-15-164445-5a8da5dc"", ""status"" => nil, ""command"" => ""/home/runner/work/testsuite/testsuite/cnf-testsuite pod_memory_hog verbose"", ""points"" => nil, ""exit_code"" => 0, ""items"" => [{""name"" => ""pod_memory_hog"", ""status"" => ""failed"", ""type"" => ""normal"", ""points"" => 0}]}     'pod_memory_hog' A 'Good' CNF should not crash when pod memory hog occurs  Failures:    1) Resilience pod memory hog Chaos 'pod_memory_hog' A 'Good' CNF should not crash when pod memory hog occurs      Failure/Error: (/PASSED: pod_memory_hog chaos test passed/ =~ response_s).should_not be_nil         Expected: nil not to be nil  Error:      # spec/workload/resilience/pod_memory_hog_spec.cr:22  Finished in 2:21 minutes 1 examples, 1 failures, 0 errors, 0 pending  Failed examples:  crystal spec spec/workload/resilience/pod_memory_hog_spec.cr:15 # Resilience pod memory hog Chaos 'pod_memory_hog' A 'Good' CNF should not crash when pod memory hog occurs Error: Process completed with exit code 1. ```  **To Reproduce** Merges to `main` reproduce the errors.  **Expected behavior** Should pass and not fail. ",2024-04-15T16:51:07+00:00,2024-09-04T21:32:59+00:00,4,https://github.com/lfn-cnti/testsuite/issues/1973,1987.0,2024-04-19T21:50:41+00:00,https://github.com/lfn-cnti/testsuite/pull/1987,1,0,5,6,39,47,4,0,100.99277777777776,bug;v1.2.0,True,False,critical,configuration,"[{""filename"": "".github/workflows/actions.yml"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""spec/workload/resilience/pod_memory_hog_spec.cr"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/litmus_setup.cr"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/reliability.cr"", ""lines_added"": 20, ""lines_deleted"": 33, ""file_type"": ""other""}, {""filename"": ""src/templates/chaos_templates/disk_fill.yml.ecr"", ""lines_added"": 10, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""src/templates/chaos_templates/pod_memory_hog.yml.ecr"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,1495,[BUG] Remove Log.info wrappers when running shell commands in spec tests,"## Describe the bug Running shell commands in `Log.info { }` would result in the command being skipped when the log level is higher or not set. So when the log level is higher, then the spec test would fail.   <img width=""1305"" alt=""CleanShot 2022-06-08 at 18 38 34@2x"" src=""https://user-images.githubusercontent.com/84005/172624339-bc5b4efa-1c99-4503-be94-2d380574eb63.png""> ",2022-06-08T13:08:55+00:00,2024-09-04T20:21:45+00:00,2,https://github.com/lfn-cnti/testsuite/issues/1495,1954.0,2024-05-07T14:57:55+00:00,https://github.com/lfn-cnti/testsuite/pull/1954,0,0,45,45,1223,1868,0,0,16777.816666666666,bug,True,False,normal,functional,"[{""filename"": ""spec/5g/core_spec.cr"", ""lines_added"": 27, ""lines_deleted"": 30, ""file_type"": ""other""}, {""filename"": ""spec/5g/ran_spec.cr"", ""lines_added"": 22, ""lines_deleted"": 24, ""file_type"": ""other""}, {""filename"": ""spec/airgap_task_spec.cr"", ""lines_added"": 1, ""lines_deleted"": 103, ""file_type"": ""other""}, {""filename"": ""spec/cluster_setup_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 15, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_config_lifecycle_spec.cr"", ""lines_added"": 5, ""lines_deleted"": 27, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_container_chaos_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 22, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_microservice_spec.cr"", ""lines_added"": 5, ""lines_deleted"": 25, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_network_chaos_spec.cr"", ""lines_added"": 0, ""lines_deleted"": 24, ""file_type"": ""other""}, {""filename"": ""spec/cnf_testsuite_all/cnf_testsuite_spec.cr"", ""lines_added"": 6, ""lines_deleted"": 43, ""file_type"": ""other""}, {""filename"": ""spec/curl_install_spec.cr"", ""lines_added"": 11, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/platform/cluster_api_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 16, ""file_type"": ""other""}, {""filename"": ""spec/platform/hardware_and_scheduler_spec.cr"", ""lines_added"": 6, ""lines_deleted"": 9, ""file_type"": ""other""}, {""filename"": ""spec/platform/observability_spec.cr"", ""lines_added"": 24, ""lines_deleted"": 32, ""file_type"": ""other""}, {""filename"": ""spec/platform/platform_spec.cr"", ""lines_added"": 16, ""lines_deleted"": 31, ""file_type"": ""other""}, {""filename"": ""spec/platform/resilience_spec.cr"", ""lines_added"": 6, ""lines_deleted"": 8, ""file_type"": ""other""}, {""filename"": ""spec/platform/security_spec.cr"", ""lines_added"": 13, ""lines_deleted"": 19, ""file_type"": ""other""}, {""filename"": ""spec/prereqs_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 16, ""file_type"": ""other""}, {""filename"": ""spec/setup_spec.cr"", ""lines_added"": 47, ""lines_deleted"": 62, ""file_type"": ""other""}, {""filename"": ""spec/spec_helper.cr"", ""lines_added"": 13, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""spec/utils/cnf_manager_spec.cr"", ""lines_added"": 41, ""lines_deleted"": 91, ""file_type"": ""other""}, {""filename"": ""spec/utils/k8s_instrumentation_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""spec/utils/kubescape_spec.cr"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""spec/utils/utils_spec.cr"", ""lines_added"": 50, ""lines_deleted"": 60, ""file_type"": ""other""}, {""filename"": ""spec/workload/compatibility_spec.cr"", ""lines_added"": 17, ""lines_deleted"": 19, ""file_type"": ""other""}, {""filename"": ""spec/workload/configuration_spec.cr"", ""lines_added"": 208, ""lines_deleted"": 288, ""file_type"": ""other""}, {""filename"": ""spec/workload/cpu_hog_spec.cr"", ""lines_added"": 8, ""lines_deleted"": 26, ""file_type"": ""other""}, {""filename"": ""spec/workload/installability_spec.cr"", ""lines_added"": 38, ""lines_deleted"": 48, ""file_type"": ""other""}, {""filename"": ""spec/workload/microservice_spec.cr"", ""lines_added"": 125, ""lines_deleted"": 150, ""file_type"": ""other""}, {""filename"": ""spec/workload/observability_spec.cr"", ""lines_added"": 77, ""lines_deleted"": 117, ""file_type"": ""other""}, {""filename"": ""spec/workload/operator_spec.cr"", ""lines_added"": 13, ""lines_deleted"": 15, ""file_type"": ""other""}, {""filename"": ""spec/workload/registry_spec.cr"", ""lines_added"": 27, ""lines_deleted"": 32, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/container_chaos_spec.cr"", ""lines_added"": 8, ""lines_deleted"": 42, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/disk_fill_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/network_chaos_spec.cr"", ""lines_added"": 3, ""lines_deleted"": 33, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/node_drain_spec.cr"", ""lines_added"": 13, ""lines_deleted"": 14, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_delete_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_dns_error_spec.cr"", ""lines_added"": 13, ""lines_deleted"": 14, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_io_stress_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_memory_hog_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 14, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_network_corruption_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_network_duplication_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/resilience/pod_network_latency_spec.cr"", ""lines_added"": 12, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""spec/workload/security_spec.cr"", ""lines_added"": 195, ""lines_deleted"": 225, ""file_type"": ""other""}, {""filename"": ""spec/workload/state_spec.cr"", ""lines_added"": 48, ""lines_deleted"": 65, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/utils.cr"", ""lines_added"": 11, ""lines_deleted"": 8, ""file_type"": ""other""}]",workload;cnf_testsuite_all;platform,True
lfn-cnti/testsuite,1999,"[BUG] ""resource_policies"" (and 4 additional) test crashes","**Describe the bug** The upstream Kubescape test has been renamed causing the testsuite resource_policies test to fail  ""resource_policies"" test crashes with an error:  ``` $ ./cnf-testsuite resource_policies -l debug I, [2024-04-23 13:42:07 +00:00 #1388680]  INFO -- cnf-testsuite: kubescape_framework_download I, [2024-04-23 13:42:07 +00:00 #1388680]  INFO -- cnf-testsuite: install_kubescape I, [2024-04-23 13:42:07 +00:00 #1388680]  INFO -- cnf-testsuite: scan command: /home/ubuntu/.cnf-testsuite/tools/kubescape/kubescape scan framework nsa --use-from /home/ubuntu/.cnf-testsuite/tools/kubescape/nsa.json --exclude-namespaces kube-system,kube-public,kube-node-lease,local-path-storage,litmus,cnf-testsuite --format json --output kubescape_results.json I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: output: I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: stderr: {""level"":""info"",""ts"":""2024-04-23T13:42:07Z"",""msg"":""ARMO security scanner starting""} {""level"":""warn"",""ts"":""2024-04-23T13:42:08Z"",""msg"":""current version 'v2.0.158' is not updated to the latest release: 'v3.0.0'""} {""level"":""warn"",""ts"":""2024-04-23T13:42:08Z"",""msg"":""Kubernetes cluster nodes scanning is disabled. This is required to collect valuable data for certain controls. You can enable it using  the --enable-host-scan flag""} {""level"":""warn"",""ts"":""2024-04-23T13:42:08Z"",""msg"":""Deprecated format version"",""run"":""--format-version=v2""} {""level"":""info"",""ts"":""2024-04-23T13:42:15Z"",""msg"":""Downloading/Loading policy definitions""} {""level"":""info"",""ts"":""2024-04-23T13:42:15Z"",""msg"":""Downloaded/Loaded policy""} {""level"":""info"",""ts"":""2024-04-23T13:42:15Z"",""msg"":""Accessing Kubernetes objects""} {""level"":""info"",""ts"":""2024-04-23T13:42:15Z"",""msg"":""Accessed to Kubernetes objects""} {""level"":""info"",""ts"":""2024-04-23T13:42:15Z"",""msg"":""Scanning"",""cluster"":""cnf-setup""} {""level"":""error"",""ts"":""2024-04-23T13:42:16Z"",""msg"":""in 'runRegoOnSingleRule', failed to compile rule, name: linux-hardening, reason: 1 error occurred: linux-hardening:23: rego_parse_error: functions must use = operator (not := operator)\\n\\tis_unsafe_obj(obj) := fix_paths {\\n\\t                   ^""} {""level"":""error"",""ts"":""2024-04-23T13:42:16Z"",""msg"":""in 'runRegoOnSingleRule', failed to compile rule, name: linux-hardening, reason: 1 error occurred: linux-hardening:23: rego_parse_error: functions must use = operator (not := operator)\\n\\tis_unsafe_obj(obj) := fix_paths {\\n\\t                   ^""} {""level"":""info"",""ts"":""2024-04-23T13:42:16Z"",""msg"":""Done scanning"",""cluster"":""cnf-setup""}  Overall risk-score (0- Excellent, 100- All failed): 7  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Scan results have not been submitted: run kubescape with the '--submit' flag Sign up for free: https://portal.armo.cloud/account/sign-up?utm_source=GitHub&utm_medium=CLI&utm_campaign=no_submit ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  🕵️  Run with '--verbose'/'-v' flag for detailed resources view   I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: task_runner args: #<Sam::Args:0x7fbc5997c480 @arr=[], @named_args={}> D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: Results.file created: results/cnf-testsuite-results-20240423-134216-579.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_config_list I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: find response: [""cnfs/coredns/cnf-testsuite.yml""] I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: ensure_cnf_installed?  true I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: check_cnf_config args: #<Sam::Args:0x7fbc5997c480 @arr=[], @named_args={}> I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: check_cnf_config cnf: I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_config_list I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: find response: [""cnfs/coredns/cnf-testsuite.yml""] I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: CNF configs found: 1 D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: single_task_runner args: #<Sam::Args:0x7fbc5902ada0 @arr=[], @named_args={""cnf-config"" => ""cnfs/coredns/cnf-testsuite.yml""}> D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: parse_config_yml config_yml_path: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: airgapped: false I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: generate_tar_mode: false I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: generate_and_set_release_name I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: generate_and_set_release_name config_yml_path: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: airgapped mode: false I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: generate_tar_mode: false I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: src_helm_directory: D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: predefined_release_name: coredns I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_installation_method I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_installation_method config: #<Totem::Config:0x7fbc5c283640> I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_installation_method config: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: directory_parameter_split : I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: directory_parameter_split : I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: directory :  parameters: I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: release_name: coredns I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: helm_directory: I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: manifest_directory: I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: Building helm_directory and manifest_directory full paths I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: full_helm_directory: /home/ubuntu/cnf-testsuite/cnfs/coredns/ exists? true I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: full_manifest_directory: /home/ubuntu/cnf-testsuite/cnfs/coredns/ exists? true D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: install type count install_type: helm_chart D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: install_type: helm_directory not found in cnfs/coredns/cnf-testsuite.yml D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: install_type: manifest_directory not found in cnfs/coredns/cnf-testsuite.yml D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: installation_type_count: 1 I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf_destination_dir config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite: cnf_destination_dir parsed_config_file config: #<Totem::Config:0x7fbc5c283460> I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: release_name: coredns I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: cnf destination dir: /home/ubuntu/cnf-testsuite/cnfs/coredns I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: USING EXPORTED CHART PATH I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite-resource_policies: Starting test D, [2024-04-23 13:42:16 +00:00 #1388680] DEBUG -- cnf-testsuite-resource_policies: cnf_config: #<CNFManager::Config:0x7fbc5c2898c0> I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: kubescape parse I, [2024-04-23 13:42:16 +00:00 #1388680]  INFO -- cnf-testsuite: kubescape test_by_test_name E, [2024-04-23 13:42:16 +00:00 #1388680] ERROR -- cnf-testsuite: Cast from Array(JSON::Any) to Hash(K, V) failed, at /usr/share/crystal/src/json/any.cr:274:5:274 E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /usr/share/crystal/src/json/any.cr:273:3 in 'as_h' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/tasks/utils/kubescape.cr:69:7 in 'parse' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/tasks/utils/kubescape.cr:51:5 in 'parse_test_report' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/tasks/workload/security.cr:314:19 in '->' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /usr/share/crystal/src/log/log.cr:36:3 in 'all_cnfs_task_runner' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/tasks/utils/task.cr:38:9 in 'task_runner:task' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/tasks/workload/security.cr:311:3 in '->' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: lib/sam/src/sam/task.cr:54:39 in 'call' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: lib/sam/src/sam/execution.cr:20:7 in 'invoke' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: lib/sam/src/sam.cr:35:5 in 'invoke' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: lib/sam/src/sam.cr:53:7 in 'process_tasks' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: src/cnf-testsuite.cr:132:3 in '__crystal_main' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /usr/share/crystal/src/crystal/main.cr:129:5 in 'main_user_code' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /usr/share/crystal/src/crystal/main.cr:115:7 in 'main' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /usr/share/crystal/src/crystal/main.cr:141:3 in 'main' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /lib/x86_64-linux-gnu/libc.so.6 in '??' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: /lib/x86_64-linux-gnu/libc.so.6 in '__libc_start_main' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: ./cnf-testsuite in '_start' E, [2024-04-23 13:42:17 +00:00 #1388680] ERROR -- cnf-testsuite: ??? D, [2024-04-23 13:42:17 +00:00 #1388680] DEBUG -- cnf-testsuite: update_yml results: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""<%= CnfTestSuite::VERSION %>"", ""status"" => nil, ""points"" => nil, ""exit_code"" => 0, ""items"" => []} D, [2024-04-23 13:42:17 +00:00 #1388680] DEBUG -- cnf-testsuite: update_yml parsed_new_yml: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""<%= CnfTestSuite::VERSION %>"", ""status"" => nil, ""points"" => nil, ""exit_code"" => 2, ""items"" => []} I, [2024-04-23 13:42:17 +00:00 #1388680]  INFO -- cnf-testsuite: exception with skipped exit code I, [2024-04-23 13:42:17 +00:00 #1388680]  INFO -- cnf-testsuite: results yaml: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""<%= CnfTestSuite::VERSION %>"", ""status"" => nil, ""points"" => nil, ""exit_code"" => 2, ""items"" => []} $ ```  **To Reproduce** - get latest main of cnf-testsuite, compile - remove any old downloaded kubescape rm -rf ~/.cnf-testsuite/tools/kubescape use coredns sample ./cnf-testsuite cnf_setup cnf-config=./sample-cnfs/sample-coredns-cnf - run the test ./cnf-testsuite resource_policies  **Expected behavior** the tests passes   Note1: The test may pass in case that old/workinging kubescape is not wiped. This may be also a reason why github actions did not detect this.  Note2:  the issue was most probably introduced by this change. #1992  Note3: ""resource_policies"" is an essential certification test.",2024-04-23T13:49:59+00:00,2024-05-21T19:35:30+00:00,12,https://github.com/lfn-cnti/testsuite/issues/1999,2004.0,2024-04-25T16:32:46+00:00,https://github.com/lfn-cnti/testsuite/pull/2004,4,0,17,21,151,135,34,0,50.71305555555556,bug;kubescape;high;v1.2.0,True,False,major,configuration,"[{""filename"": ""RATIONALE.md"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""TEST-CATEGORIES.md"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""USAGE.md"", ""lines_added"": 17, ""lines_deleted"": 17, ""file_type"": ""other""}, {""filename"": ""docs/LIST_OF_TESTS.md"", ""lines_added"": 0, ""lines_deleted"": 11, ""file_type"": ""other""}, {""filename"": ""embedded_files/points.yml"", ""lines_added"": 6, ""lines_deleted"": 4, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample-nonroot/cnf-testsuite.yml"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""sample-cnfs/sample-nonroot/manifests/pod.yml"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""shard.lock"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""shard.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""spec/platform/platform_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""spec/platform/security_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 41, ""file_type"": ""other""}, {""filename"": ""spec/utils/cnf_manager_spec.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""spec/workload/security_spec.cr"", ""lines_added"": 18, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""src/tasks/cert/cert.cr"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/cert/cert_security.cr"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/constants.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/kubescape_setup.cr"", ""lines_added"": 13, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/security.cr"", ""lines_added"": 7, ""lines_deleted"": 26, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/kubescape.cr"", ""lines_added"": 12, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/utils.cr"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/security.cr"", ""lines_added"": 34, ""lines_deleted"": 9, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,2020,Fix bug with Litmus Pod Mem Test cnti-testcatalog/testsuite#2019,"## Description Fix Litmus Pod Memory Hog Test  ## Issues: Refs: cnti-testcatalog/testsuite#2019  ## How has this been tested:  - [ ] Covered by existing integration testing  - [ ] Added integration testing to cover  - [ ] Verified all A/C passes      * [ ] develop      * [ ] master      * [ ] tag/other branch  - [ ] Test environment     * [ ] Shared Packet K8s cluster     * [ ] New Packet K8s cluster     * [ ] Kind cluster  - [ ] Have not tested  ## Types of changes:  - [ ] Bug fix (non-breaking change which fixes an issue)  - [ ] New feature (non-breaking change which adds functionality)  - [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)  - [ ] Documentation update  ## Checklist: **Documentation** - [ ] My change requires a change to the documentation. - [ ] I have updated the documentation accordingly. - [ ] No updates required.  **Code Review** - [ ] Does the test handle fatal exceptions, ie. rescue block  **Issue** - [ ] Tasks in issue are checked off ",2024-05-10T00:59:45+00:00,2024-05-10T15:44:22+00:00,0,https://github.com/lfn-cnti/testsuite/pull/2020,2020.0,2024-05-10T15:44:21+00:00,https://github.com/lfn-cnti/testsuite/pull/2020,0,0,1,1,1,1,0,0,14.743333333333334,,True,False,normal,ui,"[{""filename"": ""src/templates/chaos_templates/pod_memory_hog.yml.ecr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,94,[BUG] Postgres driver password authentication failed,,2022-10-14T11:14:58+00:00,2022-10-14T11:17:34+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/94,480.0,2024-05-09T03:49:20+00:00,https://github.com/lakesoul-io/LakeSoul/pull/480,0,0,2,2,11,4,0,0,13744.572777777776,bug,True,False,normal,security,"[{""filename"": ""rust/Cargo.lock"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,80,[Bug] CDC updates should be sorted by time and operation,,2022-09-04T05:45:21+00:00,2022-09-06T13:52:11+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/80,480.0,2024-05-09T03:49:20+00:00,https://github.com/lakesoul-io/LakeSoul/pull/480,0,0,2,2,11,4,0,0,14710.066388888888,bug;flink,True,False,normal,functional,"[{""filename"": ""rust/Cargo.lock"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,75,Maven install should skip gpg signing,We should use a maven profile for gpg signing to avoid entering passphrase when maven install,2022-08-16T09:44:17+00:00,2022-08-16T09:58:35+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/75,76.0,2022-08-16T09:58:34+00:00,https://github.com/lakesoul-io/LakeSoul/pull/76,3,0,5,8,70,18,83,0,0.2380555555555555,bug,True,False,normal,functional,"[{""filename"": ""docker/lakesoul-docker-compose-env/docker-compose.yml"", ""lines_added"": 29, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""docker/lakesoul-docker-compose-env/meta_cleanup.sql"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""docker/lakesoul-docker-compose-env/meta_init.sql"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lakesoul-common/.gitignore"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lakesoul-flink/.gitignore"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lakesoul-spark/.gitignore"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lakesoul-spark/pom.xml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""pom.xml"", ""lines_added"": 35, ""lines_deleted"": 17, ""file_type"": ""config""}]",,False
apache/incubator-kie-kogito-apps,1993,Log level cannot be set to DEBUG or TRACE for data index,"### Describe the bug  These two properties https://github.com/apache/incubator-kie-kogito-apps/blob/main/data-index/data-index-service/data-index-service-postgresql/src/main/resources/application.properties#L46-47 prevents setting the level on runtime to a more detailed level, preventing the system to be debug. They should be removed  ### Expected behavior  _No response_  ### Actual behavior  _No response_  ### How to Reproduce?  _No response_  ### Output of `uname -a` or `ver`  _No response_  ### Output of `java -version`  _No response_  ### GraalVM version (if different from Java)  _No response_  ### Kogito version or git rev (or at least Quarkus version if you are using Kogito via Quarkus platform BOM)  _No response_  ### Build tool (ie. output of `mvnw --version` or `gradlew --version`)  _No response_  ### Additional information  _No response_",2024-02-20T11:18:00+00:00,2024-02-20T15:28:34+00:00,0,https://github.com/apache/incubator-kie-kogito-apps/issues/1993,1994.0,2024-02-20T15:28:33+00:00,https://github.com/apache/incubator-kie-kogito-apps/pull/1994,1,0,0,1,0,6,6,0,4.175833333333333,,True,False,normal,database,"[{""filename"": ""data-index/data-index-service/data-index-service-postgresql/src/main/resources/application.properties"", ""lines_added"": 0, ""lines_deleted"": 6, ""file_type"": ""config""}]",data-index,False
prometheus-operator/kube-prometheus,2592,GrafanaRequestsFailing: fix many-to-one matching must be explicit error,"Hey folks 👋🏻   I got paged because this alert started failing to evaluate with the `many-to-one matching must be explicit (group_left/group_right)` error.  The left-hand side keeps the ""status_code"" label in the time series, which can lead to one single handler having multiple time series. When divided by the right-hand expression, it won't contain the status_code, leading to a many-to-one error.  I solved this by removing the `status_code` from the left-hand side. If I'm not mistaken, that also means we can get rid of `ignoring`. It could have been addressed with `group_left`, but the proposed expression seemed easier to follow. The expression should retain the ""trigger on a per-route basis"" intent.  I think this has always been incorrect, but I'm I'm surprised no one has experienced it.  I also noticed the runbook does not exist, so I removed it to avoid confusion ",2025-01-24T22:12:28+00:00,2025-01-29T15:07:04+00:00,2,https://github.com/prometheus-operator/kube-prometheus/pull/2592,2592.0,,https://github.com/prometheus-operator/kube-prometheus/pull/2592,1,0,0,1,2,4,6,0,112.91,,True,False,normal,functional,"[{""filename"": ""manifests/grafana-prometheusRule.yaml"", ""lines_added"": 2, ""lines_deleted"": 4, ""file_type"": ""config""}]",,False
vmware-tanzu/kubeapps,8200,Bump thiserror from 1.0.67 to 2.0.11 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.11. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.11</h2> <ul> <li>Add feature gate to tests that use std (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/409"">#409</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/410"">#410</a>, thanks <a href=""https://github.com/Maytha8""><code>@​Maytha8</code></a>)</li> </ul> <h2>2.0.10</h2> <ul> <li>Support errors containing a generic type parameter's associated type in a field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/408"">#408</a>)</li> </ul> <h2>2.0.9</h2> <ul> <li>Work around <code>missing_inline_in_public_items</code> clippy restriction being triggered in macro-generated code (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/404"">#404</a>)</li> </ul> <h2>2.0.8</h2> <ul> <li>Improve support for macro-generated <code>derive(Error)</code> call sites (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a>)</li> </ul> <h2>2.0.7</h2> <ul> <li>Work around conflict with #[deny(clippy::allow_attributes)] (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/397"">#397</a>, thanks <a href=""https://github.com/zertosh""><code>@​zertosh</code></a>)</li> </ul> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/0f532e326e9a4cc6c6e30ee19ab00cb9eeb44362""><code>0f532e3</code></a> Release 2.0.11</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/3d15543a9117e32050caa6a219da6299b6271576""><code>3d15543</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/410"">#410</a> from dtolnay/testnostd</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/1a226ae42c20114f71bd3ed339f9e0351351abce""><code>1a226ae</code></a> Disable two more integration tests in no-std mode</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/8b5f2d78f0576d8a64a96bd0b73c2b4eef45e6c9""><code>8b5f2d7</code></a> Fix unused import in test when built without std</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/eecd247cdf7dfa1cee9898dd29d56b0021b5f4d0""><code>eecd247</code></a> Add CI step to test with &quot;std&quot; disabled</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/8f2a76b4ba520e5615147977531bff394bed6894""><code>8f2a76b</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/409"">#409</a> from Maytha8/std-tests</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/693a6cddad750d0401942d553969310193ec2614""><code>693a6cd</code></a> Add feature gate to tests that use std</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/349f6960ff02d64bec38de392850ea9aa07bb766""><code>349f696</code></a> Release 2.0.10</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6cd87bc228612a1b8634ddb613059cc11b47f7ae""><code>6cd87bc</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/408"">#408</a> from dtolnay/assoctype</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6b3e1e50b27d9f90fd4a4be098d4693e50609784""><code>6b3e1e5</code></a> Generate trait bounds on associated types</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.11"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.11)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2025-01-13T02:41:27+00:00,2025-02-25T01:43:06+00:00,2,https://github.com/vmware-tanzu/kubeapps/pull/8200,8200.0,,https://github.com/vmware-tanzu/kubeapps/pull/8200,0,0,2,2,34,14,0,0,1031.0275,dependencies;rust;stale;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
prometheus-operator/kube-prometheus,2575,Fix broken rules with Prometheus v3,"With Prometheus v3 [histograms are normalized](https://prometheus.io/docs/prometheus/latest/migration/#le-and-quantile-label-values) and this breaks rules that select on the `le` label. This PR overwrites the label selectors that [mixin](https://github.com/kubernetes-monitoring) usese.  Before: ``` {le=""1""} ``` Now with Prometheus v3: ``` {le=""1.0""} ```  This will break set ups with prometheus v2. However, I think it is still an improvement by default Prometheus v3 is used and I would expect that the generated rules are workind with the default Prometheus version as well.  Fixes https://github.com/prometheus-operator/kube-prometheus/issues/2573  Signed-off-by: leonnicolas <leonloechner@gmx.de> ",2024-12-23T17:33:09+00:00,2025-01-07T10:12:02+00:00,2,https://github.com/prometheus-operator/kube-prometheus/pull/2575,2575.0,,https://github.com/prometheus-operator/kube-prometheus/pull/2575,1,0,1,2,40,36,72,0,352.6480555555556,,True,False,critical,functional,"[{""filename"": ""jsonnet/kube-prometheus/components/k8s-control-plane.libsonnet"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""manifests/kubernetesControlPlane-prometheusRule.yaml"", ""lines_added"": 36, ""lines_deleted"": 36, ""file_type"": ""config""}]",,False
radius-project/radius,8449,Investigate 409 error in Long Running tests,"### Steps to reproduce  Long Running Tests broke on initial install (new build) and then the subsequent runs (every 2 hours) will break till the next time for build to be run. Investigate issue , check logs on cluster, potentially increase time needed to register manifest.  Introduce ability to run LRT with new build on dispatch so we do not have known subsequent 12 failures after the initial run with new build.  ### Observed behavior  <img width=""1526"" alt=""Image"" src=""https://github.com/user-attachments/assets/bfcfd0f9-0f40-42af-943a-a7cd0ba55d9d"" />  <img width=""1497"" alt=""Image"" src=""https://github.com/user-attachments/assets/cfd7176a-b385-42e5-8bb3-826bd89bc9b0"" />  [logs_34173322299.zip](https://github.com/user-attachments/files/18739886/logs_34173322299.zip)  [all_container_logs (8).zip](https://github.com/user-attachments/files/18739903/all_container_logs.8.zip) [all_container_logs (9).zip](https://github.com/user-attachments/files/18739902/all_container_logs.9.zip)  [all-tests-pod-states.log](https://github.com/user-attachments/files/18739885/all-tests-pod-states.log)  <img width=""1471"" alt=""Image"" src=""https://github.com/user-attachments/assets/78418c5d-74dd-401d-aafb-0debd2810799"" />  <img width=""1497"" alt=""Image"" src=""https://github.com/user-attachments/assets/ad671be9-07a3-44c3-95d9-33f293663c07"" />  <img width=""1497"" alt=""Image"" src=""https://github.com/user-attachments/assets/8d84312e-b854-4760-bbff-0773eac380b6"" />  ### Desired behavior  Long running tests run without error  ### Workaround  _No response_  ### rad Version  v0.43.0  ### Operating system  _No response_  ### Additional context  _No response_  ### Would you like to support us?  - [ ] Yes, I would like to support you  [AB#14420](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/14420)",2025-02-10T19:41:57+00:00,2025-02-20T00:22:45+00:00,1,https://github.com/radius-project/radius/issues/8449,8580.0,2025-02-20T00:22:44+00:00,https://github.com/radius-project/radius/pull/8580,1,0,0,1,1,1,2,0,220.6797222222222,bug;on-call,True,False,normal,ui,"[{""filename"": ""deploy/Chart/templates/ucp/configmaps.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
prometheus-operator/kube-prometheus,2573,Broken recording rules with Prometheus v3,"With Prometheus v3, some recording rules are ""broken"". All rules that select with `le=""<some integer>""` are not working anymore and should be changed to `le=""<some float>""`. Alerting rules like `KubeAPIErrorBudgetBurn` are affected.  Eg:  ``` cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=""1""} ``` should be changed to  ``` cluster_verb_scope_le:apiserver_request_sli_duration_seconds_bucket:increase30d{le=""1.0""} ``` See: [prometheus migration guide](https://prometheus.io/docs/prometheus/latest/migration/#le-and-quantile-label-values)   *EDIT*: apparently this repo is not the best place to place this issue as the rules are defined [here](https://github.com/kubernetes-monitoring/kubernetes-mixin/blob/master/rules/kube_apiserver-availability.libsonnet)",2024-12-23T11:32:52+00:00,2025-01-07T10:11:08+00:00,1,https://github.com/prometheus-operator/kube-prometheus/issues/2573,2580.0,,https://github.com/prometheus-operator/kube-prometheus/pull/2580,3,0,0,3,60,60,120,0,358.6377777777778,kind/bug,True,False,critical,ui,"[{""filename"": ""jsonnetfile.lock.json"", ""lines_added"": 10, ""lines_deleted"": 10, ""file_type"": ""config""}, {""filename"": ""manifests/grafana-dashboardDefinitions.yaml"", ""lines_added"": 14, ""lines_deleted"": 14, ""file_type"": ""config""}, {""filename"": ""manifests/kubernetesControlPlane-prometheusRule.yaml"", ""lines_added"": 36, ""lines_deleted"": 36, ""file_type"": ""config""}]",,False
lfn-cnti/testsuite,1275,[BUG] Fix or skip tests that fail when running against a Kubespray cluster.,"**Describe the bug** The cnf-testsuite was developed and CI tested only against a Kind Cluster. Users will likely be using other K8s Platforms. We should ensure incompatible tests are either skipped or updated so they do not fail when running against a cluster that was no created using KIND. **To Reproduce** Steps to reproduce the behavior:  1. Go to '...' 2. Click on '....' 3. Scroll down to '....' 4. See error  **Expected behavior** A clear and concise description of what you expected to happen.  **Screenshots** If applicable, add screenshots to help explain your problem.  **Device (please complete the following information):**  - OS [e.g. Linux, iOS, Windows, Android] - Distro [e.g. Ubuntu] - Version [e.g. 18.04] - Architecture [e.g. x86, arm] - Browser [e.g. chrome, safari]  **How will this be tested? aka Acceptance Criteria (optional)**  (optional: unnecessary for things like spelling errors and such)  Once this issue is address how will the fix be verified?  **Additional context** Add any other context about the problem here.  ---  NOTE: you can enable higher logging level output via the command line or env var. to help with debugging  ``` # cmd line ./cnf-testsuite -l debug test  # make sure to use -- if running from source crystal src/cnf-testsuite.cr -- -l debug test  # env var LOGLEVEL=DEBUG ./cnf-testsuite test ```  Also setting the verbose option for many tasks will add extra output to help with debugging  ``` crystal src/cnf-testsuite.cr test_name verbose ```  Check [usage documentation](https://github.com/cncf/cnf-testsuite/blob/main/USAGE.md) for more info about invoking commands and logging ",2022-03-17T19:41:15+00:00,2024-05-09T21:12:48+00:00,5,https://github.com/lfn-cnti/testsuite/issues/1275,1351.0,2022-04-12T15:35:15+00:00,https://github.com/lfn-cnti/testsuite/pull/1351,1,0,2,3,22,38,2,0,619.9,bug,True,False,normal,ui,"[{""filename"": ""sample-cnfs/sample_privileged_cnf/chart/templates/deployment.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""spec/utils/utils_spec.cr"", ""lines_added"": 15, ""lines_deleted"": 35, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/cnf_manager.cr"", ""lines_added"": 6, ""lines_deleted"": 2, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8197,Bump thiserror from 1.0.67 to 2.0.10 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.10. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.10</h2> <ul> <li>Support errors containing a generic type parameter's associated type in a field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/408"">#408</a>)</li> </ul> <h2>2.0.9</h2> <ul> <li>Work around <code>missing_inline_in_public_items</code> clippy restriction being triggered in macro-generated code (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/404"">#404</a>)</li> </ul> <h2>2.0.8</h2> <ul> <li>Improve support for macro-generated <code>derive(Error)</code> call sites (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a>)</li> </ul> <h2>2.0.7</h2> <ul> <li>Work around conflict with #[deny(clippy::allow_attributes)] (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/397"">#397</a>, thanks <a href=""https://github.com/zertosh""><code>@​zertosh</code></a>)</li> </ul> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code></code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/349f6960ff02d64bec38de392850ea9aa07bb766""><code>349f696</code></a> Release 2.0.10</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6cd87bc228612a1b8634ddb613059cc11b47f7ae""><code>6cd87bc</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/408"">#408</a> from dtolnay/assoctype</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6b3e1e50b27d9f90fd4a4be098d4693e50609784""><code>6b3e1e5</code></a> Generate trait bounds on associated types</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/136859154b88758e33a5cbe57d7bd70f1d31615f""><code>1368591</code></a> Add regression test for issue 405</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/c535cecb6f8d98cbdc72f526fc4c8a8ae826e2a3""><code>c535cec</code></a> Release 2.0.9</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/0a0516db7382a18212574dd0d04dceabe7d77b2d""><code>0a0516d</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/404"">#404</a> from dtolnay/fromfn</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e5169bb127f835d5fc390a5ca9acd673d075e21e""><code>e5169bb</code></a> Unspan From impl contents</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/c0083752681756b7ad1aae2e6a15717d3d27118d""><code>c008375</code></a> FIx typo in ui test</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2bd29821f4ea339c60edfcf4734499d68128eb2e""><code>2bd2982</code></a> Release 2.0.8</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/a7de3ab22d01922e050aad4202d71a4bfb577598""><code>a7de3ab</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a> from dtolnay/respan</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.10"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.10)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2025-01-10T02:17:31+00:00,2025-01-13T02:41:29+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8197,8197.0,,https://github.com/vmware-tanzu/kubeapps/pull/8197,0,0,2,2,34,14,0,0,72.39944444444444,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8187,Bump thiserror from 1.0.67 to 2.0.9 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.9. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.9</h2> <ul> <li>Work around <code>missing_inline_in_public_items</code> clippy restriction being triggered in macro-generated code (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/404"">#404</a>)</li> </ul> <h2>2.0.8</h2> <ul> <li>Improve support for macro-generated <code>derive(Error)</code> call sites (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a>)</li> </ul> <h2>2.0.7</h2> <ul> <li>Work around conflict with #[deny(clippy::allow_attributes)] (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/397"">#397</a>, thanks <a href=""https://github.com/zertosh""><code>@​zertosh</code></a>)</li> </ul> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/c535cecb6f8d98cbdc72f526fc4c8a8ae826e2a3""><code>c535cec</code></a> Release 2.0.9</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/0a0516db7382a18212574dd0d04dceabe7d77b2d""><code>0a0516d</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/404"">#404</a> from dtolnay/fromfn</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e5169bb127f835d5fc390a5ca9acd673d075e21e""><code>e5169bb</code></a> Unspan From impl contents</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/c0083752681756b7ad1aae2e6a15717d3d27118d""><code>c008375</code></a> FIx typo in ui test</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2bd29821f4ea339c60edfcf4734499d68128eb2e""><code>2bd2982</code></a> Release 2.0.8</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/a7de3ab22d01922e050aad4202d71a4bfb577598""><code>a7de3ab</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a> from dtolnay/respan</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/f1243a0ceb1c596f808c8df8d1240ed301135a1b""><code>f1243a0</code></a> Fix spans on macro-generated bindings and format variables</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6a07345135802344616a09584c94e2f4bbceb466""><code>6a07345</code></a> Add regression test for issue 398</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/9c0f2d230da33dfec248d48d82c25a2ad19e6129""><code>9c0f2d2</code></a> Release 2.0.7</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2deec96fc0de605d114d3860f29d1d066ad4151e""><code>2deec96</code></a> Merge pull request 397 from zertosh/from_allow_expect</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.9"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-12-23T02:24:54+00:00,2025-01-10T02:17:34+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8187,8187.0,,https://github.com/vmware-tanzu/kubeapps/pull/8187,0,0,2,2,34,14,0,0,431.8777777777778,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8183,Bump thiserror from 1.0.67 to 2.0.8 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.8. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.8</h2> <ul> <li>Improve support for macro-generated <code>derive(Error)</code> call sites (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a>)</li> </ul> <h2>2.0.7</h2> <ul> <li>Work around conflict with #[deny(clippy::allow_attributes)] (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/397"">#397</a>, thanks <a href=""https://github.com/zertosh""><code>@​zertosh</code></a>)</li> </ul> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/2bd29821f4ea339c60edfcf4734499d68128eb2e""><code>2bd2982</code></a> Release 2.0.8</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/a7de3ab22d01922e050aad4202d71a4bfb577598""><code>a7de3ab</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/399"">#399</a> from dtolnay/respan</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/f1243a0ceb1c596f808c8df8d1240ed301135a1b""><code>f1243a0</code></a> Fix spans on macro-generated bindings and format variables</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6a07345135802344616a09584c94e2f4bbceb466""><code>6a07345</code></a> Add regression test for issue 398</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/9c0f2d230da33dfec248d48d82c25a2ad19e6129""><code>9c0f2d2</code></a> Release 2.0.7</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2deec96fc0de605d114d3860f29d1d066ad4151e""><code>2deec96</code></a> Merge pull request 397 from zertosh/from_allow_expect</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/100d9164f2b2dbfb5ffcafc19fd925f55626acc0""><code>100d916</code></a> Avoid associating #[from] with lint allow</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/485c2b7eed2702c4a109be3cf42bb4be27fe16bd""><code>485c2b7</code></a> Reword spurious errors comment</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2075e87257e5bfe458cc01bc3764dcd499db254d""><code>2075e87</code></a> Release 2.0.6</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e9a90851502686c39a7164f67b296b579445f9ff""><code>e9a9085</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a> from dtolnay/deprecatedfrom</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.8"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-12-19T02:15:44+00:00,2024-12-23T02:24:56+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8183,8183.0,,https://github.com/vmware-tanzu/kubeapps/pull/8183,0,0,2,2,34,14,0,0,96.15333333333334,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8181,Bump thiserror from 1.0.67 to 2.0.7 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.7. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.7</h2> <ul> <li>Work around conflict with #[deny(clippy::allow_attributes)] (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/397"">#397</a>, thanks <a href=""https://github.com/zertosh""><code>@​zertosh</code></a>)</li> </ul> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, } </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/9c0f2d230da33dfec248d48d82c25a2ad19e6129""><code>9c0f2d2</code></a> Release 2.0.7</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2deec96fc0de605d114d3860f29d1d066ad4151e""><code>2deec96</code></a> Merge pull request 397 from zertosh/from_allow_expect</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/100d9164f2b2dbfb5ffcafc19fd925f55626acc0""><code>100d916</code></a> Avoid associating #[from] with lint allow</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/485c2b7eed2702c4a109be3cf42bb4be27fe16bd""><code>485c2b7</code></a> Reword spurious errors comment</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2075e87257e5bfe458cc01bc3764dcd499db254d""><code>2075e87</code></a> Release 2.0.6</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e9a90851502686c39a7164f67b296b579445f9ff""><code>e9a9085</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a> from dtolnay/deprecatedfrom</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6e8c7244c9b396f0c194a660fcb4bcff75e50b08""><code>6e8c724</code></a> Suppress deprecation warning on generated From impls</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/caf585c978f43ef0348dcd669af301e7a7e8578b""><code>caf585c</code></a> Add test of deprecated type in From impl</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/f1f159d7e759e986335ed14dd362eb7d6c9815d4""><code>f1f159d</code></a> Release 2.0.5</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/366a7b253e4b363dfe7204fea7b2e088a81bf8ee""><code>366a7b2</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/395"">#395</a> from dtolnay/fallback</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.7"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-12-16T03:00:25+00:00,2024-12-19T02:15:47+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8181,8181.0,,https://github.com/vmware-tanzu/kubeapps/pull/8181,0,0,2,2,34,14,0,0,71.25611111111111,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8163,Bump thiserror from 1.0.67 to 2.0.6 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.6. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.6</h2> <ul> <li>Suppress deprecation warning on generated From impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a>)</li> </ul> <h2>2.0.5</h2> <ul> <li>Prevent deprecation warning on generated impl for deprecated type (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a>)</li> </ul> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, } </code></pre> </li> <li> <p>Tuple structs and tuple variants can no longer use numerical <code>{0}</code> <code>{1}</code> access at the same time as supplying extra positional arguments for a format message, as this makes it ambiguous whether the number refers to a tuple field vs a different positional arg (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/354"">#354</a>)</p> <pre lang=""rust""><code></code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/2075e87257e5bfe458cc01bc3764dcd499db254d""><code>2075e87</code></a> Release 2.0.6</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e9a90851502686c39a7164f67b296b579445f9ff""><code>e9a9085</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/396"">#396</a> from dtolnay/deprecatedfrom</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6e8c7244c9b396f0c194a660fcb4bcff75e50b08""><code>6e8c724</code></a> Suppress deprecation warning on generated From impls</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/caf585c978f43ef0348dcd669af301e7a7e8578b""><code>caf585c</code></a> Add test of deprecated type in From impl</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/f1f159d7e759e986335ed14dd362eb7d6c9815d4""><code>f1f159d</code></a> Release 2.0.5</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/366a7b253e4b363dfe7204fea7b2e088a81bf8ee""><code>366a7b2</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/395"">#395</a> from dtolnay/fallback</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/88a46035e1bbb17ada53acf28ed7b7b00a0b049a""><code>88a4603</code></a> Move fallback expansion to separate module</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6712f8cca6cd37c165da4a45ad8820fc97710116""><code>6712f8c</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/394"">#394</a> from dtolnay/deprecated</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/07e7d990facdfdbd559df323dae1f5deb1ba9ab6""><code>07e7d99</code></a> Add &quot;in this derive macro expansion&quot; to missing Display errors</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/714229d8214e77df019254afbbca18f1c921737e""><code>714229d</code></a> Work around deprecation warning on generated impl for deprecated type</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.6"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-12-09T02:03:41+00:00,2024-12-16T03:00:28+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8163,8163.0,,https://github.com/vmware-tanzu/kubeapps/pull/8163,0,0,2,2,34,14,0,0,168.9463888888889,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8157,Bump thiserror from 1.0.67 to 2.0.4 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.4. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.4</h2> <ul> <li>Eliminate needless_lifetimes clippy lint in generated <code>From</code> impls (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a>, thanks <a href=""https://github.com/matt-phylum""><code>@​matt-phylum</code></a>)</li> </ul> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, } </code></pre> </li> <li> <p>Tuple structs and tuple variants can no longer use numerical <code>{0}</code> <code>{1}</code> access at the same time as supplying extra positional arguments for a format message, as this makes it ambiguous whether the number refers to a tuple field vs a different positional arg (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/354"">#354</a>)</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;ambiguous: {0} {}&quot;, $N)] //                  ^^^ Not allowed, use #[error(&quot;... {0} {n}&quot;, n = $N)] pub struct TupleError(i32); </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/70a12613ef6459c90ea69e22cdb41ec20c98e038""><code>70a1261</code></a> Release 2.0.4</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/4fde2846c888d4954ab1a17cd2efba9cf24d30e7""><code>4fde284</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/391"">#391</a> from matt-phylum/needless-lifetimes</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/aa19b7cfce13336cb171ad6e8cdb9b50cc1c9dd5""><code>aa19b7c</code></a> suppress needless_lifetimes lints from clippy 0.1.83</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/ad2f20b9f72d2ad998727c7ed1c1d4fc8b7c805a""><code>ad2f20b</code></a> Use ui test syntax that does not interfere with rustfmt</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/15fd26e476c5c7a2e7dc13209689c747b1db82a5""><code>15fd26e</code></a> Release 2.0.3</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/70460231305d82ae9a7a60424cc4d0d22d0b6e77""><code>7046023</code></a> Simplify how has_bonus_display is accumulated</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/9cc1d0b2514105759995dfd3c7bc4de1f0f9195b""><code>9cc1d0b</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a> from dtolnay/nowrap</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/1d040f358a34d58139f1e1c12cec575319f16edf""><code>1d040f3</code></a> Use Var wrapper only for Pointer formatting</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6a6132d79bee8baf89ea0896ec6dadc3ad6b388b""><code>6a6132d</code></a> Extend no-display ui test to cover another fmt trait</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/a061beb9dc871144239dc3489dc012f39e13847c""><code>a061beb</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a> from dtolnay/both</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.4"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-12-04T02:13:55+00:00,2024-12-09T02:03:44+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8157,8157.0,,https://github.com/vmware-tanzu/kubeapps/pull/8157,0,0,2,2,34,14,0,0,119.83027777777778,dependencies;rust;cla-not-required,True,False,major,security,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8131,Bump thiserror from 1.0.67 to 2.0.3 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.3. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.3</h2> <ul> <li>Support the same Path field being repeated in both Debug and Display representation in error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a>)</li> <li>Improve error message when a format trait used in error message is not implemented by some field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a>)</li> </ul> <h2>2.0.2</h2> <ul> <li>Fix hang on invalid input inside #[error(...)] attribute (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/382"">#382</a>)</li> </ul> <h2>2.0.1</h2> <ul> <li>Support errors that contain a dynamically sized final field (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/375"">#375</a>)</li> <li>Improve inference of trait bounds for fields that are interpolated multiple times in an error message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/377"">#377</a>)</li> </ul> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, } </code></pre> </li> <li> <p>Tuple structs and tuple variants can no longer use numerical <code>{0}</code> <code>{1}</code> access at the same time as supplying extra positional arguments for a format message, as this makes it ambiguous whether the number refers to a tuple field vs a different positional arg (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/354"">#354</a>)</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;ambiguous: {0} {}&quot;, $N)] //                  ^^^ Not allowed, use #[error(&quot;... {0} {n}&quot;, n = $N)] pub struct TupleError(i32); </code></pre> </li> <li> <p>Code containing invocations of thiserror's <code>derive(Error)</code> must now have a direct dependency on the <code>thiserror</code> crate regardless of the error data structure's contents (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/368"">#368</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/369"">#369</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/370"">#370</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/372"">#372</a>)</p> </li> </ul> <h2>Features</h2> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/15fd26e476c5c7a2e7dc13209689c747b1db82a5""><code>15fd26e</code></a> Release 2.0.3</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/70460231305d82ae9a7a60424cc4d0d22d0b6e77""><code>7046023</code></a> Simplify how has_bonus_display is accumulated</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/9cc1d0b2514105759995dfd3c7bc4de1f0f9195b""><code>9cc1d0b</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/384"">#384</a> from dtolnay/nowrap</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/1d040f358a34d58139f1e1c12cec575319f16edf""><code>1d040f3</code></a> Use Var wrapper only for Pointer formatting</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/6a6132d79bee8baf89ea0896ec6dadc3ad6b388b""><code>6a6132d</code></a> Extend no-display ui test to cover another fmt trait</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/a061beb9dc871144239dc3489dc012f39e13847c""><code>a061beb</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/383"">#383</a> from dtolnay/both</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/63882935be42fbd89e7076392a4d5330e2120332""><code>6388293</code></a> Support Display and Debug of same path in error message</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/dc0359eeecf778da2038805431c61010e7aa957e""><code>dc0359e</code></a> Defer binding_value construction</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/520343e37d890e0a4b0c6e1427e8164c43ce1c7d""><code>520343e</code></a> Add test of Debug and Display of paths</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/49be39dee10d7fce1d4b2f7f6b6010f2b309794e""><code>49be39d</code></a> Release 2.0.2</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.3"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-11-11T02:04:33+00:00,2024-12-04T02:13:57+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8131,8131.0,,https://github.com/vmware-tanzu/kubeapps/pull/8131,0,0,2,2,34,14,0,0,552.1566666666666,dependencies;rust;cla-not-required,True,False,major,database,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
vmware-tanzu/kubeapps,8127,Bump thiserror from 1.0.67 to 2.0.0 in /cmd/pinniped-proxy,"Bumps [thiserror](https://github.com/dtolnay/thiserror) from 1.0.67 to 2.0.0. <details> <summary>Release notes</summary> <p><em>Sourced from <a href=""https://github.com/dtolnay/thiserror/releases"">thiserror's releases</a>.</em></p> <blockquote> <h2>2.0.0</h2> <h2>Breaking changes</h2> <ul> <li> <p>Referencing keyword-named fields by a raw identifier like <code>{r#type}</code> inside a format string is no longer accepted; simply use the unraw name like <code>{type}</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/347"">#347</a>)</p> <p>This aligns thiserror with the standard library's formatting macros, which gained support for implicit argument capture later than the release of this feature in thiserror 1.x.</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;... {type} ...&quot;)]  // Before: {r#type} pub struct Error {     pub r#type: Type, } </code></pre> </li> <li> <p>Trait bounds are no longer inferred on fields whose value is shadowed by an explicit named argument in a format message (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/345"">#345</a>)</p> <pre lang=""rust""><code>// Before: impl&lt;T: Octal&gt; Display for Error&lt;T&gt; // After: impl&lt;T&gt; Display for Error&lt;T&gt; #[derive(Error, Debug)] #[error(&quot;{thing:o}&quot;, thing = &quot;...&quot;)] pub struct Error&lt;T&gt; {     thing: T, } </code></pre> </li> <li> <p>Tuple structs and tuple variants can no longer use numerical <code>{0}</code> <code>{1}</code> access at the same time as supplying extra positional arguments for a format message, as this makes it ambiguous whether the number refers to a tuple field vs a different positional arg (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/354"">#354</a>)</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;ambiguous: {0} {}&quot;, $N)] //                  ^^^ Not allowed, use #[error(&quot;... {0} {n}&quot;, n = $N)] pub struct TupleError(i32); </code></pre> </li> <li> <p>Code containing invocations of thiserror's <code>derive(Error)</code> must now have a direct dependency on the <code>thiserror</code> crate regardless of the error data structure's contents (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/368"">#368</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/369"">#369</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/370"">#370</a>, <a href=""https://redirect.github.com/dtolnay/thiserror/issues/372"">#372</a>)</p> </li> </ul> <h2>Features</h2> <ul> <li> <p>Support disabling thiserror's standard library dependency by disabling the default &quot;std&quot; Cargo feature: <code>thiserror = { version = &quot;2&quot;, default-features = false }</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/373"">#373</a>)</p> </li> <li> <p>Support using <code>r#source</code> as field name to opt out of a field named &quot;source&quot; being treated as an error's <code>Error::source()</code> (<a href=""https://redirect.github.com/dtolnay/thiserror/issues/350"">#350</a>)</p> <pre lang=""rust""><code>#[derive(Error, Debug)] #[error(&quot;{source} ==&gt; {destination}&quot;)] pub struct Error {     r#source: char,     destination: char, </code></pre> </li> </ul> <!-- raw HTML omitted --> </blockquote> <p>... (truncated)</p> </details> <details> <summary>Commits</summary> <ul> <li><a href=""https://github.com/dtolnay/thiserror/commit/6097d61b5878c4eca29be1cb0d7f96268309dd84""><code>6097d61</code></a> Release 2.0.0</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/40113bd4b53f2a027f26558c90e9984168eb683b""><code>40113bd</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/373"">#373</a> from dtolnay/nostd</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/d8ed5fbc2f7476430ff8e50b37f466638cd732f3""><code>d8ed5fb</code></a> Allow disabling std dependency on 1.81+</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/8277ec4a734ea98c01228d7a970d591ffd9efbe8""><code>8277ec4</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/372"">#372</a> from dtolnay/stdbacktrace</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/d6d896df4c63fcb7b7c34aad5f5f049bf6cfab3b""><code>d6d896d</code></a> Access Backtrace exclusively through ::thiserror</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/d14adfbb60eb19275d60faffcac4372f100bb2cb""><code>d14adfb</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/371"">#371</a> from dtolnay/coreprovider</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/2e99c515f31e99b96c6a4b91ebefe8c798333ece""><code>2e99c51</code></a> Drop Provider API support in pre-1.81 nightlies</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/bf3d6f3f48519f33bc80ca1055d80be45f16d826""><code>bf3d6f3</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/370"">#370</a> from dtolnay/stderror</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/e0e994314b273f9410243f692ab0925ca039ba3a""><code>e0e9943</code></a> Access Error trait exclusively through ::thiserror</li> <li><a href=""https://github.com/dtolnay/thiserror/commit/db7825e956b383d2bd2f60013ef482c2ad0ccec7""><code>db7825e</code></a> Merge pull request <a href=""https://redirect.github.com/dtolnay/thiserror/issues/369"">#369</a> from dtolnay/stdpath</li> <li>Additional commits viewable in <a href=""https://github.com/dtolnay/thiserror/compare/1.0.67...2.0.0"">compare view</a></li> </ul> </details> <br />   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=thiserror&package-manager=cargo&previous-version=1.0.67&new-version=2.0.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---  <details> <summary>Dependabot commands and options</summary> <br />  You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)   </details>",2024-11-07T02:05:44+00:00,2024-11-11T02:04:35+00:00,1,https://github.com/vmware-tanzu/kubeapps/pull/8127,8127.0,,https://github.com/vmware-tanzu/kubeapps/pull/8127,0,0,2,2,34,14,0,0,95.98083333333334,dependencies;rust;cla-not-required,True,False,major,database,"[{""filename"": ""cmd/pinniped-proxy/Cargo.lock"", ""lines_added"": 33, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""cmd/pinniped-proxy/Cargo.toml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,1880,[BUG] Failing utils spec tests,"**Describe the bug** Utils spec tests are failing, when running them (individually or via command crystal spec).  **To Reproduce** `crystal spec` or `crystal spec ./spec/utils/utils_spec.cr`  **Expected behavior** A clear and concise description of what you expected to happen.  **Device (please complete the following information):**  - OS Linux - Distro Ubuntu - Version 22.04 - Architecture x86 - Crystal 1.11.1 - Kubernetes v1.28.3  **How will this be tested? aka Acceptance Criteria (optional)** For verification successfully running `crystal spec ./spec/utils/utils_spec.cr` should be enough.  **Additional context**  ``` macaktom@WPF2LRVSF:~/devel/cnf-testsuite$ crystal spec spec/utils/utils_spec.cr current_branch during compile: ""main"" current_tag during compile:  I, [2024-02-07 14:35:02 +01:00 #1209231]  INFO -- cnf-testsuite: Building ./cnf-testsuite I, [2024-02-07 14:35:25 +01:00 #1209231]  INFO -- cnf-testsuite: Build Success! .....✖️  FAILED: IP addresses found .Successfully setup coredns-1609263557 ✔️  PASSED: No privileged containers Successfully cleaned up coredns-1609263557 ..✔️  PASSED: No privileged containers ✖️  FAILED: Found 1 privileged containers: [""privileged-coredns""] Successfully cleaned up coredns-1609263557 Successfully cleaned up privileged-coredns .FF.....I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite-helm_deploy: Running helm_deploy I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite-helm_deploy: helm_deploy args: #<Sam::Args:0x7fcd34c2b500 @arr=[""verbose""], @named_args={}> I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: check_cnf_config args: #<Sam::Args:0x7fcd34c2b500 @arr=[""verbose""], @named_args={}> I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: check_cnf_config cnf:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_config_list I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find response: [""cnfs/coredns/cnf-testsuite.yml""] I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_runner args: #<Sam::Args:0x7fcd34c2b500 @arr=[""verbose""], @named_args={}> I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_config_list I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find response: [""cnfs/coredns/cnf-testsuite.yml""] I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: ensure_cnf_installed?  true I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: check_cnf_config args: #<Sam::Args:0x7fcd34c2b500 @arr=[""verbose""], @named_args={}> I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: check_cnf_config cnf:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_config_list I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find: find cnfs/* -name ""cnf-testsuite.yml"" I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: find response: [""cnfs/coredns/cnf-testsuite.yml""] I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: CNF configs found: 1 I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: airgapped: false I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: generate_tar_mode: false I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: generate_and_set_release_name I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: generate_and_set_release_name config_yml_path: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: airgapped mode: false I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: generate_tar_mode: false I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_path I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: src_helm_directory:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_installation_method I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_installation_method config: #<Totem::Config:0x7fcd33377f00> I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_installation_method config: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: directory_parameter_split :  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: directory_parameter_split :  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: directory :  parameters:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: release_name: coredns I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: helm_directory:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: manifest_directory:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: Building helm_directory and manifest_directory full paths I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: full_helm_directory: /home/macaktom/devel/cnf-testsuite/cnfs/coredns/ exists? true I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: full_manifest_directory: /home/macaktom/devel/cnf-testsuite/cnfs/coredns/ exists? true I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf_destination_dir config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: parsed_config_file: cnfs/coredns/cnf-testsuite.yml I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: release_name: coredns I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cnf destination dir: /home/macaktom/devel/cnf-testsuite/cnfs/coredns I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: ensure_cnf_testsuite_yml_dir I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: USING EXPORTED CHART PATH I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite-helm_deploy: Starting test I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: KubectlClient::Get.configmap command: kubectl get configmap cnf-testsuite-coredns-startup-information -o json ✔️  PASSED: Helm deploy successful ⚙️🛠️⬆☁️ I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_points: task: helm_deploy is worth: 5 points I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: cmd: /home/macaktom/devel/cnf-testsuite/cnf-testsuite helm_deploy verbose I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite-helm_deploy: task_runtime=263; start_time=2024-02-07 13:37:47 UTC; end_time:2024-02-07 13:37:47 UTC I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type_by_task I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: points: {""name"" => ""helm_deploy"", ""tags"" => ""compatibility, dynamic, workload, cert, normal""} I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: resp: [""compatibility"", ""dynamic"", ""workload"", ""cert"", ""normal""] I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type x: compatibility acc:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type x: dynamic acc:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type x: workload acc:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type x: cert acc:  I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type x: normal acc: cert I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: task_type: normal I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: upsert_task: task: helm_deploy has status: passed and is awarded: 5 points. Runtime: 263 seconds I, [2024-02-07 14:37:47 +01:00 #1219658]  INFO -- cnf-testsuite: results yaml: {""name"" => ""cnf testsuite"", ""testsuite_version"" => ""main-2024-02-07-143747-4115cbe1"", ""status"" => nil, ""command"" => ""/home/macaktom/devel/cnf-testsuite/cnf-testsuite helm_deploy verbose"", ""points"" => nil, ""exit_code"" => 0, ""items"" => [{""name"" => ""helm_deploy"", ""status"" => ""passed"", ""type"" => ""normal"", ""points"" => 5}]} Successfully cleaned up coredns ...  Failures:    1) Utils 'task_runner' should run a test against a single cnf if passed a cnf-config argument even if there are multiple cnfs installed      Failure/Error: (resp).includes?(""✖️  FAILED: Found 1 privileged containers"").should be_true         Expected: true             got: false       # spec/utils/utils_spec.cr:199    2) Utils 'logger' command line logger level setting via config.yml      Failure/Error: (/DEBUG -- cnf-testsuite: debug test/ =~ response_s).should be_nil         Expected: 46 to be nil       # spec/utils/utils_spec.cr:214  Finished in 2:46 minutes 19 examples, 2 failures, 0 errors, 0 pending  Failed examples:  crystal spec spec/utils/utils_spec.cr:193 # Utils 'task_runner' should run a test against a single cnf if passed a cnf-config argument even if there are multiple cnfs installed crystal spec spec/utils/utils_spec.cr:209 # Utils 'logger' command line logger level setting via config.yml ``` ",2024-02-07T13:58:54+00:00,2024-03-07T22:24:03+00:00,7,https://github.com/lfn-cnti/testsuite/issues/1880,1892.0,2024-02-15T16:34:47+00:00,https://github.com/lfn-cnti/testsuite/pull/1892,1,0,3,4,9,6,2,0,194.5980555555556,bug,True,False,normal,configuration,"[{""filename"": ""config.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""spec/platform/observability_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""spec/utils/utils_spec.cr"", ""lines_added"": 5, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/configuration.cr"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
lfn-cnti/testsuite,1864,[BUG] wrong usage of named arguments across the code,"**Describe the bug** On many occurrence across the cnf-testsuite code, passing of named arguments are done in a wrong way.  Example1: https://github.com/cncf/cnf-testsuite/blob/main/src/tasks/platform/observability.cr#L112 `task_response = CNFManager::Task.task_runner(args, check_cnf_installed=false) do |args|`  Example2: https://github.com/cncf/cnf-testsuite/blob/main/src/tasks/utils/cnf_manager.cr#L269 `cnf_configs = self.cnf_config_list(silent=true)`  The right way how to pass named arguments in crystal is using ':' https://crystal-lang.org/reference/1.11/syntax_and_semantics/default_and_named_arguments.html like this: `task_response = CNFManager::Task.task_runner(args, check_cnf_installed: false) do |args|` `cnf_configs = self.cnf_config_list(silent: true)`  How and why it is working currently: 'check_cnf_installed=false' is evaluated as an assignent of new variable named ""check_cnf_installed"" to ""false"". Then the whole assignment returns ""false"" and passes it as a positional argument. Most such cases work just accidentally, also due to crystal's loose assignement of positional and named arguments. However when some changes further changes are done, it can stop working.  **To Reproduce** 1. Look across the code 2. Shake your head  **Expected behavior** Always use the right way how to pass positional arguments.  **Screenshots**  **Device (please complete the following information):**  **How will this be tested? aka Acceptance Criteria (optional)** Best if an intelligent grep command is developed to identify all such wrong assignments. Fix it, re-run the grep, it should return no occurrences after fixing. ",2024-01-25T16:07:38+00:00,2024-03-05T16:48:05+00:00,2,https://github.com/lfn-cnti/testsuite/issues/1864,1869.0,2024-02-26T22:41:35+00:00,https://github.com/lfn-cnti/testsuite/pull/1869,0,0,15,15,35,35,0,0,774.5658333333333,bug,True,False,normal,configuration,"[{""filename"": ""spec/workload/microservice_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/hardware_and_scheduling.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/observability.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/platform.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/resilience.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/security.cr"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/cnf_manager.cr"", ""lines_added"": 9, ""lines_deleted"": 9, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/cnf_manager_airgap.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/k8s_tshark.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/kyverno.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/5g_validator.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/configuration.cr"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/mock_task.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/state.cr"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""utils/airgap/airgap.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",workload,False
openkruise/kruise,1765,[BUG] Leakage of CloneSet Expectation in kruise 1.5x,<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**:  **What you expected to happen**:  **How to reproduce it (as minimally and precisely as possible)**:  **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ,2024-09-29T02:44:31+00:00,2025-02-09T18:50:07+00:00,2,https://github.com/openkruise/kruise/issues/1765,1946.0,2025-03-04T05:12:00+00:00,https://github.com/openkruise/kruise/pull/1946,1,0,0,1,1,1,2,0,3746.4580555555553,kind/bug;wontfix,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
lfn-cnti/testsuite,1869,Invalid named arguments syntax,"## Description Fixed syntax for named arguments. Also changed name of named argument, which had typo in microservice_spec.cr.  Tested by running all tests and individual spec tests. Found only one when running microservice_spec.cr -> fixed typo is part of this pull request.  ## Issues: Refs: #1864  ## Types of changes:  - [x] Bug fix (non-breaking change which fixes an issue)  - [ ] New feature (non-breaking change which adds functionality)  - [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)  - [ ] Documentation update  ## Checklist: **Documentation** - [ ] My change requires a change to the documentation. - [ ] I have updated the documentation accordingly. - [x] No updates required.  **Code Review** - [ ] Does the test handle fatal exceptions, ie. rescue block  **Issue** - [ ] Tasks in issue are checked off ",2024-02-01T08:16:00+00:00,2024-02-26T22:41:35+00:00,0,https://github.com/lfn-cnti/testsuite/pull/1869,1869.0,2024-02-26T22:41:35+00:00,https://github.com/lfn-cnti/testsuite/pull/1869,0,0,15,15,35,35,0,0,614.4263888888889,,True,False,normal,ui,"[{""filename"": ""spec/workload/microservice_spec.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/hardware_and_scheduling.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/observability.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/platform.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/resilience.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/platform/security.cr"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/cnf_manager.cr"", ""lines_added"": 9, ""lines_deleted"": 9, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/cnf_manager_airgap.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/k8s_tshark.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/utils/kyverno.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/5g_validator.cr"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/configuration.cr"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/mock_task.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""src/tasks/workload/state.cr"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""utils/airgap/airgap.cr"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",workload,False
openkruise/kruise,1656,[BUG] imagelistpulljob的message一直卡在0.0%，SUCCEED一直为0，直到activeDeadlineSeconds之后才能显示SUCCEED个数,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: imagelistpulljob的message一直卡在0.0%，SUCCEED一直为0，直到activeDeadlineSeconds之后才能显示SUCCEED个数。尽管所有节点的nodeimage已经完成镜像拉取 **What you expected to happen**: 每个节点拉取成功后能够及时显示成功百分比和SUCCEED个数 **How to reproduce it (as minimally and precisely as possible)**: 1、使用golang client 1.6.0，创建带有secret和ActiveDeadlineSeconds的job，然后执行创建 样例如下： job := &v1alpha1.ImageListPullJob{ 		TypeMeta:   apisv1.TypeMeta{}, 		ObjectMeta: apisv1.ObjectMeta{Name: TaskName, Namespace: Namespace, ResourceVersion: ""apps.kruise.io/v1alpha1""}, 		Spec: v1alpha1.ImageListPullJobSpec{ 			Images: ImageAddresses, 			ImagePullJobTemplate: v1alpha1.ImagePullJobTemplate{ 				PullSecrets: Secrets, 				Parallelism:      &intstr.IntOrString{Type: intstr.Int, IntVal: int32(10)}, 				PullPolicy:       &v1alpha1.PullPolicy{}, 				CompletionPolicy: v1alpha1.CompletionPolicy{ActiveDeadlineSeconds: &ActiveDeadlineSeconds}, 				SandboxConfig:    nil, 				ImagePullPolicy:  ""Always"", 			}, 		}, 		Status: v1alpha1.ImageListPullJobStatus{}, 	}  2、查看nodeimage、imageListpulljob、imagepulljob的SUCCEED个数和message **Anything else we need to know?**:  **Environment**: - Kruise version: 1.5.3    - Kubernetes version (use `kubectl version`): 1.24.4 TKE - Install details (e.g. helm install args):   - Others: Golang client：1.6.0",2024-07-04T12:27:14+00:00,2025-01-03T08:03:32+00:00,1,https://github.com/openkruise/kruise/issues/1656,1827.0,,https://github.com/openkruise/kruise/pull/1827,1,0,0,1,1,1,2,0,4387.605,kind/bug;wontfix,True,False,normal,configuration,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
radius-project/radius,8535,Scheduled long running test failed - Run ID: 13361565264,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13361565264).  [AB#14526](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/14526)",2025-02-17T02:33:55+00:00,2025-02-18T17:15:43+00:00,2,https://github.com/radius-project/radius/issues/8535,5957.0,2023-07-28T21:50:17+00:00,https://github.com/radius-project/radius/pull/5957,1,0,0,1,2,2,4,0,-13660.727222222222,bug;test-failure,True,False,major,networking,"[{""filename"": "".github/workflows/purge-test-resources.yaml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}]",,False
GoogleCloudPlatform/kubernetes-engine-samples,678,transfer-datasets.sh still references deleted variables.sh file,"Following step 3 of [batch ml GKE tutorial](https://cloud.google.com/kubernetes-engine/docs/tutorials/batch-ml-workload#set_up_a_redis_job_queue):  ``` sh scripts/transfer-datasets.sh scripts/transfer-datasets.sh: line 16: scripts/variables.sh: No such file or directory ``` #510 deleted the ""obsolete"" script however transfer-datasets.sh still sources from it. No clear workaround as deleting the source command from the script errors out as well. ",2023-06-22T19:10:43+00:00,2024-07-24T19:24:43+00:00,1,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/issues/678,1635.0,,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/pull/1635,2,0,0,2,2,2,4,0,9552.233333333334,type: bug;priority: p2,True,False,normal,database,"[{""filename"": ""databases/hello-app-cloud-spanner/client/package-lock.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""databases/hello-app-cloud-spanner/client/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",databases,False
GoogleCloudPlatform/kubernetes-engine-samples,679,fix: Regression in the AI-ML tutorial' scripts,Resolves #678 ,2023-06-22T19:58:44+00:00,2024-02-23T18:20:52+00:00,3,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/pull/679,1635.0,,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/pull/1635,2,0,0,2,2,2,4,0,5902.368888888889,,True,False,normal,functional,"[{""filename"": ""databases/hello-app-cloud-spanner/client/package-lock.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""databases/hello-app-cloud-spanner/client/package.json"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",databases,False
openkruise/kruise,1749,"[BUG] When 'ordinals' are set, the 'pratition' behaves differently during updates and deleting","<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**:  After declaring the ordinal index, confusion occurred in the logical index recognition of pods when updating, scaling, or deleting pods with partition. eg. with rollingUpdate, when ordinal index=2, updating partition from 5 to 3, pod-3 still use old template, but pod-3 uses new template when recreating it.  **What you expected to happen**: In the above example, p3  should use old template, because of ordinals=2, the logic idx of p3 should be 3-2=1, which is smaller than partition=3  **How to reproduce it (as minimally and precisely as possible)**:  ordinals=2, replicas=5, partition=7, old pods [p2, p3, p4, p5, p6] update sts template and change partition=5 nothing happend, old pods [p2, p3, p4, p5, p6] update partition=3, sts updates automatically, old pods [p2, p3, p4], new pods [p5, p6] then, delete p3 then it will be created with new template, old pods [p2, p4], new pods [p3, p5, p6] delete p4  then it will be created with new template, old pods  [p2], new pods [p3, p4, p5, p6] delete p2  then it will be created with old template, old pods  [p2], new pods [p3, p4, p5, p6]  **Anything else we need to know?**: Partitioned rolling updates in k8s https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions  **Environment**: ubuntu 16.04 - Kruise version: 1.7.1 - Kubernetes version (use `kubectl version`): 1.20.7 - Install details (e.g. helm install args): - Others: ",2024-09-14T07:36:17+00:00,2024-10-16T10:35:50+00:00,2,https://github.com/openkruise/kruise/issues/1749,1898.0,2025-02-10T06:15:48+00:00,https://github.com/openkruise/kruise/pull/1898,1,0,0,1,1,1,2,0,3574.658611111111,kind/bug,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1740,fix a bug about workloadspread webhook validation on patches,"<!--  Please make sure you have read and understood the contributing guidelines; https://github.com/openkruise/kruise/blob/master/CONTRIBUTING.md -->  ### Ⅰ. Describe what this PR does  patches volume claim templates into pods before ValidatePodSpec in workloadspread patch validation  ### Ⅱ. Does this pull request fix one issue? <!--If so, add ""fixes #xxxx"" below in the next line, for example, fixes #15. Otherwise, add ""NONE"" -->  fixes #1738  ### Ⅲ. Describe how to verify it  1. create a CloneSet with volume claim template 2. create a workloadspread references it  ### Ⅳ. Special notes for reviews ",2024-09-12T08:32:21+00:00,2024-09-24T03:21:59+00:00,1,https://github.com/openkruise/kruise/pull/1740,1946.0,2025-03-04T05:12:00+00:00,https://github.com/openkruise/kruise/pull/1946,1,0,0,1,1,1,2,0,4148.660833333333,,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1742,[BUG] Race Condition While Running Tests,<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: There is a race condition while running tests.  **How to reproduce it (as minimally and precisely as possible)**: Edit test at Makefile with flag `-race`. ,2024-09-13T04:53:46+00:00,2024-09-13T06:46:59+00:00,0,https://github.com/openkruise/kruise/issues/1742,1946.0,2025-03-04T05:12:00+00:00,https://github.com/openkruise/kruise/pull/1946,1,0,0,1,1,1,2,0,4128.303888888889,kind/bug,True,False,normal,functional,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1686,[BUG] local kind test tools are invalid,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**:  I want to use command `make create-cluster` to set up a Kind cluster locally for testing purposes. By default, the Makefile builds Kind results in an error since there is no code in the `tools/src/kind` directory. To resolve this, I have to manually download the Kind executable by running the command `go install sigs.k8s.io/kind@v0.23.0`, and then copy it into the `tools/bin` directory to proceed with further operations.  **What you expected to happen**:  when I type `make create-cluster` command, the Makefile should download the Kind executable to `tools/bin` automatically instead of building the ghost code.  **How to reproduce it (as minimally and precisely as possible)**:  1. clone a clean kruise repository 2. type `make create-cluster`  **Anything else we need to know?**:  **Environment**: - Kruise version: 1.6.3 - Kubernetes version (use `kubectl version`): Any - Install details (e.g. helm install args): Nothing - Others: ",2024-08-02T06:32:26+00:00,2024-09-05T02:31:01+00:00,1,https://github.com/openkruise/kruise/issues/1686,1853.0,,https://github.com/openkruise/kruise/pull/1853,1,0,0,1,1,1,2,0,811.9763888888889,kind/bug,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1597,[BUG]fake client break changes affect NodePodProbe ut cases when bump deps k8s to v1.28,<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: fake client break changes affect NodePodProbe ut cases when bump deps k8s to v1.28 ![image](https://github.com/openkruise/kruise/assets/24547351/99a5b3ed-35ef-48f5-b020-c501750fb3ef)  I ignore three cases.  May we will make it  more general in the feature. ![image](https://github.com/openkruise/kruise/assets/24547351/22c59f6c-6dac-4bc9-a5d1-8d502efd06d4)  **What you expected to happen**:  NodePodProbe ut cases can pass.  **How to reproduce it (as minimally and precisely as possible)**:  **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ,2024-04-26T08:18:58+00:00,2024-08-03T02:35:47+00:00,1,https://github.com/openkruise/kruise/issues/1597,1827.0,,https://github.com/openkruise/kruise/pull/1827,1,0,0,1,1,1,2,0,2370.280277777778,kind/bug;wontfix,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
ray-project/kuberay,3068,[Chore] Consistency check operates on non-existent folder,### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  Consistency check `ray-operator-verify-crd-rbac` verifies changed files in two directories:  1. ./config/bases/*.yaml 2. ./config/rbac/*.yaml  However first one does not exist in the ray-operator (and any other place in kuberay). Shouldn't it be ` ./config/crd/bases/*.yaml`?  ### Reproduction script  Check .github/workflows/consistency-check.yaml And verify there is no `config/bases` folder in `ray-operator` directory.  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [x] Yes I am willing to submit a PR!,2025-02-17T11:54:55+00:00,2025-02-27T19:28:28+00:00,0,https://github.com/ray-project/kuberay/issues/3068,3103.0,2025-02-27T19:28:27+00:00,https://github.com/ray-project/kuberay/pull/3103,1,0,0,1,1,1,2,0,247.5588888888889,bug;triage,True,False,normal,configuration,"[{""filename"": "".github/workflows/consistency-check.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1509,Export defaults package to the kruise-api repo,"We're using CloneSet v1alpha1 in a production setting, and our controller is racing with the CloneSet defaulting webhook.  When we calculate a CloneSet.spec and submit it to the apiserver, the defaulting happens. Next time we get the object, we reconcile the managed CloneSet again from scratch (without the defaults), so an extra ""Update"" happens (because what we calculated != what we got via informer).  We want to use the `defaults.SetDefaultsCloneSet`  method https://github.com/openkruise/kruise/blob/5421ee7c8e9e54ec38c1c1e20f4deb4a722f47a7/apis/apps/defaults/v1alpha1.go#L256 however it is hellish to import the kruise repo due to its imports e.g.   - 	k8s.io/kube-scheduler v0.0.0 - 	github.com/docker/distribution v2.8.2+incompatible - 	github.com/google/cadvisor v0.39.3 - 	github.com/heketi/heketi v10.3.0+incompatible - 	github.com/ishidawataru/sctp v0.0.0-20190723014705-7c296d48a2b5 - 	github.com/moby/ipvs v1.0.1  and we're not looking to import these packages.  It would be ideal   1. if you copy the `defaults` package to the https://godoc.org/github.com/openkruise/kruise-api 2. make the `SetDefaultsCloneSets` method **versioned** i.e. define it directly on the `v1alpha1.CloneSet` type.  Appreciate if you can address this.",2024-02-23T23:36:55+00:00,2024-04-24T17:43:13+00:00,4,https://github.com/openkruise/kruise/issues/1509,1937.0,2025-03-18T05:22:48+00:00,https://github.com/openkruise/kruise/pull/1937,1,0,0,1,1,1,2,0,9317.764722222222,kind/bug,True,False,normal,ui,"[{""filename"": "".github/workflows/scorecard.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
ray-project/kuberay,2943,[Bug] RayJob submitter doesn't support entrypoint with `[]`,### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  https://github.com/ray-project/kuberay/blob/90c8dc9e21ac54050d0901ac2ff3ff611e11fce9/ray-operator/config/samples/ray-job.kueue-toy-sample.yaml#L11  ### Reproduction script  https://github.com/ray-project/kuberay/blob/90c8dc9e21ac54050d0901ac2ff3ff611e11fce9/ray-operator/config/samples/ray-job.kueue-toy-sample.yaml#L11  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [ ] Yes I am willing to submit a PR!,2025-02-06T08:33:53+00:00,2025-02-06T18:30:31+00:00,3,https://github.com/ray-project/kuberay/issues/2943,2941.0,2025-02-06T08:36:08+00:00,https://github.com/ray-project/kuberay/pull/2941,6,0,0,6,19,58,77,0,0.0375,bug;rayjob;release-blocker;1.3.0-rc.1,True,False,normal,configuration,"[{""filename"": ""ray-operator/config/samples/ray-job.custom-head-svc.yaml"", ""lines_added"": 4, ""lines_deleted"": 5, ""file_type"": ""config""}, {""filename"": ""ray-operator/config/samples/ray-job.kueue-toy-sample.yaml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": ""ray-operator/config/samples/ray-job.modin.yaml"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""config""}, {""filename"": ""ray-operator/config/samples/ray-job.resources.yaml"", ""lines_added"": 3, ""lines_deleted"": 27, ""file_type"": ""config""}, {""filename"": ""ray-operator/config/samples/ray-job.sample.yaml"", ""lines_added"": 4, ""lines_deleted"": 5, ""file_type"": ""config""}, {""filename"": ""ray-operator/config/samples/ray-job.shutdown.yaml"", ""lines_added"": 3, ""lines_deleted"": 16, ""file_type"": ""config""}]",samples,False
openkruise/kruise,1567,[BUG] pub webhooks unexpectedly return error when PUB is NOT FOUND,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**:  PUB may unexpectedly interrupt Pod gc issued by KCM, which can lead Pod leak if KCM gc did not retry or retry many hours later.  **What you expected to happen**:  PUB webhooks never interrupt Pod gc.  **How to reproduce it (as minimally and precisely as possible)**:  Delete workload let's say Sts or CloneSet, then Pod will be deleted by KCM gc later. Sometimes, Pod to delete will be leaking there for a lone time.  **Anything else we need to know?**:  The root cause is we return error when PUB CR is deleted in RetryOnConflict. Related codes is here, https://github.com/openkruise/kruise/blob/8bb89648de6c4944239c03a9a4a22f18e9616193/pkg/control/pubcontrol/pub_control_utils.go#L105  The solution is simple, just check error type as we can , ignore it if it is NotFound error.  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ",2024-04-11T05:06:15+00:00,2024-04-23T11:39:58+00:00,1,https://github.com/openkruise/kruise/issues/1567,1775.0,2024-10-09T09:42:47+00:00,https://github.com/openkruise/kruise/pull/1775,1,0,0,1,1,1,2,0,4348.608888888889,kind/bug;kind/good-first-issue,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1478,[BUG] cloneset scaling down failed (manually/automatically) when minReadySeconds is set big,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: cloneset scaling down failed (manually / automatically) when minReadySeconds is set big, like 99999. spec.replicas of the cloneset is patched as expectedly, but the pods keep unremoved  **What you expected to happen**: deployment in the same setting could scale down successfully  **How to reproduce it (as minimally and precisely as possible)**:  cloneset.yaml here: ```yaml apiVersion: apps.kruise.io/v1alpha1 kind: CloneSet metadata:   labels:     app: sample   name: sample spec:   replicas: 5   selector:     matchLabels:       app: sample   minReadySeconds: 99999   updateStrategy:     type: ReCreate   template:     metadata:       labels:         app: sample     spec:       containers:       - name: openresty         image: openresty/openresty:alpine         ports:         - containerPort: 80         resources:           requests:             cpu: ""200m""            limits:             cpu: ""500m"" ``` first, `kubectl apply -f cloneset.yaml` after the pods ready, `kubectl scale cloneset/sampe --replicas=3` **Anything else we need to know?**:  1. After `kubectl patch cloneset sample --type=merge -p '{""spec"":{""minReadySeconds"":null}}'`, problem solved.  2. Deployment scales down well in the same setting  **Environment**: - Kruise version:1.5.1 - Kubernetes version (use `kubectl version`): v1.28.3 - Install details (e.g. helm install args): none - Others: none ",2023-12-27T06:21:21+00:00,2024-04-09T05:35:36+00:00,1,https://github.com/openkruise/kruise/issues/1478,1937.0,2025-03-18T05:22:48+00:00,https://github.com/openkruise/kruise/pull/1937,1,0,0,1,1,1,2,0,10727.024166666666,kind/bug;wontfix,True,False,normal,configuration,"[{""filename"": "".github/workflows/scorecard.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1180,Container failed due to cgroup cannot write file,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: I create a pod under advanced statefulset (asts) with inplace update for image enable.   And I used in-place update feature on CPU/Mem from upstream.  https://github.com/kubernetes/kubernetes/pull/102884/ When I edit nginx-asts and change image from nginx:1.14.2 to nginx:1.14.1 and CPU from 1 to 2 both in limits and requests, the pod will get into the CrashLoopBackoff. Checked the pod error message:  ```yaml 'failed to create containerd task: OCI runtime create failed: container_linux.go:370:           starting container process caused: process_linux.go:459: container init           caused: process_linux.go:422: setting cgroup config for procHooks process           caused: failed to write ""200000"" to ""/sys/fs/cgroup/cpu,cpuacct/kubepods/podc366e2e3-155e-4d4b-a36d-369042e75266/1d33192617684dc3ddccc42d4d370c0e8a3b27bb19630a3c29037d52093a5935/cpu.cfs_quota_us"":           write /sys/fs/cgroup/cpu,cpuacct/kubepods/podc366e2e3-155e-4d4b-a36d-369042e75266/1d33192617684dc3ddccc42d4d370c0e8a3b27bb19630a3c29037d52093a5935/cpu.cfs_quota_us:           invalid argument: unknown' ``` **What you expected to happen**: Since pod enable Open Kruise inplace update on image, pod will have RESTART counter +1 Since kubelet is used inplace update for CPU/Mem, container status will have  ```yaml resourcesAllocated:       cpu: ""2"" ``` **How to reproduce it (as minimally and precisely as possible)**: Build kubelet with https://github.com/kubernetes/kubernetes/pull/102884/ Deploy Openkruise Apply asts.yaml ```yaml apiVersion: apps.kruise.io/v1beta1 kind: StatefulSet metadata:   creationTimestamp: ""2023-02-13T19:12:44Z""   generation: 5   labels:     app: nginx   name: nginx-asts spec:   podManagementPolicy: Parallel   replicas: 1   revisionHistoryLimit: 10   selector:     matchLabels:       app: nginx   template:     metadata:       creationTimestamp: null       labels:         app: nginx     spec:       readinessGates:       - conditionType: InPlaceUpdateReady       containers:       - image: nginx:1.14.2         imagePullPolicy: IfNotPresent         name: nginx         ports:         - containerPort: 80         resources:           limits:             cpu: 1             memory: 1Gi           requests:             cpu: 1             memory: 1Gi   updateStrategy:     rollingUpdate:       maxUnavailable: 1       minReadySeconds: 0       partition: 0       podUpdatePolicy: InPlaceIfPossible     type: RollingUpdate ``` Wait for the pod is in running stage Edit asts and: 1. change spec/template/spec/containers/0/image from nginx:1.14.2 to nginx:1.14.1 2. change spec/template/spec/containers/0/resources/requests/cpu from 1 to 2  3. change spec/template/spec/containers/0/resources/limits/cpu from 1 to 2 Save asts and exit   **Anything else we need to know?**: None  **Environment**: - Kruise version: release-1.0 - Kubernetes version (use `kubectl version`): ```bash   Client Version: version.Info{Major:""1"", Minor:""19+"", GitVersion:""v1.19.16-vke.1"", GitCommit:""9be13fe754df90aadcf00811ad071ea36ea29116"", GitTreeState:""clean"", BuildDate:""2022-09-07T15:42:42Z"", GoVersion:""go1.15.15"", Compiler:""gc"", Platform:""linux/amd64""}   Server Version: version.Info{Major:""1"", Minor:""23+"", GitVersion:""v1.23.0-alpha.3.511+4017d508885e65-dirty"", GitCommit:""4017d508885e65a298fbc2f5091765faf1c838cb"", GitTreeState:""dirty"", BuildDate:""2022-04-06T16:39:56Z"", GoVersion:""go1.17.2"", Compiler:""gc"", Platform:""linux/amd64""} ``` - Install details (e.g. helm install args): None - Others: None ",2023-02-18T00:46:22+00:00,2024-04-09T03:25:52+00:00,1,https://github.com/openkruise/kruise/issues/1180,1768.0,2024-10-08T01:26:05+00:00,https://github.com/openkruise/kruise/pull/1768,10,0,0,10,39,39,78,0,14352.661944444444,kind/bug,True,False,critical,configuration,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""config""}, {""filename"": "".github/workflows/codeql.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/docker-image.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/e2e-1.18.yaml"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""config""}, {""filename"": "".github/workflows/e2e-1.20-EphemeralJob.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/e2e-1.24.yaml"", ""lines_added"": 8, ""lines_deleted"": 8, ""file_type"": ""config""}, {""filename"": "".github/workflows/e2e-1.26.yaml"", ""lines_added"": 8, ""lines_deleted"": 8, ""file_type"": ""config""}, {""filename"": "".github/workflows/e2e-1.28.yaml"", ""lines_added"": 8, ""lines_deleted"": 8, ""file_type"": ""config""}, {""filename"": "".github/workflows/license.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": "".github/workflows/scorecard.yml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1545,[BUG] cloneset 发布的实例数与灰度比例不符,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: 一个 cloneset 的副本数设置为 4，灰度比例设置为 50%，出现了发布 3 个实例的情况  **What you expected to happen**: 副本数为 4，灰度比例为 50%，期望发布 2 个实例  **How to reproduce it (as minimally and precisely as possible)**: 1. 首先将一个 cloneset 的副本数设置为 2，以灰度比例 50% 发布新版本，此时会得到 1 个新实例 1 个旧实例 2. 删除步骤 1 中旧的那个实例 3. 删除步骤 1 中新的那个实例，然后立即将 clonset 的副本数调整为 4 4. 此时会得到 3 个新实例，1 个旧实例，与 50% 的灰度比例不符  **Anything else we need to know?**:  **Environment**: - Kruise version: v1.1 - Kubernetes version (use `kubectl version`): client-go v0.20.2 - Install details (e.g. helm install args): - Others: yaml 截图 <img width=""303"" alt=""image"" src=""https://github.com/openkruise/kruise/assets/20581567/aa280015-d32f-415d-816b-6fd962d75099"">  ",2024-03-27T03:22:08+00:00,2024-04-02T06:07:07+00:00,2,https://github.com/openkruise/kruise/issues/1545,1775.0,2024-10-09T09:42:47+00:00,https://github.com/openkruise/kruise/pull/1775,1,0,0,1,1,1,2,0,4710.344166666667,kind/bug,True,False,normal,configuration,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1555,[BUG] flaky unit test TestRevisionManage," **What happened**: unit test fails sometimes  ``` I0331 08:45:12.439640   29806 uniteddeployment_update.go:110] UnitedDeployment default/foo needs creating subset (StatefulSet) with name: [subset-a] --- FAIL: TestRevisionManage (4.18s)     revision_test.go:117:          Expected             <*errors.StatusError | 0xc0001488c0>: {                 ErrStatus: {                     TypeMeta: {Kind: """", APIVersion: """"},                     ListMeta: {                         SelfLink: """",                         ResourceVersion: """",                         Continue: """",                         RemainingItemCount: nil,                     },                     Status: ""Failure"",                     Message: ""Operation cannot be fulfilled on uniteddeployments.apps.kruise.io \\""foo\\"": the object has been modified; please apply your changes to the latest version and try again"",                     Reason: ""Conflict"",                     Details: {                         Name: ""foo"",                         Group: ""apps.kruise.io"",                         Kind: ""uniteddeployments"",                         UID: """",                         Causes: nil,                         RetryAfterSeconds: 0,                     },                     Code: 409,                 },             }         to be nil ``` **What you expected to happen**:    test case should be stable   **How to reproduce it (as minimally and precisely as possible)**:  **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ",2024-04-01T01:52:31+00:00,2024-04-01T08:32:41+00:00,0,https://github.com/openkruise/kruise/issues/1555,1775.0,2024-10-09T09:42:47+00:00,https://github.com/openkruise/kruise/pull/1775,1,0,0,1,1,1,2,0,4591.837777777778,kind/bug,True,False,normal,ui,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
openkruise/kruise,1528,[BUG] ImagePullJob delayed for five minutes.,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: ImagePullJob delayed for five minutes.  **How to reproduce it (as minimally and precisely as possible)**: 1. Create ImageListPullJob for nginx and busybox: ```yaml  apiVersion: apps.kruise.io/v1alpha1 metadata:   name: test-imagelistpulljob spec:   images:     - registry.k8s.io/e2e-test-images/nginx:1.14-4     - registry.k8s.io/e2e-test-images/busybox:1.29-4   parallelism: 4   pullPolicy:     timeoutSeconds: 50     backoffLimit: 2   completionPolicy:     type: Always     activeDeadlineSeconds: 50     ttlSecondsAfterFinished: 20   imagePullPolicy: Always ``` This will create imagepulljob-1 for image nginx:1.14-4 and imagepulljob-2 for image busybox:1.29-4 2. ImagePullJob controller reconcile imagepulljob-1, and sync nginx:1.14-4 to NodeImage worker-1. ```yaml apiVersion: apps.kruise.io/v1alpha1 kind: NodeImage metadata:   name: worker-1 spec:   images:     nginx:       tags:         - createdAt: ""2024-03-20T02:14:49Z""           pullPolicy:             activeDeadlineSeconds: 50             backoffLimit: 2             timeoutSeconds: 50             ttlSecondsAfterFinished: 570           tag: 1.14-4 ``` Also, to prevent frequent Updates, add the NodeImage to the Expectation with the following code: ![image](https://github.com/openkruise/kruise/assets/25051767/29073666-2c37-441e-81a0-76961b021281) 3. ImagePullJob controller reconcile imagepulljob-2, and the above update of NodeImage work-1 is not synchronized back to Kruise Cache, then will be requeued. ![image](https://github.com/openkruise/kruise/assets/25051767/3c6d182a-2722-431f-a310-a9c0558ccb9b) 4. NodeImage worker-1 synchronized back to Kruise Cache, but since the NodeImage work-1 doesn't contain the image busybox:1.29-4, it doesn't trigger the ImagePullJob imagepulljob-2  reconcile. ![image](https://github.com/openkruise/kruise/assets/25051767/4c240e08-d19f-45f7-9548-6dc98903c129) 5. ImagePullJob controller reconcile imagepulljob-2 with the above 5 mins ![image](https://github.com/openkruise/kruise/assets/25051767/1c2e106c-7508-4363-bae7-8a5439cd35da)  **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others:    **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ",2024-03-20T02:28:00+00:00,2024-03-21T05:06:16+00:00,0,https://github.com/openkruise/kruise/issues/1528,1775.0,2024-10-09T09:42:47+00:00,https://github.com/openkruise/kruise/pull/1775,1,0,0,1,1,1,2,0,4879.246388888889,kind/bug,True,False,normal,configuration,"[{""filename"": "".github/workflows/ci.yaml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
GoogleCloudPlatform/bank-of-anthos,2193,Services failing to start with a state of `CrashLoopBackOff`,"### Describe the bug  The following services are failing to start with a state of `CrashLoopBackOff`: - `contacts` - `userservice`  - `frontend`  ### To Reproduce  Build the images for the above services and deploy to GKE via `skaffold`.  ### Logs   ``` - bank-of-anthos-staging:pod/contacts-77f6b5d6f4-w8kh7: container contacts is backing off waiting to restart       > [contacts-77f6b5d6f4-w8kh7 contacts] Traceback (most recent call last):       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/bin/gunicorn"", line 8, in <module>       > [contacts-77f6b5d6f4-w8kh7 contacts]     sys.exit(run())       > [contacts-77f6b5d6f4-w8kh7 contacts]              ^^^^^       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/app/wsgiapp.py"", line 67, in run       > [contacts-77f6b5d6f4-w8kh7 contacts]     WSGIApplication(""%(prog)s [OPTIONS] [APP_MODULE]"", prog=prog).run()       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/app/base.py"", line 236, in run       > [contacts-77f6b5d6f4-w8kh7 contacts]     super().run()       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/app/base.py"", line 72, in run       > [contacts-77f6b5d6f4-w8kh7 contacts]     Arbiter(self).run()       > [contacts-77f6b5d6f4-w8kh7 contacts]     ^^^^^^^^^^^^^       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/arbiter.py"", line 58, in __init__       > [contacts-77f6b5d6f4-w8kh7 contacts]     self.setup(app)       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/arbiter.py"", line 93, in setup       > [contacts-77f6b5d6f4-w8kh7 contacts]     self.log = self.cfg.logger_class(app.cfg)       > [contacts-77f6b5d6f4-w8kh7 contacts]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/glogging.py"", line 194, in __init__       > [contacts-77f6b5d6f4-w8kh7 contacts]     self.setup(cfg)       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/site-packages/gunicorn/glogging.py"", line 262, in setup       > [contacts-77f6b5d6f4-w8kh7 contacts]     fileConfig(cfg.logconfig, defaults=defaults,       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/logging/config.py"", line 83, in fileConfig       > [contacts-77f6b5d6f4-w8kh7 contacts]     formatters = _create_formatters(cp)       > [contacts-77f6b5d6f4-w8kh7 contacts]                  ^^^^^^^^^^^^^^^^^^^^^^       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/logging/config.py"", line 116, in _create_formatters       > [contacts-77f6b5d6f4-w8kh7 contacts]     flist = cp[""formatters""][""keys""]       > [contacts-77f6b5d6f4-w8kh7 contacts]             ~~^^^^^^^^^^^^^^       > [contacts-77f6b5d6f4-w8kh7 contacts]   File ""/usr/local/lib/python3.12/configparser.py"", line 941, in __getitem__       > [contacts-77f6b5d6f4-w8kh7 contacts]     raise KeyError(key)       > [contacts-77f6b5d6f4-w8kh7 contacts] KeyError: 'formatters' ```  This is caused by a lack of permissions for non-root users on the application and configuration files in `/app`, resulting in the above error (for `logging.conf` being inacessible), as the service containers run as non-root users. I have a fix for this incoming.  ### Related  #517, #2167    ",2024-08-22T12:43:18+00:00,2024-08-31T17:20:10+00:00,2,https://github.com/GoogleCloudPlatform/bank-of-anthos/issues/2193,2194.0,2024-08-31T17:20:09+00:00,https://github.com/GoogleCloudPlatform/bank-of-anthos/pull/2194,0,0,3,3,9,0,0,0,220.61416666666668,,True,False,critical,configuration,"[{""filename"": ""src/accounts/contacts/Dockerfile"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""src/accounts/userservice/Dockerfile"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""src/frontend/Dockerfile"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}]",accounts,False
knative/eventing,8412,Malformed CRD manifest in v1.16.x releases,"**Describe the bug**  The CRD manifests attached to the releases for 1.16.1, 1.16.2 and 1.16.3 have a duplictate `observedGeneration` key in the spec for `jobsinks.sinks.knative.dev`. This leads to problems unmarshalling the yaml file (e.g. when generating jsonnet libs for the CRDs using https://github.com/jsonnet-libs/k8s)    **Expected behavior**  No duplicate keys in the manifest   **To Reproduce**  1. Download [eventing-crds.yaml](https://github.com/knative/eventing/releases/download/knative-v1.16.3/eventing-crds.yaml) from https://github.com/knative/eventing/releases/tag/knative-v1.16.3 2. Validate the file, e.g. with a tool like [yamllint](https://yamllint.readthedocs.io/en/stable/index.html)     ```     yamllint --no-warnings -d relaxed ~/Downloads/eventing-crds.yaml     /Users/me/Downloads/eventing-crds.yaml     1776:17   error    duplication of key ""observedGeneration"" in mapping  (key-duplicates)     ```   **Knative release version**  `>v1.16.0`   **Additional context**  none ",2025-01-16T15:11:40+00:00,2025-01-22T20:08:45+00:00,3,https://github.com/knative/eventing/issues/8412,8423.0,2025-01-22T18:28:11+00:00,https://github.com/knative/eventing/pull/8423,1,0,0,1,0,4,4,0,147.27527777777777,kind/bug,True,False,normal,configuration,"[{""filename"": ""config/core/resources/jobsink.yaml"", ""lines_added"": 0, ""lines_deleted"": 4, ""file_type"": ""config""}]",,False
kubeflow/spark-operator,2018,[BUG] spark.eventLog.enable and spark.eventLog.dir not working,"## Description Please provide a clear and concise description of the issue you are encountering, and a reproduction of your configuration.  If your request is for a new feature, please use the `Feature request` template.  - [ ] ✋ I have searched the open/closed issues and my issue is not listed.  ## Reproduction Code [Required]  ```   hadoopConf:     # EMRFS filesystem     fs.s3.customAWSCredentialsProvider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider     fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem     fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem     fs.s3a.endpoint: s3.us-east-1.amazonaws.com     fs.s3.buffer.dir: /mnt/s3     fs.s3.getObject.initialSocketTimeoutMilliseconds: ""2000""     mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem: ""2""     mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem: ""true""   sparkConf:     # Required for EMR Runtime     spark.driver.extraClassPath: /usr/share/aws/aws-java-sdk-v2/*:/usr/lib/hudi/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*     spark.driver.extraLibraryPath: /usr/share/aws/aws-java-sdk-v2/*:/usr/lib/hudi/*:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native     spark.executor.extraClassPath: /usr/share/aws/aws-java-sdk-v2/*:/usr/lib/hudi/*:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/home/hadoop/extrajars/*     spark.executor.extraLibraryPath: /usr/share/aws/aws-java-sdk-v2/*:/usr/lib/hudi/*:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native     spark.hadoop.hive.metastore.client.factory.class: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory     # History logs     spark.eventLog.dir: s3://abc/def/     spark.eventLog.enable: ""true""  ```  Steps to reproduce the behavior:   ## Expected behavior  It will produce a log file at s3://abc/def/  ## Actual behavior  nothing   ## Environment & Versions  - Spark Operator App version: v1beta2-1.3.8-3.1.1-amzn-4 - Helm Chart Version: spark-operator-7.0.0 - Kubernetes Version: AWS EKS 1.29.0 - Apache Spark version: 3.5.0    ",2024-05-09T10:32:17+00:00,2025-03-05T16:07:05+00:00,5,https://github.com/kubeflow/spark-operator/issues/2018,189.0,,https://github.com/kubeflow/spark-operator/pull/189,1,0,2,3,41,2,3,0,7205.58,lifecycle/stale,True,False,normal,configuration,"[{""filename"": ""Dockerfile"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""entrypoint"", ""lines_added"": 37, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""manifest/spark-operator.yaml"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
kubeflow/spark-operator,2402,Helm Chart - Controller should not be permitted to create Events in all namespaces,"### What happened?  - [X] ✋ I have searched the open/closed issues and my issue is not listed.  https://github.com/kubeflow/spark-operator/blob/b2411033f0fa2d4f004cc3e3b6f904e04061cb24/charts/spark-operator-chart/templates/controller/rbac.yaml defines a ClusterRole that permits creating and updating Events and a ClusterRolebinding to grant it to controller ServiceAccount. This grants the controller permissions in any namespace, not just the controller and job namespaces.    For security auditing purposes the controller should not have any permissions in other namespaces than the controller and job namespaces.     ### Reproduction Code  deploy the helm chart with `values.yaml` ``` spark:   jobNamespaces:   - ns1   - ns2 ```   ### Expected behavior  spark-operator does not have permissions to create events in other namespaces then the controller and job namespaces  ### Actual behavior  the problem is that the controller has these extra permissions, not that the current code uses those extra permissions.   ### Environment & Versions  - Kubernetes Version: does not matter (1.32) - Spark Operator Version: 2.1.0 - Apache Spark Version: does not matter   ### Additional context  _No response_  ### Impacted by this bug?  Give it a 👍 We prioritize the issues with most 👍",2025-01-24T17:49:36+00:00,2025-02-12T09:57:31+00:00,0,https://github.com/kubeflow/spark-operator/issues/2402,2426.0,2025-02-12T09:57:30+00:00,https://github.com/kubeflow/spark-operator/pull/2426,1,0,1,2,8,8,8,0,448.13166666666666,kind/bug,True,False,normal,configuration,"[{""filename"": ""charts/spark-operator-chart/templates/controller/_helpers.tpl"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""charts/spark-operator-chart/templates/controller/rbac.yaml"", ""lines_added"": 0, ""lines_deleted"": 8, ""file_type"": ""config""}]",,False
