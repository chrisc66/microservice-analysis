repo_name,issue_number,issue_title,issue_body,issue_created_at,issue_closed_at,issue_comments_count,issue_url,pr_number,pr_merged_at,pr_url,config_files_changed,app_code_files_changed,other_files_changed,total_files_changed,lines_added,lines_deleted,config_files_lines_changed,app_code_files_lines_changed,resolution_time_hours,labels,has_config_changes,has_code_changes,bug_severity,bug_type,changed_files,services_affected,is_cross_service_bug
microservices-demo/microservices-demo,435,aws-ecs: Setup script fails if STORE_DNS_NAME_HERE not set,"Steps to reproduce: ``` cd deploy/aws-ecs ./setup.sh ```  Expected result: Despite not setting STORE_DNS_NAME_HERE, the script would complete as expected without putting the dns name into a file at the end.  Actual result: ``` STORE_DNS_NAME_HERE: unbound variable ```  This is because the script is run with `set -u`, which makes all references to unassigned variables an error. So the following code (`deploy/aws-ecs/scripts/setup3-showconf.sh:21-24`) does not act as intended: ``` # And store it in a file, if requested. if [ ""x$STORE_DNS_NAME_HERE"" != ""x"" ]; then   echo ""$dns_name"" > $STORE_DNS_NAME_HERE fi ```  Note this is actually benign, since this is the very last thing the script does. But it's still a scary and confusing message.",2016-11-27T06:09:16+00:00,2016-12-22T23:05:00+00:00,0,https://github.com/microservices-demo/microservices-demo/issues/435,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,616.9288888888889,bug;deployment,False,True,normal,networking,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,435,aws-ecs: Setup script fails if STORE_DNS_NAME_HERE not set,"Steps to reproduce: ``` cd deploy/aws-ecs ./setup.sh ```  Expected result: Despite not setting STORE_DNS_NAME_HERE, the script would complete as expected without putting the dns name into a file at the end.  Actual result: ``` STORE_DNS_NAME_HERE: unbound variable ```  This is because the script is run with `set -u`, which makes all references to unassigned variables an error. So the following code (`deploy/aws-ecs/scripts/setup3-showconf.sh:21-24`) does not act as intended: ``` # And store it in a file, if requested. if [ ""x$STORE_DNS_NAME_HERE"" != ""x"" ]; then   echo ""$dns_name"" > $STORE_DNS_NAME_HERE fi ```  Note this is actually benign, since this is the very last thing the script does. But it's still a scary and confusing message.",2016-11-27T06:09:16+00:00,2016-12-22T23:05:00+00:00,0,https://github.com/microservices-demo/microservices-demo/issues/435,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,616.9288888888889,bug;deployment,False,True,normal,networking,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,286,Bugs in deployment scripts for aws-ecs & Kubernetes ,"Hi I'm a big fan of your project and I want to use this opportunity to thank you for sharing it!  I noticed a minor bug in the aws-ecs script: The [setup1-infra.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/scripts/setup1-infra.sh) script doesn't receive the SCOPE_AAS_PROBE_TOKEN from the invoking [setup.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/setup.sh) script. Hence, the services will never show up in Scope. Another thing that I noticed: Your script requires jq. My first deployment attempt got stuck in the middle because I didn't have it installed. It would be very nice if you could add a check for the existence of jq before running any further steps in the script.  Kubernetes deployment: I get stuck at the 'terraform get' step when I try to run your kubernetes deploy script. The [main.tf](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/terraform/main.tf) file refers to a github directory that doesn't exist anymore (According to [this github issue](https://github.com/kubernetes/kubernetes-anywhere/issues/192) they are working on putting it back up). I tried to run it anyway by using the github history, download the [archived files](https://github.com/kubernetes/kubernetes-anywhere/tree/bd6a907d8614c0d3457d29503dc7f407dafd10d3/phase1/aws-ec2-terraform) to my local machine and change the link in main.tf accordingly. However, I'm still running into the following error message: ""Error loading Terraform: Error downloading modules: module ubuntu_ami: Error loading .terraform/modules/bb7........./main.tf: Error reading config for output ami_id: parse error: syntax error"". Do you have any idea how I could fix that? Thanks a lot! ",2016-08-25T15:25:33+00:00,2016-12-16T02:56:08+00:00,4,https://github.com/microservices-demo/microservices-demo/issues/286,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,2863.6575,bug,False,True,normal,configuration,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,286,Bugs in deployment scripts for aws-ecs & Kubernetes ,"Hi I'm a big fan of your project and I want to use this opportunity to thank you for sharing it!  I noticed a minor bug in the aws-ecs script: The [setup1-infra.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/scripts/setup1-infra.sh) script doesn't receive the SCOPE_AAS_PROBE_TOKEN from the invoking [setup.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/setup.sh) script. Hence, the services will never show up in Scope. Another thing that I noticed: Your script requires jq. My first deployment attempt got stuck in the middle because I didn't have it installed. It would be very nice if you could add a check for the existence of jq before running any further steps in the script.  Kubernetes deployment: I get stuck at the 'terraform get' step when I try to run your kubernetes deploy script. The [main.tf](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/terraform/main.tf) file refers to a github directory that doesn't exist anymore (According to [this github issue](https://github.com/kubernetes/kubernetes-anywhere/issues/192) they are working on putting it back up). I tried to run it anyway by using the github history, download the [archived files](https://github.com/kubernetes/kubernetes-anywhere/tree/bd6a907d8614c0d3457d29503dc7f407dafd10d3/phase1/aws-ec2-terraform) to my local machine and change the link in main.tf accordingly. However, I'm still running into the following error message: ""Error loading Terraform: Error downloading modules: module ubuntu_ami: Error loading .terraform/modules/bb7........./main.tf: Error reading config for output ami_id: parse error: syntax error"". Do you have any idea how I could fix that? Thanks a lot! ",2016-08-25T15:25:33+00:00,2016-12-16T02:56:08+00:00,4,https://github.com/microservices-demo/microservices-demo/issues/286,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,2863.6575,bug,False,True,normal,configuration,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,435,aws-ecs: Setup script fails if STORE_DNS_NAME_HERE not set,"Steps to reproduce: ``` cd deploy/aws-ecs ./setup.sh ```  Expected result: Despite not setting STORE_DNS_NAME_HERE, the script would complete as expected without putting the dns name into a file at the end.  Actual result: ``` STORE_DNS_NAME_HERE: unbound variable ```  This is because the script is run with `set -u`, which makes all references to unassigned variables an error. So the following code (`deploy/aws-ecs/scripts/setup3-showconf.sh:21-24`) does not act as intended: ``` # And store it in a file, if requested. if [ ""x$STORE_DNS_NAME_HERE"" != ""x"" ]; then   echo ""$dns_name"" > $STORE_DNS_NAME_HERE fi ```  Note this is actually benign, since this is the very last thing the script does. But it's still a scary and confusing message.",2016-11-27T06:09:16+00:00,2016-12-22T23:05:00+00:00,0,https://github.com/microservices-demo/microservices-demo/issues/435,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,616.9288888888889,bug;deployment,False,True,normal,networking,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
microservices-demo/microservices-demo,286,Bugs in deployment scripts for aws-ecs & Kubernetes ,"Hi I'm a big fan of your project and I want to use this opportunity to thank you for sharing it!  I noticed a minor bug in the aws-ecs script: The [setup1-infra.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/scripts/setup1-infra.sh) script doesn't receive the SCOPE_AAS_PROBE_TOKEN from the invoking [setup.sh](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/aws-ecs/setup.sh) script. Hence, the services will never show up in Scope. Another thing that I noticed: Your script requires jq. My first deployment attempt got stuck in the middle because I didn't have it installed. It would be very nice if you could add a check for the existence of jq before running any further steps in the script.  Kubernetes deployment: I get stuck at the 'terraform get' step when I try to run your kubernetes deploy script. The [main.tf](https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/terraform/main.tf) file refers to a github directory that doesn't exist anymore (According to [this github issue](https://github.com/kubernetes/kubernetes-anywhere/issues/192) they are working on putting it back up). I tried to run it anyway by using the github history, download the [archived files](https://github.com/kubernetes/kubernetes-anywhere/tree/bd6a907d8614c0d3457d29503dc7f407dafd10d3/phase1/aws-ec2-terraform) to my local machine and change the link in main.tf accordingly. However, I'm still running into the following error message: ""Error loading Terraform: Error downloading modules: module ubuntu_ami: Error loading .terraform/modules/bb7........./main.tf: Error reading config for output ami_id: parse error: syntax error"". Do you have any idea how I could fix that? Thanks a lot! ",2016-08-25T15:25:33+00:00,2016-12-16T02:56:08+00:00,4,https://github.com/microservices-demo/microservices-demo/issues/286,476.0,2016-12-22T23:05:00+00:00,https://github.com/microservices-demo/microservices-demo/pull/476,0,0,2,2,19,5,0,0,2863.6575,bug,False,True,normal,configuration,"[{""filename"": ""deploy/aws-ecs/scripts/setup3-showconf.sh"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""deploy/aws-ecs/setup.sh"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""other""}]",,False
nestjs/nest,14803,fix(core): infinite loop on broken circular reference,"## PR Checklist Please check if your PR fulfills the following requirements:  - [x] The commit message follows our guidelines: https://github.com/nestjs/nest/blob/master/CONTRIBUTING.md - [ ] Tests for the changes have been added (for bug fixes / features) - [ ] Docs have been added / updated (for bug fixes / features)   ## PR Type What kind of change does this PR introduce?  <!-- Please check the one that applies to this PR using ""x"". --> - [x] Bugfix - [ ] Feature - [ ] Code style update (formatting, local variables) - [ ] Refactoring (no functional changes, no api changes) - [ ] Build related changes - [ ] CI related changes - [ ] Other... Please describe:  ## What is the current behavior? <!-- Please describe the current behavior that you are modifying, or link to a relevant issue. -->  Issue Number: #14799   ## What is the new behavior?   ## Does this PR introduce a breaking change? - [ ] Yes - [x] No  <!-- If this PR contains a breaking change, please describe the impact and migration path for existing applications below. -->   ## Other information",2025-03-19T09:47:32+00:00,2025-03-19T10:17:12+00:00,2,https://github.com/nestjs/nest/pull/14803,14803.0,2025-03-19T10:17:12+00:00,https://github.com/nestjs/nest/pull/14803,0,1,0,1,7,1,0,8,0.4944444444444444,type: bug :sob:;scope: core,False,True,critical,ui,"[{""filename"": ""packages/core/injector/instance-wrapper.ts"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",injector,False
nestjs/nest,14430,Optional Enums with SWC throw validation errors (400),"### Is there an existing issue for this?  - [x] I have searched the existing issues  ### Current behavior  When using SWC as builder, optional enums in NestJS fail validation and throw a 400 error.  If I define a controller like this: ```ts @Get() example(   @Query('test', new ParseEnumPipe(MyEnum, { optional: true })) test?: MyEnum, ) {   return `result:${test}`; } ```  When querying without the `test` query param I get this: ```json {   ""message"": ""Validation failed (enum string is expected)"",   ""error"": ""Bad Request"",   ""statusCode"": 400 } ```  **This issue does not occur when using TypeScript as builder.** It is related to a previous problem reported in [#12680](https://github.com/nestjs/nest/issues/12680), which was partially fixed in [#14181](https://github.com/nestjs/nest/pull/14181). While the fix resolved the 500 error, it now throws a 400 error when using SWC.   ### Minimum reproduction code  https://codesandbox.io/p/devbox/goofy-darwin-lmzv37?file=%2Fsrc%2Fapp.controller.ts  ### Steps to reproduce  - Run the project using SWC as the builder (`npm run start:swc`). - Open a browser on port 300 or send a GET request without the test query parameter or with an empty value. - Observe the 400 error response. - Switch to TypeScript as the builder (`npm run start`) - Send the same request, and note that no error is thrown (200) => `result:undefined` is returned as expected.  ### Expected behavior  Optional enums should not throw a validation error when the value is `undefined`, `null` or not provided in the request payload, regardless of whether SWC or TypeScript is used as the builder.  Therefore in the exemple it must pass `undefined` in the `test` parameter  ### Package  - [ ] I don't know. Or some 3rd-party package - [x] <code>@nestjs/common</code> - [ ] <code>@nestjs/core</code> - [ ] <code>@nestjs/microservices</code> - [ ] <code>@nestjs/platform-express</code> - [ ] <code>@nestjs/platform-fastify</code> - [ ] <code>@nestjs/platform-socket.io</code> - [ ] <code>@nestjs/platform-ws</code> - [ ] <code>@nestjs/testing</code> - [ ] <code>@nestjs/websockets</code> - [ ] Other (see below)  ### Other package  _No response_  ### NestJS version  10.4.15  ### Packages versions  ```json ""dependencies"": {     ""@nestjs/common"": ""^10.4.15"",     ""@nestjs/core"": ""^10.4.15"",     ""@nestjs/platform-express"": ""^10.4.15"",     ""@swc/jest"": ""^0.2.37"",     ""class-transformer"": ""^0.5.1"",     ""class-validator"": ""^0.14.1"",     ""reflect-metadata"": ""^0.2.2"",     ""rxjs"": ""^7.8.1""   }, ```   ### Node.js version  20.12.0  ### In which operating systems have you tested?  - [ ] macOS - [x] Windows - [x] Linux  ### Other  _No response_",2025-01-15T11:26:29+00:00,2025-02-06T13:31:27+00:00,3,https://github.com/nestjs/nest/issues/14430,14579.0,2025-02-10T13:12:59+00:00,https://github.com/nestjs/nest/pull/14579,0,1,0,1,9,1,0,10,625.775,needs triage,False,True,normal,configuration,"[{""filename"": ""packages/common/pipes/validation.pipe.ts"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19405,[3.5] Fix a performance regression due to uncertain compaction sleep interval,"The compaction behavior [here](https://github.com/etcd-io/etcd/blob/v3.5.16/server/mvcc/kvstore_compaction.go#L93) is changed in commit [02635](https://github.com/etcd-io/etcd/commit/0263597ba84f65047948141696be926e53aa2429) and introduces a latency issue. To be more speicific, the `ticker.C`  acts as a fixed timer that triggers every 10ms, regardless of how long each batch of compaction takes. This means that if a previous compaction batch takes longer than 10ms, the next batch starts immediately, making compaction a blocking operation for etcd.  To fix the issue, this commit revert the compaction to the previous behavior which ensures a 10ms delay between each batch of compaction, allowing other read and write operations to proceed smoothly.  issue https://github.com/etcd-io/etcd/issues/19406  Please read https://github.com/etcd-io/etcd/blob/main/CONTRIBUTING.md#contribution-flow. ",2025-02-12T22:39:46+00:00,2025-02-13T14:13:25+00:00,13,https://github.com/etcd-io/etcd/pull/19405,19405.0,2025-02-13T14:13:25+00:00,https://github.com/etcd-io/etcd/pull/19405,0,1,0,1,1,3,0,4,15.560833333333331,ok-to-test;size/XS;approved,False,True,normal,performance,"[{""filename"": ""server/mvcc/kvstore_compaction.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",server,False
nestjs/nest,14579,fix(common): revert to original value (swc builders regression),"## PR Checklist Please check if your PR fulfills the following requirements:  - [x] The commit message follows our guidelines: https://github.com/nestjs/nest/blob/master/CONTRIBUTING.md - [x] Tests for the changes have been added (for bug fixes / features) - [ ] Docs have been added / updated (for bug fixes / features)   ## PR Type What kind of change does this PR introduce?  <!-- Please check the one that applies to this PR using ""x"". --> - [x] Bugfix - [ ] Feature - [ ] Code style update (formatting, local variables) - [ ] Refactoring (no functional changes, no api changes) - [ ] Build related changes - [ ] CI related changes - [ ] Other... Please describe:  ## What is the current behavior? <!-- Please describe the current behavior that you are modifying, or link to a relevant issue. -->  Issue Number: #14430   ## What is the new behavior?   ## Does this PR introduce a breaking change? - [ ] Yes - [ ] No  <!-- If this PR contains a breaking change, please describe the impact and migration path for existing applications below. -->   ## Other information",2025-02-06T13:31:18+00:00,2025-02-10T13:12:59+00:00,0,https://github.com/nestjs/nest/pull/14579,14579.0,2025-02-10T13:12:59+00:00,https://github.com/nestjs/nest/pull/14579,0,1,0,1,9,1,0,10,95.69472222222224,type: bug :sob:;scope: common,False,True,normal,ui,"[{""filename"": ""packages/common/pipes/validation.pipe.ts"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19411,[release-3.6] Fix a performance regression due to uncertain compaction sleep interval,This is an automated cherry-pick of #19410  /assign ahrtr,2025-02-13T10:37:44+00:00,2025-02-13T12:26:10+00:00,5,https://github.com/etcd-io/etcd/pull/19411,19411.0,2025-02-13T12:26:10+00:00,https://github.com/etcd-io/etcd/pull/19411,0,1,0,1,1,3,0,4,1.8072222222222225,stage/merge-when-tests-green;ok-to-test;size/XS;approved,False,True,normal,performance,"[{""filename"": ""server/storage/mvcc/kvstore_compaction.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",server,False
nestjs/nest,14455,"regression: moduleIdGeneratorAlgorithm default value changed, and it can't be controlled in tests","### Did you read the migration guide?  - [x] I have read the whole migration guide  ### Is there an existing issue that is already proposing this?  - [x] I have searched the existing issues  ### Potential Commit/PR that introduced the regression  PR #13336  ### NestJS version  11.0.1  ### Describe the regression  Before v11 the default behavior of moduleIdGeneratorAlgorithm (which didn't really exist) was the hash one.    V11 introduced a reference algorithm and changed it to be the default. This is a regression and is not documented in the non-existing release notes of v11.   Even if we decide that the default value should be changed back, there is currently no way of controlling the value of moduleIdGeneratorAlgorithm in TestingModuleBuilder.   Please change the default value and allow to change the algorithm in TestingModuleBuilder.  ### Minimum reproduction code  _No response_  ### Input code  ```ts  ```   ### Expected behavior  change the default algorithm back to hash and allow controlling the algorithm in TestingModuleBuilder  ### Other  _No response_",2025-01-19T21:24:20+00:00,2025-01-20T07:44:41+00:00,3,https://github.com/nestjs/nest/issues/14455,14456.0,2025-01-20T07:49:05+00:00,https://github.com/nestjs/nest/pull/14456,0,2,0,2,13,3,0,16,10.4125,type: bug :sob:;needs triage,False,True,normal,ui,"[{""filename"": ""packages/testing/test.ts"", ""lines_added"": 10, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""packages/testing/testing-module.builder.ts"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19410,Fix a performance regression due to uncertain compaction sleep interval,Fix https://github.com/etcd-io/etcd/issues/19406  cc @fuweid @serathius @chaochn47 @miancheng7 ,2025-02-13T09:41:03+00:00,2025-02-13T10:36:58+00:00,4,https://github.com/etcd-io/etcd/pull/19410,19410.0,2025-02-13T10:36:58+00:00,https://github.com/etcd-io/etcd/pull/19410,0,1,0,1,1,3,0,4,0.9319444444444444,size/XS;approved,False,True,normal,performance,"[{""filename"": ""server/storage/mvcc/kvstore_compaction.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",server,False
etcd-io/etcd,19390,[release-3.6] Fix a log formatting bug in the e2e downgrade tests,This is an automated cherry-pick of #19367  /assign ahrtr,2025-02-11T21:33:12+00:00,2025-02-12T07:49:28+00:00,5,https://github.com/etcd-io/etcd/pull/19390,19390.0,2025-02-12T07:49:28+00:00,https://github.com/etcd-io/etcd/pull/19390,0,2,0,2,2,3,0,5,10.27111111111111,area/testing;ok-to-test;size/XS;approved,False,True,normal,functional,"[{""filename"": ""tests/e2e/cluster_downgrade_test.go"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/framework/e2e/downgrade.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19367,Fix a log formatting bug in the e2e downgrade tests,"Please read https://github.com/etcd-io/etcd/blob/main/CONTRIBUTING.md#contribution-flow.  The log message was of the following format before fixing ``` cluster_downgrade_test.go:163: Elect members for operations         %!(EXTRA zapcore.Field={members 1 0  [2 1]}) ```  After ```  cluster_downgrade_test.go:163: Elect members for operations zapcore.Field{Key:""members"", Type:0x1, Integer:0, String:"""", Interface:zap.ints{1}} ```",2025-02-09T22:32:25+00:00,2025-02-10T11:51:41+00:00,6,https://github.com/etcd-io/etcd/pull/19367,19367.0,2025-02-10T11:51:41+00:00,https://github.com/etcd-io/etcd/pull/19367,0,2,0,2,2,3,0,5,13.321111111111112,area/testing;size/XS;approved,False,True,normal,ui,"[{""filename"": ""tests/e2e/cluster_downgrade_test.go"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/framework/e2e/downgrade.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
EventStore/EventStore,4765,[release/v24.10] [KDB-615] Fix bug in ticks conversion: Stopwatch.Frequency is not necessarily a multiple of TimeSpan.TicksPerSecond,Cherry picked from https://github.com/EventStore/EventStore/pull/4764,2025-01-25T07:56:28+00:00,2025-01-25T10:34:19+00:00,1,https://github.com/EventStore/EventStore/pull/4765,4765.0,2025-01-25T10:34:19+00:00,https://github.com/EventStore/EventStore/pull/4765,0,2,0,2,22,10,0,32,2.6308333333333334,,False,True,normal,functional,"[{""filename"": ""src/EventStore.Core.XUnit.Tests/Metrics/InstantTests.cs"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Core/Time/Instant.cs"", ""lines_added"": 4, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",,False
EventStore/EventStore,4764,[KDB-615] Fix bug in ticks conversion: Stopwatch.Frequency is not necessarily a multiple of TimeSpan.TicksPerSecond,Fixed: Fixed bug in ticks conversion: Stopwatch.Frequency is not necessarily a multiple of TimeSpan.TicksPerSecond  Fixes: https://github.com/EventStore/EventStore/issues/4758 Fixes: https://github.com/EventStore/EventStore/issues/4726,2025-01-24T07:37:28+00:00,2025-01-25T07:56:10+00:00,1,https://github.com/EventStore/EventStore/pull/4764,4764.0,2025-01-25T07:56:10+00:00,https://github.com/EventStore/EventStore/pull/4764,0,2,0,2,22,10,0,32,24.311666666666667,cherry-pick:release/v24.10,False,True,normal,functional,"[{""filename"": ""src/EventStore.Core.XUnit.Tests/Metrics/InstantTests.cs"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Core/Time/Instant.cs"", ""lines_added"": 4, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",,False
EventStore/EventStore,4683,Calculate disk stats on linux for correct mount point instead of `/`,Fixed: Disk stats bug introduced in 24.6.0. On Linux the disk usage/capacity were showing the values for the disk mounted at `/` even if the database was on a different disk  Fixes #4680,2024-12-16T12:39:20+00:00,2025-01-22T12:42:28+00:00,2,https://github.com/EventStore/EventStore/pull/4683,4683.0,2025-01-22T12:42:27+00:00,https://github.com/EventStore/EventStore/pull/4683,0,1,0,1,27,6,0,33,888.0519444444444,bug;cherry-pick:release/v24.10,False,True,normal,database,"[{""filename"": ""src/EventStore.SystemRuntime/Diagnostics/DriveStats.cs"", ""lines_added"": 27, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19258,[3.5] Fix race condition (also a regression of the PR 19139),Backport https://github.com/etcd-io/etcd/pull/19221  Fix https://github.com/etcd-io/etcd/issues/19172  cc @fuweid @serathius  ,2025-01-22T15:11:51+00:00,2025-01-22T15:41:41+00:00,1,https://github.com/etcd-io/etcd/pull/19258,19258.0,2025-01-22T15:41:41+00:00,https://github.com/etcd-io/etcd/pull/19258,0,2,0,2,67,13,0,80,0.4972222222222222,size/M;approved,False,True,normal,functional,"[{""filename"": ""server/embed/etcd.go"", ""lines_added"": 39, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""server/embed/serve.go"", ""lines_added"": 28, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",server,False
EventStore/EventStore,4657,[KDB-528] Log SLOW QUEUE messages by default again,"Fixed: 'slow queue' messages are now enabled by default again  When EventStore:Metrics:Queues:Processing is false (which is it by default), the tracker which is used was not returning the current time when recording. This caused the queues to think that no time had passed while processing the message and prevented slow queue messages from being logged.  Slow BUS messages were still logged",2024-11-28T06:57:21+00:00,2024-11-28T13:22:44+00:00,1,https://github.com/EventStore/EventStore/pull/4657,4657.0,2024-11-28T13:22:44+00:00,https://github.com/EventStore/EventStore/pull/4657,0,2,0,2,16,1,0,17,6.423055555555556,bug;cherry-pick:release/v24.10,False,True,normal,performance,"[{""filename"": ""src/EventStore.Core.XUnit.Tests/Metrics/QueueProcessingTrackerTests.cs"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Core/Metrics/QueueProcessingTracker.cs"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19221,Fix race condition (also a regression of the PR 19139),"Fix https://github.com/etcd-io/etcd/issues/19172  Please review this PR commit by commit.  Two high level thoughts, - There are multiple levels of goroutines. The grandparent ( [StartEtcd](https://github.com/etcd-io/etcd/blob/5d47d7fc2e6bed14c6d3cf71ed3e6a939dd76bd5/server/embed/etcd.go#L99) ) creates multiple child goroutines ( [client listeners](https://github.com/etcd-io/etcd/blob/5d47d7fc2e6bed14c6d3cf71ed3e6a939dd76bd5/server/embed/etcd.go#L776-L780), peer listeners and metrics listeners). The [client listeners](https://github.com/etcd-io/etcd/blob/5d47d7fc2e6bed14c6d3cf71ed3e6a939dd76bd5/server/embed/etcd.go#L776-L780) creates some grandson goroutines (see the first commit).  Each one should only manage their immediate children. - For `sync.WaitGroup`, we should always call `wg.Add` and `wg.Wait` in the same goroutine.    cc @serathius @fuweid @ivanvc @jmhbnz  @joshuazh-x ",2025-01-17T15:17:26+00:00,2025-01-22T14:44:21+00:00,13,https://github.com/etcd-io/etcd/pull/19221,19221.0,2025-01-22T14:44:21+00:00,https://github.com/etcd-io/etcd/pull/19221,0,2,0,2,68,14,0,82,119.4486111111111,area/testing;backport/v3.5;size/M;approved,False,True,normal,functional,"[{""filename"": ""server/embed/etcd.go"", ""lines_added"": 39, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""server/embed/serve.go"", ""lines_added"": 29, ""lines_deleted"": 7, ""file_type"": ""app_code""}]",server,False
EventStore/EventStore,4669,[KDB-554] Get up to 100 entitlements,Fixed: Bug where only first 10 license entitlements were retrieved  https://keygen.sh/docs/api/entitlements/#entitlements-list,2024-12-09T20:01:06+00:00,2024-12-10T09:05:40+00:00,1,https://github.com/EventStore/EventStore/pull/4669,4669.0,2024-12-10T09:05:40+00:00,https://github.com/EventStore/EventStore/pull/4669,0,2,0,2,2,1,0,3,13.07611111111111,bug;cherry-pick:release/v24.10,False,True,normal,functional,"[{""filename"": ""src/EventStore.Licensing.Tests/Keygen/KeygenSimulator.Should.cs"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Licensing/Keygen/KeygenClient.cs"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19172,Race condition when closing the embedded etcd,### Which Github Action / Prow Jobs are flaking?  https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/etcd-io_etcd/19168/pull-etcd-integration-4-cpu-amd64/1878113519820345344 ``` === FAIL: integration/embed TestEmbedEtcd (2.17s) ================== WARNING: DATA RACE Write at 0x00c000346760 by goroutine 132:   runtime.racewrite()       <autogenerated>:1 +0x1e   go.etcd.io/etcd/server/v3/embed.(*Etcd).Close()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:460 +0xddc   go.etcd.io/etcd/tests/v3/integration/embed_test.TestEmbedEtcd()       /home/prow/go/src/github.com/etcd-io/etcd/tests/integration/embed/embed_test.go:120 +0x14d1   testing.tRunner()       /usr/local/go/src/testing/testing.go:[169](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/etcd-io_etcd/19168/pull-etcd-integration-4-cpu-amd64/1878113519820345344#1:build-log.txt%3A169)0 +0x226   testing.(*T).Run.gowrap1()       /usr/local/go/src/testing/testing.go:1743 +0x44 Previous read at 0x00c000346760 by goroutine 282:   runtime.raceread()       <autogenerated>:1 +0x1e   go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:875 +0x6a   go.etcd.io/etcd/server/v3/embed.(*Etcd).servePeers.func3() 25 skipped lines...   testing.tRunner()       /usr/local/go/src/testing/testing.go:1690 +0x226   testing.(*T).Run.gowrap1()       /usr/local/go/src/testing/testing.go:[174](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/etcd-io_etcd/19168/pull-etcd-integration-4-cpu-amd64/1878113519820345344#1:build-log.txt%3A174)3 +0x44 ==================     testing.go:1399: race detected during execution of test ```  ### Which tests are flaking?  .  ### Github Action / Prow Job link  _No response_  ### Reason for failure (if possible)  _No response_  ### Anything else we need to know?  _No response_,2025-01-11T16:29:51+00:00,2025-01-22T14:44:22+00:00,3,https://github.com/etcd-io/etcd/issues/19172,19231.0,,https://github.com/etcd-io/etcd/pull/19231,0,1,0,1,0,5,0,5,262.24194444444447,type/bug;backport/v3.5,False,True,normal,database,"[{""filename"": ""server/embed/etcd.go"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",server,False
EventStore/EventStore,4670,[release/v24.10] [KDB-554] Get up to 100 entitlements,Cherry picked from https://github.com/EventStore/EventStore/pull/4669,2024-12-10T09:06:01+00:00,2024-12-10T09:47:18+00:00,1,https://github.com/EventStore/EventStore/pull/4670,4670.0,2024-12-10T09:47:18+00:00,https://github.com/EventStore/EventStore/pull/4670,0,2,0,2,2,1,0,3,0.6880555555555555,bug,False,True,normal,functional,"[{""filename"": ""src/EventStore.Licensing.Tests/Keygen/KeygenSimulator.Should.cs"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Licensing/Keygen/KeygenClient.cs"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
EventStore/EventStore,4660,[release/v24.10] [KDB-528] Log SLOW QUEUE messages by default again,Cherry picked from https://github.com/EventStore/EventStore/pull/4657,2024-11-28T13:23:05+00:00,2024-11-28T16:13:28+00:00,1,https://github.com/EventStore/EventStore/pull/4660,4660.0,2024-11-28T16:13:28+00:00,https://github.com/EventStore/EventStore/pull/4660,0,2,0,2,16,1,0,17,2.839722222222222,bug,False,True,normal,performance,"[{""filename"": ""src/EventStore.Core.XUnit.Tests/Metrics/QueueProcessingTrackerTests.cs"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Core/Metrics/QueueProcessingTracker.cs"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,17514,"When process messages to send to peers, leader should skip checking message type when detect removed peer.","### Bug report criteria  - [X] This bug report is not security related, security issues should be disclosed privately via security@etcd.io. - [X] This is not a support request or question, support requests or questions should be raised in the etcd [discussion forums](https://github.com/etcd-io/etcd/discussions). - [X] You have read the etcd [bug reporting guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md). - [X] Existing open issues along with etcd [frequently asked questions](https://etcd.io/docs/latest/faq) have been checked and this is not a duplicate.  ### What happened?  From reading code, in `processMessages`, if a peer has been removed, the leader should skip checking the message type. https://github.com/etcd-io/etcd/blob/e68afe7eb7ce3e5f83deb2a8ad58faa3b35494b9/server/etcdserver/raft.go#L345-L350  Otherwise, when the message type is `raftpb.MsgHeartbeat`, registering `id = 0` in `r.td` is unintended, because `r.td` should only observe peer's ID greater than zero. This might lead to incorrectly increase `heartbeatSendFailures`. https://github.com/etcd-io/etcd/blob/e68afe7eb7ce3e5f83deb2a8ad58faa3b35494b9/server/etcdserver/raft.go#L372-L375   ### What did you expect to happen?  `r.td` contains peer's ID greater than zero.  ### How can we reproduce it (as minimally and precisely as possible)?  It might be hard to reproduce. It can happen when leader apply config change remove node and then process message detects node removed.  ### Anything else we need to know?  _No response_  ### Etcd version (please run commands below)  main branch  ### Etcd configuration (command line flags or environment variables)  <details>  # paste your configuration here  </details>   ### Etcd debug information (please run commands below, feel free to obfuscate the IP address or FQDN in the output)  <details>  ```console $ etcdctl member list -w table # paste output here  $ etcdctl --endpoints=<member list> endpoint status -w table # paste output here ```  </details>   ### Relevant log output  _No response_",2024-03-02T08:46:01+00:00,2025-01-16T19:07:33+00:00,4,https://github.com/etcd-io/etcd/issues/17514,17518.0,2024-03-05T09:30:42+00:00,https://github.com/etcd-io/etcd/pull/17518,0,1,0,1,1,0,0,1,72.74472222222222,type/bug,False,True,normal,configuration,"[{""filename"": ""server/etcdserver/raft.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",server,False
etcd-io/etcd,19193,server/embed: data-race about errHandler,"### Bug report criteria  - [x] This bug report is not security related, security issues should be disclosed privately via [etcd maintainers](mailto:etcd-maintainers@googlegroups.com). - [x] This is not a support request or question, support requests or questions should be raised in the etcd [discussion forums](https://github.com/etcd-io/etcd/discussions). - [x] You have read the etcd [bug reporting guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md). - [x] Existing open issues along with etcd [frequently asked questions](https://etcd.io/docs/latest/faq) have been checked and this is not a duplicate.  ### What happened?  Saw one data-race issue in CI https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/etcd-io_etcd/19188/pull-etcd-integration-4-cpu-arm64/1879200899084062720  I think it's related to https://github.com/etcd-io/etcd/pull/19139. We should add `wg.Add(1)` before creating new goroutine.  ``` {Failed  === RUN   TestEmbedEtcd ================== WARNING: DATA RACE Write at 0x00c0004ec768 by goroutine 124:   runtime.racewrite()       <autogenerated>:1 +0x10   go.etcd.io/etcd/server/v3/embed.(*Etcd).Close()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:460 +0x9a4   go.etcd.io/etcd/tests/v3/integration/embed_test.TestEmbedEtcd()       /home/prow/go/src/github.com/etcd-io/etcd/tests/integration/embed/embed_test.go:120 +0xf28   testing.tRunner()       /usr/local/go/src/testing/testing.go:1690 +0x184   testing.(*T).Run.gowrap1()       /usr/local/go/src/testing/testing.go:1743 +0x40  Previous read at 0x00c0004ec768 by goroutine 223:   runtime.raceread()       <autogenerated>:1 +0x10   go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:875 +0x48   go.etcd.io/etcd/server/v3/embed.(*Etcd).servePeers.func3()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:616 +0x148   go.etcd.io/etcd/server/v3/embed.(*Etcd).servePeers.gowrap2()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:617 +0x44  Goroutine 124 (running) created at:   testing.(*T).Run()       /usr/local/go/src/testing/testing.go:1743 +0x5e0   testing.runTests.func1()       /usr/local/go/src/testing/testing.go:2168 +0x80   testing.tRunner()       /usr/local/go/src/testing/testing.go:1690 +0x184   testing.runTests()       /usr/local/go/src/testing/testing.go:2166 +0x6e0   testing.(*M).Run()       /usr/local/go/src/testing/testing.go:2034 +0xb74   main.main()       _testmain.go:53 +0x110  Goroutine 223 (finished) created at:   go.etcd.io/etcd/server/v3/embed.(*Etcd).servePeers()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:610 +0x70c   go.etcd.io/etcd/server/v3/embed.StartEtcd()       /home/prow/go/src/github.com/etcd-io/etcd/server/embed/etcd.go:274 +0x1bac   go.etcd.io/etcd/tests/v3/integration/embed_test.TestEmbedEtcd()       /home/prow/go/src/github.com/etcd-io/etcd/tests/integration/embed/embed_test.go:97 +0xb3c   testing.tRunner()       /usr/local/go/src/testing/testing.go:1690 +0x184   testing.(*T).Run.gowrap1()       /usr/local/go/src/testing/testing.go:1743 +0x40 ==================     testing.go:1399: race detected during execution of test --- FAIL: TestEmbedEtcd (1.49s) } ```  ### What did you expect to happen?  No data race  ### How can we reproduce it (as minimally and precisely as possible)?  good luck  ```bash $ cd tests/integration/embed $ go test -v -race '-run=^TestEmbedEtcd$' -failfast -count=100 ./ ```  ### Anything else we need to know?  _No response_  ### Etcd version (please run commands below)  <details>  ```console $ etcd --version # paste output here  $ etcdctl version # paste output here ```  </details>   ### Etcd configuration (command line flags or environment variables)  <details>  # paste your configuration here  </details>   ### Etcd debug information (please run commands below, feel free to obfuscate the IP address or FQDN in the output)  <details>  ```console $ etcdctl member list -w table # paste output here  $ etcdctl --endpoints=<member list> endpoint status -w table # paste output here ```  </details>   ### Relevant log output  ```Shell  ```",2025-01-14T16:44:14+00:00,2025-01-14T16:49:20+00:00,1,https://github.com/etcd-io/etcd/issues/19193,2607.0,2015-03-31T03:31:00+00:00,https://github.com/etcd-io/etcd/pull/2607,0,2,0,2,22,3,0,25,-85861.22055555556,type/bug,False,True,normal,configuration,"[{""filename"": ""wal/encoder.go"", ""lines_added"": 21, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""wal/wal.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
EventStore/EventStore,4549,Add error to /license endpoint,"Added: license error to /license endpoint  Endpoint can now return ``` 200 with license (as before) 404 with no reason - indicates no error, because no license key was provided 404 with a reason - which is the error ```",2024-10-18T13:02:48+00:00,2024-10-18T16:41:25+00:00,1,https://github.com/EventStore/EventStore/pull/4549,4549.0,2024-10-18T16:41:25+00:00,https://github.com/EventStore/EventStore/pull/4549,0,2,0,2,30,8,0,38,3.643611111111111,cherry-pick:master,False,True,normal,functional,"[{""filename"": ""src/EventStore.Licensing.Tests/LicensingPluginTests.cs"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""src/EventStore.Licensing/LicensingPlugin.cs"", ""lines_added"": 29, ""lines_deleted"": 7, ""file_type"": ""app_code""}]",,False
etcd-io/etcd,19150,Backport https://github.com/etcd-io/etcd/pull/19139 to 3.5 and 3.4,"### Bug report criteria  - [ ] This bug report is not security related, security issues should be disclosed privately via [etcd maintainers](mailto:etcd-maintainers@googlegroups.com). - [ ] This is not a support request or question, support requests or questions should be raised in the etcd [discussion forums](https://github.com/etcd-io/etcd/discussions). - [ ] You have read the etcd [bug reporting guidelines](https://github.com/etcd-io/etcd/blob/main/Documentation/contributor-guide/reporting_bugs.md). - [ ] Existing open issues along with etcd [frequently asked questions](https://etcd.io/docs/latest/faq) have been checked and this is not a duplicate.  ### What happened?  See https://github.com/etcd-io/etcd/pull/19139  ### What did you expect to happen?  .  ### How can we reproduce it (as minimally and precisely as possible)?  .  ### Anything else we need to know?  _No response_  ### Etcd version (please run commands below)  <details>  ```console $ etcd --version # paste output here  $ etcdctl version # paste output here ```  </details>   ### Etcd configuration (command line flags or environment variables)  <details>  # paste your configuration here  </details>   ### Etcd debug information (please run commands below, feel free to obfuscate the IP address or FQDN in the output)  <details>  ```console $ etcdctl member list -w table # paste output here  $ etcdctl --endpoints=<member list> endpoint status -w table # paste output here ```  </details>   ### Relevant log output  _No response_",2025-01-09T13:51:05+00:00,2025-01-13T09:23:05+00:00,5,https://github.com/etcd-io/etcd/issues/19150,19167.0,2025-01-13T08:33:50+00:00,https://github.com/etcd-io/etcd/pull/19167,0,4,0,4,82,5,0,87,90.7125,type/bug,False,True,normal,configuration,"[{""filename"": ""server/embed/etcd.go"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""server/embed/serve.go"", ""lines_added"": 18, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""server/etcdserver/server.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/embed/embed_test.go"", ""lines_added"": 57, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",server,False
etcd-io/etcd,19069,Update the allowedErrors in TestNoErrorLogsDuringNormalOperations,"Revert https://github.com/etcd-io/etcd/pull/19060.  After https://github.com/etcd-io/etcd/pull/19068 is merged, we shouldn't see the following error anymore,  ``` Messages:   	error level log message found: {""level"":""error"",""ts"":""2024-11-16T20:58:00.500222Z"",""caller"":""version/monitor.go:120"",""msg"":""failed to update storage version"",""cluster-version"":""3.6.0"",""error"":""cannot detect storage schema version: missing confstate information"",""stacktrace"":""go.etcd.io/etcd/server/v3/etcdserver/version.(*Monitor).UpdateStorageVersionIfNeeded\\n\\tgo.etcd.io/etcd/server/v3/etcdserver/version/monitor.go:120\\ngo.etcd.io/etcd/server/v3/etcdserver.(*EtcdServer).monitorStorageVersion\\n\\tgo.etcd.io/etcd/server/v3/etcdserver/server.go:2286\\ngo.etcd.io/etcd/server/v3/etcdserver.(*EtcdServer).GoAttach.func1\\n\\tgo.etcd.io/etcd/server/v3/etcdserver/server.go:2467""} ``` ",2024-12-16T14:03:00+00:00,2024-12-16T14:55:33+00:00,5,https://github.com/etcd-io/etcd/pull/19069,19069.0,2024-12-16T14:55:33+00:00,https://github.com/etcd-io/etcd/pull/19069,0,1,0,1,0,15,0,15,0.8758333333333334,area/testing;size/S;approved,False,True,normal,functional,"[{""filename"": ""tests/e2e/logging_test.go"", ""lines_added"": 0, ""lines_deleted"": 15, ""file_type"": ""app_code""}]",,False
zeromicro/go-zero,4686,fix: fix the bug of the numeric/decimal data type in pg,"Issue #4624.  I tried the PostgreSQL DDL and use both numeric and decimal type to generate the model. `balance_test_tab.sql` ```sql CREATE TABLE balance_test_tab (       id bigserial NOT NULL,       balance numeric(78) DEFAULT 0 NOT NULL,       CONSTRAINT balance_test_tab_pk PRIMARY KEY (id) );  CREATE TABLE balance_test_tab (       id bigserial NOT NULL,       balance decimal(78) DEFAULT 0 NOT NULL,       CONSTRAINT balance_test_tab_pk PRIMARY KEY (id) ); ```  ### Output These cases both gave the int64 type to the code. ```go BalanceTestTab struct { 		Id      int64 `db:""id""` 		Balance int64 `db:""balance""` } ```   Otherwise, I use DDL in MySQL below: ```sql CREATE TABLE balance_test_tab (   id BIGINT AUTO_INCREMENT NOT NULL,   balance DECIMAL(78,0) DEFAULT 0 NOT NULL,   PRIMARY KEY (id) ); ```   ### Output The type was float64. ```go 	BalanceTestTab struct { 		Id      int64   `db:""id""` 		Balance float64 `db:""balance""` 	} ```  ## Key Reason When goctl use datasource to get the columns info, it will transfer the type to mysql type. The `numeric/decimal` was transfer to the bigint type, and it was not reasonable.  `tools/goctl/model/sql/model/postgresqlmodel.go` ```go var p2m = map[string]string{ 	""int8"":        ""bigint"", 	""numeric"":     ""double"", // it was bigint, after I changed it  	""decimal"":     ""double"", // add it to support the type 	""float8"":      ""double"", 	""float4"":      ""float"", 	""int2"":        ""smallint"", 	""int4"":        ""integer"", 	""timestamptz"": ""timestamp"", }  func (m *PostgreSqlModel) convertPostgreSqlTypeIntoMysqlType(in string) string { 	r, ok := p2m[strings.ToLower(in)] 	if ok { 		return r 	}  	return in } ```  ",2025-03-05T07:45:32+00:00,2025-03-07T11:12:19+00:00,1,https://github.com/zeromicro/go-zero/pull/4686,4686.0,2025-03-07T11:12:19+00:00,https://github.com/zeromicro/go-zero/pull/4686,0,1,0,1,2,1,0,3,51.44638888888889,,False,True,normal,database,"[{""filename"": ""tools/goctl/model/sql/model/postgresqlmodel.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,337,Fix handler to use http.Request context to react to client connection,"When client connection closes, the server would still attempt process the request to completion. Changes allow server to error and exit early",2021-07-13T14:22:31+00:00,2021-07-13T15:58:15+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/337,337.0,2021-07-13T15:58:15+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/337,0,3,0,3,36,7,0,43,1.5955555555555556,bug;backport/0.1;backport/0.2,False,True,normal,networking,"[{""filename"": ""api/client.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""api/task.go"", ""lines_added"": 1, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""api/task_test.go"", ""lines_added"": 34, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",api,False
zeromicro/go-zero,4398,The AddGlobalFields function fails after using the integrated zap component zapx as a writer,"The AddGlobalFields function failed to set the global value after using zero contrib This is my code, written based on example. (If zapx is not used as a writer, the global value will be corrected) ![go-zerolog](https://github.com/user-attachments/assets/f34dde91-c1d2-429a-ae34-b05ac1229d09) The obtained result does not have a global value The case of zapx does not include AddGlobalFields. How can I set the global feild using zapx？",2024-09-28T07:25:13+00:00,2025-01-31T13:51:39+00:00,0,https://github.com/zeromicro/go-zero/issues/4398,4616.0,,https://github.com/zeromicro/go-zero/pull/4616,0,2,0,2,7,7,0,14,3006.4405555555554,,False,True,normal,functional,"[{""filename"": ""core/logx/logs.go"", ""lines_added"": 7, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""core/logx/writer.go"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
zeromicro/go-zero,4598,add Debugfn and Infofn to logx/logc #4595,,2025-01-23T17:19:44+00:00,2025-01-25T14:21:50+00:00,1,https://github.com/zeromicro/go-zero/pull/4598,4598.0,2025-01-25T14:21:50+00:00,https://github.com/zeromicro/go-zero/pull/4598,0,5,0,5,80,1,0,81,45.035,,False,True,normal,functional,"[{""filename"": ""core/logc/logs.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/logx/logger.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/logx/logs.go"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/logx/logs_test.go"", ""lines_added"": 41, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""core/logx/richlogger.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,322,Fix re-enabled task to trigger,"This PR includes #321 from @lornasong, it has more details around the bug for #320  Fix UpdateTask to enable a disabled task that has never run  Fixes the failing tests that were waiting on the wrong event, and prematurely continuing the test before the catalog changes were realized as Terraform changes.  Closes #320",2021-06-22T22:20:15+00:00,2021-06-24T15:29:24+00:00,1,https://github.com/hashicorp/consul-terraform-sync/pull/322,322.0,2021-06-24T15:29:23+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/322,0,12,2,14,212,46,0,241,41.15222222222222,bug;backport/0.1;backport/0.2,False,True,normal,ui,"[{""filename"": ""api/task.go"", ""lines_added"": 32, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""api/task_test.go"", ""lines_added"": 29, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""api/test.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""driver/terraform.go"", ""lines_added"": 38, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""driver/terraform_test.go"", ""lines_added"": 10, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""e2e/api_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""e2e/command_test.go"", ""lines_added"": 61, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""e2e/condition_test.go"", ""lines_added"": 14, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""e2e/e2e_test.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""e2e/tasks_test.go"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""go.mod"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 2, ""lines_deleted"": 10, ""file_type"": ""other""}, {""filename"": ""mocks/templates/watcher.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""templates/hcat.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",e2e;api,True
zeromicro/go-zero,4450,Bug: httpx.ParseJsonBody don't support bytes field,"**Describe the bug** ```go t.Run(""bytes field"", func(t *testing.T) { 		type v struct { 			Signature []byte `json:""signature,optional""` 		} 		v1 := v{ 			Signature: []byte{0x01, 0xff, 0x00}, 		} 		body, _ := json.Marshal(v1) 		t.Logf(""body:%s"", string(body)) 		r := httptest.NewRequest(http.MethodPost, ""/"", strings.NewReader(string(body))) 		r.Header.Set(ContentType, header.JsonContentType) 		if assert.NoError(t, ParseJsonBody(r, &v1)) { 			assert.Greater(t, len(v1.Signature), 0) 		}  	}) ```  2. The error is     ```        requests_test.go:287: body:{""signature"":""Af8A""}     requests_test.go:290:          	Error Trace:	/Users/devinzeng/go/src/github.com/zeromicro/go-zero/rest/httpx/requests_test.go:290         	Error:      	Received unexpected error:         	            	fullName: `signature`, error: `string: `Af8A`, error: `invalid character 'A' looking for beginning of value``         	Test:       	TestParseJsonBody/bytes_field    ```  **Expected behavior** No error, like json.Unmarshal  ",2024-11-06T11:22:13+00:00,2025-01-22T13:37:37+00:00,2,https://github.com/zeromicro/go-zero/issues/4450,4471.0,2025-01-22T13:35:32+00:00,https://github.com/zeromicro/go-zero/pull/4471,0,2,0,2,29,1,0,30,1850.2219444444445,,False,True,normal,configuration,"[{""filename"": ""core/mapping/unmarshaler.go"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""rest/httpx/requests_test.go"", ""lines_added"": 20, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",core,False
hashicorp/consul-terraform-sync,321,Fix re-enabled task to trigger,"Fixes: https://github.com/hashicorp/consul-terraform-sync/issues/320  Draft PR:   - Depends on related hcat proposal https://github.com/hashicorp/hcat/pull/57 to be okayed  - Failing e2e test requires this hcat update  - Depends on a bug fix in branch `enable-event`. Once that branch is merged into v0.2.0, I'll adjust this PR to merge into v0.2.0 as well.  Fixes an issue where tasks that were originally enabled, then disabled, then re-enabled :) would hang and not trigger even when condition satisfied.  Commits (more details in commit msg):  - Fix the original issue which occurs when a task is re-enabled, it creates another hcat template with the same template content (hcat template content must be unique https://github.com/hashicorp/consul-terraform-sync/pull/167 - thanks @ findkim!). Fix this by only creating a new hcat template only if content is different.  - Fix a new issue that the fix above introduced. New issue occurs when existing (vs. new) hcat templates are forced to re-render when there are no dependency changes (as can happen when a task is enabled), the re-rendering will hang. Fix this by proposing an [hcat change](https://github.com/hashicorp/hcat/pull/57) to return result.Complete == true (which will prevent hanging) when there are no dependency changes",2021-06-19T01:10:58+00:00,2021-06-22T16:10:29+00:00,1,https://github.com/hashicorp/consul-terraform-sync/pull/321,321.0,,https://github.com/hashicorp/consul-terraform-sync/pull/321,0,5,2,7,106,14,0,103,86.99194444444444,bug,False,True,normal,ui,"[{""filename"": ""driver/terraform.go"", ""lines_added"": 23, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""driver/terraform_test.go"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""e2e/command_test.go"", ""lines_added"": 58, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""go.mod"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 2, ""lines_deleted"": 10, ""file_type"": ""other""}, {""filename"": ""mocks/templates/watcher.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""templates/hcat.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,319,Store an event when a task becomes enabled and executes,"When a task is disabled and then enabled via CLI, the task may execute if there are changes. Whenever a task executes, an event should be stored.  When enable CLI was implemented, storing an event on task execution was unintentionally not implemented. This change fixes this and stores an event.  Changes:  - Identifies when a task is executed: when ?run=now i.e. runOp == driver.RunOptionNow  - Stores an event  - Retrieves an error that may result from driver.UpdateTask to store in the event  Fixes: https://github.com/hashicorp/consul-terraform-sync/issues/318",2021-06-18T21:14:01+00:00,2021-06-21T17:40:08+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/319,319.0,2021-06-21T17:40:08+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/319,0,2,0,2,61,5,0,66,68.43527777777778,bug;backport/0.1,False,True,normal,functional,"[{""filename"": ""api/task.go"", ""lines_added"": 32, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""api/task_test.go"", ""lines_added"": 29, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",api,False
zeromicro/go-zero,4397,fix:  wrong way of Unmarshal,"I faced a minor problem when I used it. for examlpe: ```api type request { 	Bytes []byte `json:""bytes""` 	Name  string `json:""name""` }  type response {}  service template { 	@handler postDataTest 	post /test (request) returns (response)  	@handler getJsonData 	get /get returns (request) } ``` and `get` returns  ```go &types.Request{ 		Bytes: []byte(""11122344wsss""), 		Name:  ""test"", 	} ``` It's ok when I visit `/get` api ,but when I put `/get` response (the json data of `request`) into `/test` error happend:  ```log fullName: `bytes`, error: `string: `MTExMjIzNDR3c3Nz`, error: `invalid character 'M' looking for beginning of value`` ``` and I notice that ,there use json.Marshal() to make the response but use slef custom mapping.UnmarshalJsonReader to Unmarshal json data  so I just fix it (core in mapping.processFieldNotFromString).  You can see test in TestUnmarshalJsonReaderMultiArray ",2024-09-27T15:11:25+00:00,2025-01-22T13:25:52+00:00,2,https://github.com/zeromicro/go-zero/pull/4397,4397.0,2025-01-22T13:25:52+00:00,https://github.com/zeromicro/go-zero/pull/4397,0,2,0,2,30,2,0,32,2806.2408333333333,,False,True,normal,configuration,"[{""filename"": ""core/mapping/unmarshaler.go"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/mapping/unmarshaler_test.go"", ""lines_added"": 16, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",core,False
hashicorp/consul-terraform-sync,291,Fix catalog-service race condition during once-mode,"Once-mode requires all dependencies to be retrieved before sending a notification to CTS to render the complete template and execute the task.  A task has a template which can have multiple types of dependencies. A task configured with catalog-service condition will only notify on a catalog-service type dependency changes and not on other types of dependency changes.  During once-mode, the template dependencies are received asynchronously and therefore can cause a race:  - If catalog-service dependency is received last: this is fine with once-mode i.e. all  dependencies are received, notification is sent, task is executed  - If a different dependency is received last: no notification is sent because only catalog-service  dependencies send notification. However, once-mode is waiting for all the dependencies to  be received. Therefore, once-mode hangs even though all dependencies are received and task  is ready to execute for once-mode.  Change:  - Remove existing code to handle once-mode. This only partially handles the once-mode complexity.  It only resolves the case when catalog-service dependency has no registration change during once-mode  - Add new code to use a dependency counter to determine when all dependencies have been received.  Once received, trigger notification regardless of what type of dependency is received last  - Alternative ideas welcome!   Sample logs showing the hang:  ``` 2021/05/26 19:41:05.979458 [INFO] (ctrl) driver initialized 2021/05/26 19:41:05.979477 [INFO] (ctrl) executing all tasks once through 2021/05/26 19:41:05.979917 [DEBUG] (ctrl) rendered template false 2021/05/26 19:41:05.979926 [DEBUG] (ctrl) watching 3 dependencies                                 // notice: waiting for 3 dependency changes 2021/05/26 19:41:06.082522 [DEBUG] (notifier) dependency change type []*dep.HealthService 2021/05/26 19:41:06.086875 [DEBUG] (notifier) dependency change type []*dep.CatalogSnippet 2021/05/26 19:41:06.086891 [DEBUG] (notifier) notify registration change                          // catalog-service dep (log above) sends notification as expected 2021/05/26 19:41:06.087272 [DEBUG] (ctrl) rendered template false                                 // because of once-mode, template is not rendered because still missing 3rd dependency 2021/05/26 19:41:06.088918 [DEBUG] (notifier) dependency change type []*dep.HealthService         // last dependency received. but is not catalog-service type. therefore no notification sent. 2021/05/26 19:41:20.769355 [INFO] (cli) signal received to initiate graceful shutdown: interrupt  // note the timstamp. cts hangs for 14s 2021/05/26 19:41:30.769673 [INFO] (cli) graceful shutdown timed out, exiting ```",2021-05-27T15:56:30+00:00,2021-05-27T19:26:15+00:00,3,https://github.com/hashicorp/consul-terraform-sync/pull/291,291.0,2021-05-27T19:26:15+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/291,0,3,0,3,130,23,0,153,3.495833333333333,bug,False,True,normal,configuration,"[{""filename"": ""driver/terraform.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""templates/tftmpl/notifier/notifier.go"", ""lines_added"": 62, ""lines_deleted"": 13, ""file_type"": ""app_code""}, {""filename"": ""templates/tftmpl/notifier/notifier_test.go"", ""lines_added"": 66, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",,False
alibaba/nacos,6557,fix #1733 again for nacos 1.x,"fix #1733 again for nacos 1.x to avoid creating beat task more than once by doing ConcurrentHashMap.put concurrently. i met the issue again by using nacos sync: 1. restart N instances of one sync task at the same time, it trigged sync event more than once, 2. for each sync event, nacos sync called registerInstance concurrently, 3. ConcurrentHashMap.put was called concurrently for the same instance. ",2021-08-04T07:09:07+00:00,2021-08-05T01:31:05+00:00,0,https://github.com/alibaba/nacos/pull/6557,6557.0,2021-08-05T01:31:05+00:00,https://github.com/alibaba/nacos/pull/6557,0,1,0,1,2,3,0,5,18.36611111111111,kind/bug,False,True,normal,functional,"[{""filename"": ""client/src/main/java/com/alibaba/nacos/client/naming/beat/BeatReactor.java"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,271,Update fallback Terraform version from 0.13.5 to 0.13.7,Terraform 0.13.7 was just released with the rotated HashiCorp GPG signing key used for provider package verification ([HCSEC-2021-12](https://discuss.hashicorp.com/t/hcsec-2021-12-codecov-security-event-and-hashicorp-gpg-key-exposure/23512)). Update CTS's fallback  version of Terraform to this release.,2021-04-27T16:48:06+00:00,2021-04-27T17:13:03+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/271,271.0,2021-04-27T17:13:03+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/271,0,1,0,1,1,1,0,2,0.4158333333333333,security;backport/0.1,False,True,normal,security,"[{""filename"": ""driver/terraform_download.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,261,all but last consul retry log set to DEBUG level,Consul's retries were logging every retry for every watched field. It was very verbose. This quiets it down by changing all the retry logs to DEBUG level except for the last one which is (unchanged from) ERR level.  Fixes #260,2021-04-21T23:43:39+00:00,2021-04-22T19:58:47+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/261,261.0,2021-04-22T19:58:47+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/261,0,1,0,1,1,1,0,2,20.252222222222223,enhancement,False,True,normal,ui,"[{""filename"": ""controller/watcher.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
dapr/dapr,8329,[Workflows] Activity restart loop after TerminateWorkflowAsync,"## In what area(s)?  /area runtime  ## What version of Dapr?  1.14.4 1.15.0-rc.1  ## Expected Behavior  After calling TerminateWorkflowAsync on a root workflow, I expect both the root and child workflows to terminate, and the executing workflow activity to complete.  ## Actual Behavior  After TerminateWorkflowAsync, the executing activity completes and the loop restarts indefinitely, executing and completing repeatedly.   ## Steps to Reproduce the Problem  ``` C# public class TestWorkflow : Workflow<Message, Message> // This is the root workflow {    public override async Task<Message> RunAsync(WorkflowContext context, Message input)   {          await context.CallChildWorkflowAsync<TestChildWorkflow>(nameof(TestChildWorkflow),new Message { Input = input.Input + ""1"" });      await context.CallChildWorkflowAsync<TestChildWorkflow>(nameof(TestChildWorkflow), new Message { Input = input.Input + ""2"" });      return input;   } }  public class TestChildWorkflow : Workflow<Message, Message> {    public override async Task<Message> RunAsync(WorkflowContext context, Message input)   {           await context.CallActivityAsync<TestActivity>(nameof(TestActivity), input);      await context.CallActivityAsync<TestActivity>(nameof(TestActivity), input);      return input;   } }  public class TestActivity : WorkflowActivity<Message, Message> {   private readonly ILogger<TestActivity> logger;    public TestActivity()   {        }    public TestActivity(ILogger<TestActivity> logger)   {     this.logger = logger;   }    public override async Task<Message> RunAsync(WorkflowActivityContext context, Message input)   {           logger.LogError($""TestActivity started: input {input.Input}, actId {context.InstanceId}"");      for (int i = 0; i < 10; i++)     {       await Task.Delay(TimeSpan.FromSeconds(2));       logger.LogError($""TestActivity {i}: input {input.Input}, actId {context.InstanceId}"");     }      logger.LogError($""TestActivity completed: input {input.Input}, actId {context.InstanceId}"");      return input;   } }  public class Message {   public string Input { get; set; } }  ```  I start the TestWorkflow by an .NET Api Controller invoking `DaprWorkflowClient`: `await client.ScheduleNewWorkflowAsync(nameof(TestWorkflow), input: new Message { Input = input });`.  Then I terminate the workfow invoking `await client.TerminateWorkflowAsync(id);`  In the sidecar log I found these messages: ``` json {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""63c6b326f19746039bfe6a48023c9293: 'TestWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:24:29.929302444Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""Workflow Actor '63c6b326f19746039bfe6a48023c9293': workflow completed with status 'ORCHESTRATION_STATUS_TERMINATED' workflowName 'TestWorkflow'"",""scope"":""dapr.wfengine.backend.actors"",""time"":""2024-12-04T09:24:29.952382927Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""523f499de9a15119820e34ce43762c42: 'TestChildWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:24:29.953715478Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""warning"",""msg"":""Workflow actor '523f499de9a15119820e34ce43762c42': execution failed with a recoverable error and will be retried later: 'method AddWorkflowEvent on actor '63c6b326f19746039bfe6a48023c9293' returned an error: error from internal actor: failed to unmarshal history event from history state key 'history-000000': unreadable history event payload: proto: cannot parse invalid wire-format data'"",""scope"":""dapr.wfengine.backend.actors"",""time"":""2024-12-04T09:24:29.970146407Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""523f499de9a15119820e34ce43762c42: 'TestChildWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:24:43.211220556Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""warning"",""msg"":""Workflow actor '523f499de9a15119820e34ce43762c42': execution failed with a recoverable error and will be retried later: 'method AddWorkflowEvent on actor '63c6b326f19746039bfe6a48023c9293' returned an error: error from internal actor: failed to unmarshal history event from history state key 'history-000000': unreadable history event payload: proto: cannot parse invalid wire-format data'"",""scope"":""dapr.wfengine.backend.actors"",""time"":""2024-12-04T09:24:43.231731081Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""523f499de9a15119820e34ce43762c42: 'TestChildWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:25:03.303620941Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""warning"",""msg"":""Workflow actor '523f499de9a15119820e34ce43762c42': execution failed with a recoverable error and will be retried later: 'method AddWorkflowEvent on actor '63c6b326f19746039bfe6a48023c9293' returned an error: error from internal actor: failed to unmarshal history event from history state key 'history-000000': unreadable history event payload: proto: cannot parse invalid wire-format data'"",""scope"":""dapr.wfengine.backend.actors"",""time"":""2024-12-04T09:25:03.325679449Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""523f499de9a15119820e34ce43762c42: 'TestChildWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:25:23.377877411Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""warning"",""msg"":""Workflow actor '523f499de9a15119820e34ce43762c42': execution failed with a recoverable error and will be retried later: 'method AddWorkflowEvent on actor '63c6b326f19746039bfe6a48023c9293' returned an error: error from internal actor: failed to unmarshal history event from history state key 'history-000000': unreadable history event payload: proto: cannot parse invalid wire-format data'"",""scope"":""dapr.wfengine.backend.actors"",""time"":""2024-12-04T09:25:23.402750814Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} {""app_id"":""plugandcom-notifications-worker"",""instance"":""076e6f44739f"",""level"":""info"",""msg"":""523f499de9a15119820e34ce43762c42: 'TestChildWorkflow' completed with a TERMINATED status."",""scope"":""wfengine.durabletask.backend"",""time"":""2024-12-04T09:25:43.471848134Z"",""type"":""log"",""ver"":""1.15.0-rc.1""} ```  And in the application log, there were the messages related to the execution loop: ``` json {""@t"":""2024-12-04T09:24:43.2434825Z"",""@mt"":""TestActivity started: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:24:45.2454628Z"",""@mt"":""TestActivity 0: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} .... {""@t"":""2024-12-04T09:25:03.2552535Z"",""@mt"":""TestActivity 9: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:03.2553285Z"",""@mt"":""TestActivity completed: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:03.3182798Z"",""@mt"":""TestActivity started: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:05.3130687Z"",""@mt"":""TestActivity 0: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} ... {""@t"":""2024-12-04T09:25:23.3130274Z"",""@mt"":""TestActivity 9: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:23.3131039Z"",""@mt"":""TestActivity completed: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:23.3920470Z"",""@mt"":""TestActivity started: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} {""@t"":""2024-12-04T09:25:25.3922482Z"",""@mt"":""TestActivity 0: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} ... {""@t"":""2024-12-04T09:25:43.3891403Z"",""@mt"":""TestActivity 9: input input1, actId 523f499de9a15119820e34ce43762c42"",""@l"":""Error"",""SourceContext"":""Efil.PlugAndCom.Notifications.Domain.Workflows.Test.TestActivity"",""Application"":""Notifications.Worker""} ... ```  I'm using Microsoft SQL Server as actor state store.  ",2024-12-04T09:49:33+00:00,2025-02-09T10:02:35+00:00,2,https://github.com/dapr/dapr/issues/8329,5071.0,,https://github.com/dapr/dapr/pull/5071,0,3,2,5,31,28,0,53,1608.2172222222223,kind/bug;stale,False,True,normal,configuration,"[{""filename"": ""go.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""pkg/encryption/encryption_test.go"", ""lines_added"": 5, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/runtime/runtime.go"", ""lines_added"": 15, ""lines_deleted"": 15, ""file_type"": ""app_code""}, {""filename"": ""pkg/runtime/runtime_test.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,250,Fix api-serve unit test race condition retry,The retry was no longer working because testutils.RequestHTTP no longer returns an error and instead checks & handles errors within. The err that was attempted to be handled for the retry was actually an err from above FreePort().  Change: wait a little after serving the API before testing endpoints. 3s is used in other places in our code for serving the API and it so has so far worked well for those tests e.g. api/client_test.go,2021-04-15T15:19:45+00:00,2021-04-15T15:33:42+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/250,250.0,2021-04-15T15:33:42+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/250,0,1,0,1,3,17,0,20,0.2325,bug,False,True,normal,functional,"[{""filename"": ""api/api_test.go"", ""lines_added"": 3, ""lines_deleted"": 17, ""file_type"": ""app_code""}]",api,False
dapr/dapr,7031,Placement server fails to disseminate placement tables.,"## What version of Dapr?  1.11.2, running in AKS (aks version 1.25.5)  ## Expected Behavior  When I update my application deployment, and new pods are started with new IPs, I expect the Placement Server to distribute the updated tables to all pods We use a Helm chart to manage our application. Our Release pipeline does a `helm upgrade --namespace myNamespace --install --wait --debug --create-namespace  /path/to/myChart` to install the application.  ## Actual Behavior  The Placement Server Starts the dissemination, but never completes, and never generates a timeout nor error.  Tail of logs shown here. We can see the last line is a `Start disseminating tables` but we never get a `Completed dissemination` nor do we get any error logs.  ``` time=""2023-10-10T19:38:39.299921228Z"" level=info msg=""Completed dissemination. memberUpdateCount: 2, streams: 37, targets: 8, table generation: 770"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-10T19:39:00.121337481Z"" level=info msg=""Start disseminating tables. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 772"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-10T19:39:00.197528468Z"" level=info msg=""Completed dissemination. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 772"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-11T20:53:09.620082431Z"" level=info msg=""Start disseminating tables. memberUpdateCount: 2, streams: 39, targets: 8, table generation: 774"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-11T20:53:09.805892521Z"" level=info msg=""Completed dissemination. memberUpdateCount: 2, streams: 39, targets: 8, table generation: 774"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-11T20:53:30.619910487Z"" level=info msg=""Start disseminating tables. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 776"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-11T20:53:30.704178149Z"" level=info msg=""Completed dissemination. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 776"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-12T12:27:44.120112958Z"" level=info msg=""Start disseminating tables. memberUpdateCount: 2, streams: 39, targets: 8, table generation: 778"" instance=dapr-placement-server-0 scope=dapr.placement type=log ver=1.11.2 ``` ## Steps to Reproduce the Problem  We are unable to repro this on demand. It happens after some amount of time. The placement server pod was 77 days old in this case. After removing placement-server-0, placement-server-1 immediately assumed the leadership role, and disseminated the tables. ``` time=""2023-10-12T14:37:34.143432489Z"" level=info msg=""Cluster leadership acquired"" instance=dapr-placement-server-1 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-12T14:37:34.188969089Z"" level=info msg=""leader is established."" instance=dapr-placement-server-1 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-12T14:37:43.69409227Z"" level=info msg=""Start disseminating tables. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 780"" instance=dapr-placement-server-1 scope=dapr.placement type=log ver=1.11.2 time=""2023-10-12T14:37:43.792961432Z"" level=info msg=""Completed dissemination. memberUpdateCount: 2, streams: 30, targets: 6, table generation: 780"" instance=dapr-placement-server-1 scope=dapr.placement type=log ver=1.11.2  ```  ## Release Note <!-- How should this new feature be announced in our release notes? It can be populated later. --> <!-- Keep it as a single line. Examples: -->  <!-- RELEASE NOTE: **ADD** New feature in Dapr. --> <!-- RELEASE NOTE: **FIX** Bug in runtime. --> <!-- RELEASE NOTE: **UPDATE** Runtime dependency. -->  RELEASE NOTE:  **FIX** Fixes placement service dissemination failure on stream hang ",2023-10-12T14:44:33+00:00,2025-02-07T21:00:54+00:00,24,https://github.com/dapr/dapr/issues/7031,8326.0,2025-02-06T00:09:38+00:00,https://github.com/dapr/dapr/pull/8326,0,19,1,20,953,650,0,1601,11577.418055555556,kind/bug,False,True,normal,networking,"[{""filename"": ""cmd/placement/app/app.go"", ""lines_added"": 22, ""lines_deleted"": 31, ""file_type"": ""app_code""}, {""filename"": ""docs/development/dapr-metrics.md"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/placement/leadership.go"", ""lines_added"": 94, ""lines_deleted"": 82, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/leadership_test.go"", ""lines_added"": 58, ""lines_deleted"": 30, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/membership.go"", ""lines_added"": 76, ""lines_deleted"": 56, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/membership_test.go"", ""lines_added"": 40, ""lines_deleted"": 63, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/monitoring/metrics.go"", ""lines_added"": 41, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/placement.go"", ""lines_added"": 193, ""lines_deleted"": 130, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/placement_test.go"", ""lines_added"": 67, ""lines_deleted"": 189, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/pool.go"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/raft/ha_test.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/raft/server.go"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/raft/util.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/raft/util_test.go"", ""lines_added"": 14, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/tests/tests.go"", ""lines_added"": 10, ""lines_deleted"": 50, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/placement/placement.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/resources/uniquename.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/subscriptions/programmatic/grpc/slowappstartup.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/placement/dissemination/streamhang.go"", ""lines_added"": 233, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/placement/metrics/leadership.go"", ""lines_added"": 73, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",placement;raft;grpc,True
hashicorp/consul-terraform-sync,238,Default version name,"CTS versions 0.1.0-techpreview2 and 0.1.0-beta output excluded the CLI name due to the build process not setting it. We likely missed this because the local development build process using the [Makefile set the name](https://github.com/hashicorp/consul-terraform-sync/blob/master/Makefile#L26).  ``` $ consul-terraform-sync -version  0.1.0-techpreview2 (d844653) Compatible with Terraform >= 0.13.0, < 0.15 ```  should be  ``` $ consul-terraform-sync -version consul-terraform-sync 0.1.0-techpreview2 (d844653) Compatible with Terraform >= 0.13.0, < 0.15 ```  not going to back port this, fixing for 0.1.0 GA and beyond",2021-03-29T18:04:08+00:00,2021-03-29T18:05:40+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/238,238.0,2021-03-29T18:05:40+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/238,0,2,0,2,4,4,0,8,0.0255555555555555,bug,False,True,normal,ui,"[{""filename"": ""command/cli.go"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""version/version.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
dapr/dapr,8308,Default resiliencies do not seem to be instrumented when activated,"<!-- If you need to report a security issue please visit https://docs.dapr.io/operations/support/support-security-issues -->  ## In what area(s)?  > /area runtime  ## What version of Dapr?  all  ## Expected Behavior  Currently we seem to only get metrics for resiliencies that had been added by the user but not for default resiliency policies (when the user does not add a custom one).   ## Actual Behavior  I'd like to be able to capture this metrics. This could be behind a config flag if not of general interest.   ## Steps to Reproduce the Problem  Run an app with some need for retries and check the metrics, there will be none for default resiliencies  ## Release Note  <!-- How should the fix for this issue be communicated in our release notes? It can be populated later. --> <!-- Keep it as a single line. Examples: -->  <!-- RELEASE NOTE: **ADD** New feature in Dapr. --> <!-- RELEASE NOTE: **FIX** Bug in runtime. --> <!-- RELEASE NOTE: **UPDATE** Runtime dependency. -->  RELEASE NOTE: ",2024-11-25T16:05:21+00:00,2025-01-31T16:28:10+00:00,2,https://github.com/dapr/dapr/issues/8308,4842.0,2022-08-23T20:37:33+00:00,https://github.com/dapr/dapr/pull/4842,0,5,0,5,85,8,0,93,-19795.463333333333,kind/bug;stale,False,True,normal,configuration,"[{""filename"": ""pkg/actors/actors.go"", ""lines_added"": 26, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pkg/actors/actors_test.go"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/actors/reminder_track.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/apps/actorfeatures/app.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/e2e/actor_features/actor_features_test.go"", ""lines_added"": 45, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",tests,False
zeromicro/go-zero,4289,refactor(core/errorx): use errors.Join simplify error handle,Replace the manual error string concatenation in errorArray.Error() with errors.Join() from the errors package to streamline error formatting.  refer to: https://github.com/zeromicro/go-zero/pull/3639,2024-07-31T10:57:01+00:00,2024-08-03T03:01:15+00:00,1,https://github.com/zeromicro/go-zero/pull/4289,4289.0,2024-08-03T03:01:15+00:00,https://github.com/zeromicro/go-zero/pull/4289,0,2,0,2,35,10,0,45,64.07055555555556,,False,True,normal,functional,"[{""filename"": ""core/errorx/batcherror.go"", ""lines_added"": 6, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""core/errorx/batcherror_test.go"", ""lines_added"": 29, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,201,Change API error from string to object,"**Breaking change**: This will give room for future details to be added to the API error responses.  Prior to change ``` {   ""error"": ""unsupported status parameter value. only supporting status values successful, errored, critical, and unknown but got xxx"" } ```  After change ``` {   ""error"": {     ""message"": ""unsupported status parameter value. only supporting status values successful, errored, critical, and unknown but got XXX""   } } ```  ",2021-02-19T23:51:09+00:00,2021-02-22T15:44:09+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/201,201.0,2021-02-22T15:44:09+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/201,0,5,0,5,49,37,0,86,63.88333333333333,,False,True,critical,functional,"[{""filename"": ""api/api.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""api/client.go"", ""lines_added"": 5, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""api/error.go"", ""lines_added"": 29, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""api/task.go"", ""lines_added"": 8, ""lines_deleted"": 24, ""file_type"": ""app_code""}, {""filename"": ""api/task_status.go"", ""lines_added"": 3, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",api,False
hashicorp/consul-terraform-sync,151,Fix leaky idle connections on shutdown,"`hcat.Watcher.Stop()` closes out client idle connections. It was not being called for 3 shutdown scenarios * Errors * Once mode * Inspect mode  Changes now ensure the controller calls stop on its watcher. Also adds conditional logging on the hcat dependency size. This PR doesn't fix #146 where CTS quickly reaches the connection limit for the Consul agent, but is related to help debug.  Resolves #149",2020-12-07T22:49:01+00:00,2020-12-08T14:37:06+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/151,151.0,2020-12-08T14:37:06+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/151,0,8,0,8,55,7,0,62,15.801388888888887,bug,False,True,normal,networking,"[{""filename"": ""cli.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""controller/controller.go"", ""lines_added"": 20, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""controller/readonly.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""controller/readonly_test.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""controller/readwrite.go"", ""lines_added"": 6, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""controller/readwrite_test.go"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""mocks/templates/watcher.go"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""templates/hcat.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
dapr/dapr,8364,Workflow: gracefully recover from legacy state read errors,"Currently, after upgrading from Dapr v1.14 to v1.15 and executing a workflow which has state from v1.14, the workflow state will fail to parse and cause the workflow to error. This is due to the storage API changing moving to v1 GA.  To resolve this, we fall back to parsing the workflow metadata key as JSON, then in the case of event history, ignoring the existing state if it fails to parse. Practically, this means a mid-running workflow will be abandoned when upgrading to v1.15 which seems acceptable. The other approach to this would be for use intervention to manually clean up the actor state store which is unreasonable.  cc @olitomlinson ",2025-01-08T14:02:53+00:00,2025-01-09T18:54:00+00:00,4,https://github.com/dapr/dapr/pull/8364,8364.0,2025-01-09T18:54:00+00:00,https://github.com/dapr/dapr/pull/8364,0,2,0,2,36,6,0,42,28.851944444444445,,False,True,normal,configuration,"[{""filename"": ""pkg/runtime/wfengine/state/state.go"", ""lines_added"": 27, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/workflow/timer.go"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,117,Return version before processing config files,"Recent change returns an error for no config files, which revealed that version flag was processed after loading configs. Swapped the order to exit earlier",2020-10-09T17:46:37+00:00,2020-10-09T17:48:04+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/117,117.0,2020-10-09T17:48:04+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/117,0,1,0,1,7,7,0,14,0.0241666666666666,bug,False,True,normal,configuration,"[{""filename"": ""cli.go"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""app_code""}]",,False
dapr/dapr,8209,[Clone] Error Standardization: Secrets API ,"Original https://github.com/dapr/dapr/issues/7484  Richer errors in Dapr for the **Secrets API**.  **Note**: Please clone this issue and assign the clone of this issue to yourself if you would like to contribute to the error standardization efforts.  # Scope of work * [ ]  Choose 5-10 errors in the Secrets API to enrich. Ideally, cover as many errors as possible from the API. * [ ]  Follow the steps outlined in [this README](https://github.com/dapr/dapr/blob/master/pkg/api/errors/README.md) for each error being updated  # Definition of done * [ ]  Update 5-10 errors in the Secrets API * [ ]  Update the unit tests, if necessary * [ ]  Write integration tests to cover each updated errors  # Keep in mind Unless a major version is being released and there is a solid reason for introducing backwards incompatibility, we can't change the API, meaning the HTTP and gRPC status codes, and the error codes must stay the same. An exception to this is the gRPC `StatusCode.UNKNOWN` which should be converted to a specific status.  See the [kit errors pkg README](https://github.com/dapr/kit/blob/main/errors/README.md) for another usage example. For each error being updated, you can use the following helper functions to add error details, see the kit [errors pkg](https://github.com/dapr/kit/tree/main/errors) for more details:  * `WithHelpLink()` * `WithHelp()` * `WithErrorInfo()` - **Required** * `WithFieldViolation()` * `WithDetails()`  # Reference PRs Existing error standardization for:  * [State API](https://github.com/dapr/dapr/pull/7257) * [PubSub API](https://github.com/dapr/dapr/pull/7322)",2024-10-17T16:49:28+00:00,2024-12-23T17:02:23+00:00,3,https://github.com/dapr/dapr/issues/8209,8210.0,,https://github.com/dapr/dapr/pull/8210,0,14,0,14,1264,19,0,1283,1608.2152777777778,stale,False,True,major,ui,"[{""filename"": ""pkg/api/errors/secret.go"", ""lines_added"": 115, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/api/universal/secrets.go"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/api/universal/secrets_test.go"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/messages/predefined.go"", ""lines_added"": 0, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/secretstore/component.go"", ""lines_added"": 95, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/secretstore/localfile/localfile.go"", ""lines_added"": 92, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/secretstore/localfile/options.go"", ""lines_added"": 38, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/secretstore/options.go"", ""lines_added"": 36, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/framework/process/secretstore/secretstore.go"", ""lines_added"": 89, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/hotreload/operator/secret.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/hotreload/selfhosted/secret.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/secret/grpc/errors.go"", ""lines_added"": 336, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/secret/http/errors.go"", ""lines_added"": 450, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/suite/daprd/secret/secret.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
hashicorp/consul-terraform-sync,104,Update hcat to wait on Consul agent on start up,"Pulls in @eikenb 's changes to [hcat/#12](https://github.com/hashicorp/hcat/pull/12). This is useful on a few fronts: * When the Consul server is not reachable on startup, Sync will error earlier instead of allowing controller/driver/tasks are set up to finally error and exit on first fetch to a template dependency on `watcher.Wait` ``` 2020/10/05 19:06:28.538096 [INFO] 1de7959 (1de7959) 2020/10/05 19:06:28.538149 [INFO] (cli) setting up controller 2020/10/05 19:06:28.538152 [INFO] (controller) setting up Terraform driver 2020/10/05 19:06:28.539394 [ERR] (cli) error setting up controller: Get ""http://localhost:8500/v1/status/leader"": dial tcp 127.0.0.1:8500: connect: connection refused ``` * Waits for Consul leader to be elected to ensure consistency across agents before fetching info from Consul Catalog (#98)  Resolves #98",2020-10-05T19:12:28+00:00,2020-10-06T01:06:14+00:00,5,https://github.com/hashicorp/consul-terraform-sync/pull/104,104.0,2020-10-06T01:06:14+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/104,0,3,2,5,34,14,0,44,5.896111111111111,bug,False,True,normal,networking,"[{""filename"": ""controller/consul.go"", ""lines_added"": 10, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""controller/readwrite.go"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""controller/readwrite_test.go"", ""lines_added"": 16, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""go.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
traefik/traefik,11509,Regression - Do not compress when content type text/event-stream,"### Welcome!  - [x] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any. - [x] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.  ### What did you do?  I've just upgraded from v2.11 to v3.3 and all request with content type`text/event-stream` are broken because of enabled compression.  It wasn't part of any release notes/migration guide, wasn't marked as a breaking change, so I believe it's a regression, since it [was already part of Traefik since 2019](https://github.com/traefik/traefik/pull/5120).  ### What did you see instead?  Had to manually exclude it from compression to fix the problem.  ### What version of Traefik are you using?  v3.3.3  ### What is your environment & configuration?  ```yaml networks:   coolify:     external: true services:   traefik:     container_name: coolify-proxy     image: 'traefik:v3.3'     restart: unless-stopped     extra_hosts:       - 'host.docker.internal:host-gateway'     networks:       - coolify     ports:       - '80:80'       - '443:443'       - '443:443/udp'       - '8080:8080'     healthcheck:       test: 'wget -qO- http://localhost:80/ping || exit 1'       interval: 4s       timeout: 2s       retries: 5     volumes:       - '/var/run/docker.sock:/var/run/docker.sock:ro'       - '/data/coolify/proxy:/traefik'     command:       - '--ping=true'       - '--ping.entrypoint=http'       - '--api.dashboard=true'       - '--api.insecure=false'       - '--entrypoints.http.address=:80'       - '--entrypoints.https.address=:443'       - '--entrypoints.http.http.encodequerysemicolons=true'       - '--entryPoints.https.http3'       - '--entrypoints.https.http.encodequerysemicolons=true'       - '--entryPoints.https.http3'       - '--providers.file.directory=/traefik/dynamic/'       - '--providers.file.watch=true'       - '--certificatesresolvers.letsencrypt.acme.httpchallenge=true'       - '--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=http'       - '--certificatesresolvers.letsencrypt.acme.storage=/traefik/acme.json'       - '--api.insecure=false'       - '--providers.docker=true'       - '--providers.docker.exposedbydefault=false'     labels:       - traefik.enable=true       - traefik.http.routers.traefik.entrypoints=http       - traefik.http.routers.traefik.service=api@internal       - traefik.http.services.traefik.loadbalancer.server.port=8080       - coolify.managed=true       - coolify.proxy=true ```  It's just a generic Coolify instance, nothing crazy, and not really related to the core of the issue.   ### If applicable, please paste the log output in DEBUG level  _No response_",2025-02-03T23:23:06+00:00,2025-03-07T15:16:06+00:00,6,https://github.com/traefik/traefik/issues/11509,11583.0,2025-03-07T15:16:04+00:00,https://github.com/traefik/traefik/pull/11583,0,3,1,4,175,69,0,236,759.8827777777777,priority/P1;kind/bug/confirmed;area/middleware,False,True,critical,configuration,"[{""filename"": ""docs/content/middlewares/http/compress.md"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/middlewares/compress/compress_test.go"", ""lines_added"": 84, ""lines_deleted"": 61, ""file_type"": ""app_code""}, {""filename"": ""pkg/middlewares/compress/compression_handler.go"", ""lines_added"": 17, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/middlewares/compress/compression_handler_test.go"", ""lines_added"": 67, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,54,Update TF workspace selection using API,Setting the workspace through ENV unintentionally affected how environment variables were loaded for providers. It changed the behavior of using the os.Envrion to ignoring all ENV without a means for users to configure providers through ENV via Consul NIA.  * Fixes `plan` to now also have the workspace selected instead of setting workspace per command by moving it to init logic. * Updates `tfexec` to master ([source](https://github.com/hashicorp/terraform-exec/pull/75): removes the ability to set workspace via env),2020-09-08T15:54:19+00:00,2020-09-08T16:53:34+00:00,1,https://github.com/hashicorp/consul-terraform-sync/pull/54,54.0,2020-09-08T16:53:34+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/54,0,5,2,7,66,17,0,39,0.9875,bug,False,True,normal,configuration,"[{""filename"": ""client/terraform_cli.go"", ""lines_added"": 9, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""client/terraform_cli_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/terraform_exec.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""controller/readwrite.go"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""go.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 40, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""mocks/client/terraformExec.go"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,973,Support once-mode failing silently,"We want to be able to support when once-mode does not exit on error. One use-case for this is when the tasks have been known to run successfully before. When CTS starts up, we expect the tasks to be successful and any errors are intermittent. In this case, we want CTS to not exit and just log the error.   Commits  - 1 & 2: Split out error test case for `Test_Once_Run_Terraform` in preparation for new error case in commit 4  - 3: Add TaskFailsSilently (open to better naming ideas) in tasks manager  - 4: Update once-mode with a failSilently flag that calls TaskFailsSilently when true  Edit: from PR feedback changed ""fail silently"" language to ""allow fail""",2022-06-27T15:18:28+00:00,2022-06-28T14:45:00+00:00,0,https://github.com/hashicorp/consul-terraform-sync/pull/973,973.0,2022-06-28T14:45:00+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/973,0,4,0,4,171,36,0,207,23.442222222222224,,False,True,normal,functional,"[{""filename"": ""controller/once.go"", ""lines_added"": 16, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""controller/once_test.go"", ""lines_added"": 63, ""lines_deleted"": 32, ""file_type"": ""app_code""}, {""filename"": ""controller/tasks_manager.go"", ""lines_added"": 44, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""controller/tasks_manager_test.go"", ""lines_added"": 48, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
dapr/dapr,7275,W3C Correlation of Event Publishing broken,"## In what area(s)? /area runtime  ## What version of Dapr? 1.12.x ## Expected Behavior When using the DaprClient, I would expect the messenger to correlate the messages using the traceparent header correctly.  ## Actual Behavior When using the DaprClient, I see each of the `PublishEvents` as a separate ""root"" log. Check this log. ![image](https://github.com/dapr/dapr/assets/12340489/51aa0ad1-f9f3-41da-9829-eaa7aede8087)  After each PubSub publish process, a new ""Root"" trace gets generated.  When I add the `traceparent` header manually, like here:  ![image](https://github.com/dapr/dapr/assets/12340489/321e2294-224f-4259-89ca-630c5623000b) I *kind of* get a better result: ![image](https://github.com/dapr/dapr/assets/12340489/3592cc33-6039-492e-9d3a-bab1dd74c92f)  But notice, that still the bottom displays 2 uncorrelated logs, which I don't know how to classify further.  ## Steps to Reproduce the Problem  1. Queue a Message. 2. Use a PubSub Topic Trigger. 3. Chain trigger / publish functions.  ## Release Note RELEASE NOTE: **FIX** W3C Correlation of Event Publishing ",2023-12-06T17:15:11+00:00,2024-11-30T09:41:38+00:00,9,https://github.com/dapr/dapr/issues/7275,7494.0,,https://github.com/dapr/dapr/pull/7494,0,7,0,7,21,8,0,29,8632.440833333334,kind/bug;P0;stale,False,True,critical,ui,"[{""filename"": ""pkg/actors/actors.go"", ""lines_added"": 12, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/actors/actors_mock.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/actors/actors_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/actors/internal_actor_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/ha_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/placement/hashing/consistent_hash.go"", ""lines_added"": 1, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/runtime/wfengine/wfengine_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
traefik/traefik,11405,websocket errors after upgrading traefik from 3.2.3 to 3.2.4 or 3.3.0,"### Welcome!  - [x] Yes, I've searched similar issues on [GitHub](https://github.com/traefik/traefik/issues) and didn't find any. - [x] Yes, I've searched similar issues on the [Traefik community forum](https://community.traefik.io) and didn't find any.  ### What did you do?  I am running MinIO behind traefik using docker compose, and after upgrading today to 3.2.4 or 3.3.0 MinIO fails with websocket errors when trying to list/browse objects in a bucket through the console web interface. Downgrading to 3.2.3 fixes it again with the following compose file listed below. What broke!?!  ### What did you see instead?  websocket connection error in  browser developer console and nothing is listed....  ### What version of Traefik are you using?  3.2.4  ### What is your environment & configuration? ```  services:   minio1:     image: minio/minio:RELEASE.2024-12-18T13-15-44Z     restart: unless-stopped     volumes:       - /opt/minio:/opt/minio     expose:       - ""9000""       - ""9001""     environment:       MINIO_ROOT_USER: ${MINIO_ADMIN_USER}       MINIO_ROOT_PASSWORD: ${MINIO_ADMIN_PASS}       MINIO_VOLUMES: ""/opt/minio""       MINIO_PROMETHEUS_AUTH_TYPE: public     command:       - server       - --address       - "":9000""       - --console-address       - "":9001""     healthcheck:       test: [ ""CMD"", ""mc"", ""ready"", ""local"" ]       interval: 5s       timeout: 5s       retries: 5     labels:       - ""traefik.enable=true""       ## minio-console       - ""traefik.http.routers.minio-console.rule=Host(`minio-console-osl.example.com`)""       - ""traefik.http.routers.minio-console.entrypoints=web,websecure""       - ""traefik.http.routers.minio-console.tls=true""       - ""traefik.http.routers.minio-console.service=minio-console""       - ""traefik.http.services.minio-console.loadbalancer.server.port=9001""       ## minio-api       - ""traefik.http.routers.minio-api.rule=HostRegexp(`[host:.+]`)""       - ""traefik.http.routers.minio-api.entrypoints=web,websecure""       - ""traefik.http.routers.minio-api.tls=true""       - ""traefik.http.routers.minio-api.service=minio-api""       - ""traefik.http.services.minio-api.loadbalancer.server.port=9000""    traefik:     # image: ""traefik:v3.2.3""     image: ""traefik:v3.2.4""     container_name: ""traefik""     restart: unless-stopped     command:       - ""--log.level=DEBUG""       - ""--api""       - ""--api.insecure=true""       - ""--api.dashboard=true""       - ""--accesslog=true""       - ""--accesslog.format=json""       - ""--providers.docker=true""       - ""--providers.docker.exposedbydefault=false""       - ""--entrypoints.web.address=:80""       - ""--entrypoints.websecure.address=:443""       - ""--entrypoints.traefik.address=:8080""       - ""--providers.file.directory=/traefik-conf/""       - ""--providers.file.watch=true""       - ""--metrics.prometheus=true""       - ""--metrics.prometheus.addEntryPointsLabels=true""       - ""--metrics.prometheus.addServicesLabels=true""       - ""--entrypoints.web.http.redirections.entryPoint.to=websecure""       - ""--entrypoints.web.http.redirections.entryPoint.scheme=https""       - ""--entrypoints.web.http.redirections.entrypoint.permanent=true""     ports:       - ""80:80""       - ""443:443""       - ""8080:8080""     volumes:       - ""/var/run/docker.sock:/var/run/docker.sock:ro""       - $PWD/traefik-conf/dynamic-conf.toml:/traefik-conf/dynamic-conf.toml       - $PWD/ssl:/ssl       - /var/log/:/var/log/     depends_on:       - minio1  ```  ### If applicable, please paste the log output in DEBUG level  _No response_",2025-01-06T20:44:14+00:00,2025-01-07T13:58:06+00:00,30,https://github.com/traefik/traefik/issues/11405,11418.0,2025-01-08T10:02:37+00:00,https://github.com/traefik/traefik/pull/11418,0,1,1,2,41,1,0,39,37.30638888888889,priority/P0;area/websocket;kind/bug/confirmed;status/5-frozen-due-to-age,False,True,normal,configuration,"[{""filename"": ""docs/content/deprecation/releases.md"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""integration/websocket_test.go"", ""lines_added"": 39, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
hashicorp/consul-terraform-sync,818,Endpoint error handling for valid error scenarios and improved retries,"## Summary This PR serves to improve handling of potentially valid error scenarios, and to improve retries by not performing retries in certain scenarios by  - Removing dependency on `/agent/self` for version check. This was found to be a workaround, and may not be the most dependable way to check if CTS is communicating with Consul Enterprise or OSS - Add logic to retries to only retry on 5xx and 429 (too many requests) errors - Rely on 404’s returned from Consul api to determine if connected to Enterprise or OSS Consul  ## Solution To accomplish the above, I have added two new Error types:  1. `NonRetryableError` which can be used to signal to the retry logic to halt retries, for example in the case where the error returned was a 4xx (except 429) error 2. `NonEnterpriseConsulError` which is returned by the Consul Client to indicate to the caller that the error was a result of not connecting to a Consul Enterprise Server  The only API call currently implemented `GetLicense` Consul Client will process response codes and wrap errors with `NonRetryableError` to indicate to the retry code to stop retries. If a response code is a `404`, this will be interpreted as a result of connecting to an OSS Consul server, where the `license` endpoint would not exist. In this case, wrap the error in the `NonEnterpriseConsulError` and leave it to the caller to handle it.",2022-04-27T22:22:42+00:00,2022-04-29T15:40:15+00:00,1,https://github.com/hashicorp/consul-terraform-sync/pull/818,818.0,2022-04-29T15:40:14+00:00,https://github.com/hashicorp/consul-terraform-sync/pull/818,0,6,0,6,187,163,0,350,41.29222222222222,,False,True,normal,functional,"[{""filename"": ""api/response_writers.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""api/task_lifecycle_client.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""client/consul.go"", ""lines_added"": 68, ""lines_deleted"": 53, ""file_type"": ""app_code""}, {""filename"": ""client/consul_test.go"", ""lines_added"": 40, ""lines_deleted"": 103, ""file_type"": ""app_code""}, {""filename"": ""retry/retry.go"", ""lines_added"": 26, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""retry/retry_test.go"", ""lines_added"": 47, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",api,False
apache/pulsar,24043,[Bug] Getting UnsupportedOperationException while setting subscription level dispatch rate policy,"### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  Pulsar Version: 3.0.7   ### Minimal reproduce step  Note: The issue is coming intermittently  1. Create a topic and subscription or use an existing topic/subscription 2. Set dispatch rate policy at subscription level using below command 3.`bin\\pulsar-admin topicPolicies set-subscription-dispatch-rate  -bd 34323 -s <<subname>> <<topicfqn>>`   ### What did you expect to see?  The subscription dispatch rate should get set with 200 response.   ### What did you see instead?  Getting below exception  ``` Message: null  Stacktrace:  **java.lang.UnsupportedOperationException**         at java.base/java.util.Collections$EmptyMap.**computeIfAbsent**(Collections.java:4764)         at org.apache.pulsar.broker.admin.impl.PersistentTopicsBase.lambda$**internalSetSubscriptionLevelDispatchRate**$487(PersistentTopicsBase.java:4960)         at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1150)         at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)         at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2147)         at org.apache.pulsar.client.util.RetryUtil.lambda$executeWithRetry$2(RetryUtil.java:62)         at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)         at java.base/java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:887)         at java.base/java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2325)         at org.apache.pulsar.client.util.RetryUtil.executeWithRetry(RetryUtil.java:48)         at org.apache.pulsar.client.util.RetryUtil.lambda$retryAsynchronously$0(RetryUtil.java:42)         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)         at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)         at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)         at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)         at java.base/java.lang.Thread.run(Thread.java:840) ```   ### Anything else?  The error is coming when  `subscriptionPolicies` is null; it's returning `Collections.emptyMap()` which is immutable, and `internalSetSubscriptionLevelDispatchRate` is trying to add the element in that map. Fix would be instead of returning an immutable map, it should be a mutable map.  Ref: https://github.com/apache/pulsar/blob/master/pulsar-broker/src/main/java/org/apache/pulsar/broker/admin/impl/PersistentTopicsBase.java#L4778  ### Are you willing to submit a PR?  - [x] I'm willing to submit a PR!",2025-03-03T12:35:45+00:00,2025-03-17T07:07:11+00:00,2,https://github.com/apache/pulsar/issues/24043,24048.0,2025-03-17T07:07:10+00:00,https://github.com/apache/pulsar/pull/24048,0,2,0,2,46,3,0,49,330.5236111111111,type/bug,False,True,normal,functional,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/admin/TopicPoliciesTest.java"", ""lines_added"": 39, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-common/src/main/java/org/apache/pulsar/common/policies/data/TopicPolicies.java"", ""lines_added"": 7, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
pingcap/tidb,56017,non-dist-task fast-reorg ADD INDEX can't be canceled timely,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  It has been stuck in ""cancelling"" for a long time, and lightning's goroutine didn't exit.  ``` 1 @ 0x474cf87 0x474fbea 0x474e93b 0x474e9c5 0x474e9c5 0x472e3cc 0x472e38b 0x472d8aa 0x472dba5 0x476b42b 0x48a8970 0x48a83d3 0x481cb25 0x481c4e9 0x481c1a5 0x492d6f0 0x492b545 0x30f90ef 0x1f6c901 #	0x474cf86	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).flushKVs.func1+0xc6				/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1355 #	0x474fbe9	slices.partitionCmpFunc[...]+0xe49										/usr/local/go/src/slices/zsortanyfunc.go:154 #	0x474e93a	slices.pdqsortCmpFunc[...]+0x49a										/usr/local/go/src/slices/zsortanyfunc.go:114 #	0x474e9c4	slices.pdqsortCmpFunc[...]+0x524										/usr/local/go/src/slices/zsortanyfunc.go:121 #	0x474e9c4	slices.pdqsortCmpFunc[...]+0x524										/usr/local/go/src/slices/zsortanyfunc.go:121 #	0x472e3cb	slices.SortFunc[...]+0x8b											/usr/local/go/src/slices/sort.go:28 #	0x472e38a	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).flushKVs+0x4a					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1355 #	0x472d8a9	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).appendRowsUnsorted+0x5e9				/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1242 #	0x472dba4	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).AppendRows+0x264					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1279 #	0x476b42a	github.com/pingcap/tidb/pkg/ddl/ingest.(*writerContext).WriteRow+0x1aa						/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/ingest/engine.go:217 #	0x48a896f	github.com/pingcap/tidb/pkg/ddl.writeOneKVToLocal+0x1ef								/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/index.go:1781 #	0x48a83d2	github.com/pingcap/tidb/pkg/ddl.writeChunkToLocal+0xb92								/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/index.go:1740 #	0x481cb24	github.com/pingcap/tidb/pkg/ddl.(*indexIngestBaseWorker).WriteChunk+0x124					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:874 #	0x481c4e8	github.com/pingcap/tidb/pkg/ddl.(*indexIngestBaseWorker).HandleTask+0xa8					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:812 #	0x481c1a4	github.com/pingcap/tidb/pkg/ddl.(*indexIngestLocalWorker).HandleTask+0x124					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:763 #	0x492d6ef	github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool.(*WorkerPool[...]).handleTaskWithRecover+0x1cf	/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool/workerpool.go:142 #	0x492b544	github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool.(*WorkerPool[...]).runAWorker.func1+0x84		/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool/workerpool.go:158 #	0x30f90ee	github.com/pingcap/tidb/pkg/util.(*WaitGroupWrapper).Run.func1+0x4e						/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/util/wait_group_wrapper.go:157  1 @ 0x474cf87 0x474fdc4 0x474e93b 0x474ea38 0x474ea38 0x474ea38 0x474ea38 0x474e9c5 0x474ea38 0x472e3cc 0x472e38b 0x472d8aa 0x472dba5 0x476b42b 0x48a8970 0x48a83d3 0x481cb25 0x481c4e9 0x481c1a5 0x492d6f0 0x492b545 0x30f90ef 0x1f6c901 #	0x474cf86	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).flushKVs.func1+0xc6				/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1355 #	0x474fdc3	slices.partitionCmpFunc[...]+0x1023										/usr/local/go/src/slices/zsortanyfunc.go:157 #	0x474e93a	slices.pdqsortCmpFunc[...]+0x49a										/usr/local/go/src/slices/zsortanyfunc.go:114 #	0x474ea37	slices.pdqsortCmpFunc[...]+0x597										/usr/local/go/src/slices/zsortanyfunc.go:125 #	0x474ea37	slices.pdqsortCmpFunc[...]+0x597										/usr/local/go/src/slices/zsortanyfunc.go:125 #	0x474ea37	slices.pdqsortCmpFunc[...]+0x597										/usr/local/go/src/slices/zsortanyfunc.go:125 #	0x474ea37	slices.pdqsortCmpFunc[...]+0x597										/usr/local/go/src/slices/zsortanyfunc.go:125 #	0x474e9c4	slices.pdqsortCmpFunc[...]+0x524										/usr/local/go/src/slices/zsortanyfunc.go:121 #	0x474ea37	slices.pdqsortCmpFunc[...]+0x597										/usr/local/go/src/slices/zsortanyfunc.go:125 #	0x472e3cb	slices.SortFunc[...]+0x8b											/usr/local/go/src/slices/sort.go:28 #	0x472e38a	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).flushKVs+0x4a					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1355 #	0x472d8a9	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).appendRowsUnsorted+0x5e9				/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1242 #	0x472dba4	github.com/pingcap/tidb/pkg/lightning/backend/local.(*Writer).AppendRows+0x264					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/lightning/backend/local/engine.go:1279 #	0x476b42a	github.com/pingcap/tidb/pkg/ddl/ingest.(*writerContext).WriteRow+0x1aa						/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/ingest/engine.go:217 #	0x48a896f	github.com/pingcap/tidb/pkg/ddl.writeOneKVToLocal+0x1ef								/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/index.go:1781 #	0x48a83d2	github.com/pingcap/tidb/pkg/ddl.writeChunkToLocal+0xb92								/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/index.go:1740 #	0x481cb24	github.com/pingcap/tidb/pkg/ddl.(*indexIngestBaseWorker).WriteChunk+0x124					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:874 #	0x481c4e8	github.com/pingcap/tidb/pkg/ddl.(*indexIngestBaseWorker).HandleTask+0xa8					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:812 #	0x481c1a4	github.com/pingcap/tidb/pkg/ddl.(*indexIngestLocalWorker).HandleTask+0x124					/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/ddl/backfilling_operators.go:763 #	0x492d6ef	github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool.(*WorkerPool[...]).handleTaskWithRecover+0x1cf	/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool/workerpool.go:142 #	0x492b544	github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool.(*WorkerPool[...]).runAWorker.func1+0x84		/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/resourcemanager/pool/workerpool/workerpool.go:158 #	0x30f90ee	github.com/pingcap/tidb/pkg/util.(*WaitGroupWrapper).Run.func1+0x4e						/home/jenkins/agent/workspace/build-common/go/src/github.com/pingcap/tidb/pkg/util/wait_group_wrapper.go:157 ```  Then it begins to `generateAndSendJob`. Seems the `ctx` is not cancelled  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ``` +-----------------------------------------------------------+ | tidb_version()                                            | +-----------------------------------------------------------+ | Release Version: v8.5.0-alpha                             | | Edition: Community                                        | | Git Commit Hash: b97baba693964ef0625eb7f0b4b321d40a16b3fe | | Git Branch: heads/refs/tags/v8.5.0-alpha                  | | UTC Build Time: 2024-09-09 04:40:51                       | | GoVersion: go1.21.10                                      | | Race Enabled: false                                       | | Check Table Before Drop: false                            | | Store: tikv                                               | +-----------------------------------------------------------+ ```",2024-09-11T08:14:53+00:00,2024-10-15T03:19:17+00:00,0,https://github.com/pingcap/tidb/issues/56017,56243.0,2024-09-27T12:03:24+00:00,https://github.com/pingcap/tidb/pull/56243,0,27,0,27,184,114,0,298,387.80861111111113,type/bug;severity/major;affects-6.5;affects-7.1;component/ddl;affects-7.5;affects-8.1;report/customer,False,True,normal,ui,"[{""filename"": ""br/pkg/backup/client.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/backfilling.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/backfilling_operators.go"", ""lines_added"": 12, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/backfilling_read_index.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/cluster.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/db_change_test.go"", ""lines_added"": 6, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ddl.go"", ""lines_added"": 35, ""lines_deleted"": 26, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/executor_test.go"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/export_test.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ingest/backend.go"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ingest/checkpoint.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ingest/integration_test.go"", ""lines_added"": 14, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ingest/mock.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_scheduler.go"", ""lines_added"": 13, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_scheduler_testkit_test.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_submitter_test.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_worker.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_worker_test.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/multi_schema_change_test.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/partition.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/table_modify_test.go"", ""lines_added"": 5, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/tests/adminpause/pause_negative_test.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/tests/fastcreatetable/fastcreatetable_test.go"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/executor.go"", ""lines_added"": 6, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/lightning/backend/local/local.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/realtikvtest/addindextest3/ingest_test.go"", ""lines_added"": 34, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/realtikvtest/addindextest3/operator_test.go"", ""lines_added"": 10, ""lines_deleted"": 11, ""file_type"": ""app_code""}]",,False
apache/pulsar,21954,[Bug] Java Client : Pulsar Java client should be able to configure compression for message rejection in the DLQ,### Search before asking  - [X] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Version  * Pulsar 3.0.1 * Java Client 3.0.1  ### Minimal reproduce step  . Configure a producer with LZ4 compression enabled . Configure a consumer with DLQ. . Reject a very big message (negative ack)   ### What did you expect to see?  Message sent to the DLQ  ### What did you see instead?  Execption : o.a.pulsar.client.impl.ConsumerImpl : [persistent://zzz/mailbox/message-event] Failed to send DLQ message to zzz/mailbox/message-event-recorder-DLQ for message id 64563:0:-1 java.util.concurrent.CompletionException: org.apache.pulsar.client.api.PulsarClientException$InvalidMessageException: Message size is bigger than 20971520 bytes  NB: Max Message size and netty are set 20Mb.  ### Anything else?  Maybe related to :  * https://github.com/apache/pulsar/issues/19463 * https://github.com/apache/pulsar/pull/20959  ### Are you willing to submit a PR?  - [ ] I'm willing to submit a PR!,2024-01-23T08:07:01+00:00,2025-03-06T08:41:35+00:00,3,https://github.com/apache/pulsar/issues/21954,24020.0,2025-03-06T08:41:34+00:00,https://github.com/apache/pulsar/pull/24020,0,7,1,8,433,20,0,360,9792.575833333334,,False,True,normal,configuration,"[{""filename"": ""pip/pip-409.md"", ""lines_added"": 79, ""lines_deleted"": 14, ""file_type"": ""other""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/DeadLetterTopicTest.java"", ""lines_added"": 88, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/RetryTopicTest.java"", ""lines_added"": 77, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client-api/src/main/java/org/apache/pulsar/client/api/DeadLetterPolicy.java"", ""lines_added"": 20, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client-api/src/main/java/org/apache/pulsar/client/api/DeadLetterProducerBuilderContext.java"", ""lines_added"": 58, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client-api/src/main/java/org/apache/pulsar/client/api/DeadLetterProducerBuilderCustomizer.java"", ""lines_added"": 38, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ConsumerImpl.java"", ""lines_added"": 23, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/DeadLetterProducerBuilderContextImpl.java"", ""lines_added"": 50, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pulsar-client-api;client,True
apache/pulsar,24053,[Bug] message eventtime is reset to 0 on reconsumeLater,"### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  OS - macOS Sequoia 15.3.1 Java - 21 Pulsar client - 4.0.2 Pulsar broker - 4.0.2   ### Minimal reproduce step  publish a message to a topic with eventTime set to some value, e.g., current epoch seconds.  On consumption, call reconsumeLater method.  On reconsumption, log message.getEventTime.  ### What did you expect to see?  message.getEventTime should return the original eventTime set during publish  ### What did you see instead?  0  ### Anything else?  _No response_  ### Are you willing to submit a PR?  - [x] I'm willing to submit a PR!",2025-03-04T16:47:01+00:00,2025-03-05T19:26:17+00:00,1,https://github.com/apache/pulsar/issues/24053,24059.0,2025-03-05T19:26:16+00:00,https://github.com/apache/pulsar/pull/24059,0,4,0,4,75,1,0,76,26.654166666666665,type/bug,False,True,normal,functional,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/DeadLetterTopicTest.java"", ""lines_added"": 62, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/RetryTopicTest.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ConsumerImpl.java"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/TypedMessageBuilderImpl.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",client,False
apache/pulsar,23359,[fix][broker] Fix the broker registery cannot recover from the metadata node deletion,"### Motivation  https://github.com/apache/pulsar/pull/23298 introduce a regression that once the metadata node of this broker was deleted (e.g. by session timeout), the broker register would never have a chance to recover. In this case, the clients whose owner is this broker would never be able to produce or consume.  ### Modifications  Add a default listener to `BrokerRegisterImpl` that will register itself again in an asynchronous way if the deleted node is the current broker. Add a new state `Unregistering` to prevent the broker from registering itself again after `unregister()` is called.  Add `BrokerRegistryIntegrationTest` to verify this fix and the behavior introduced from https://github.com/apache/pulsar/pull/23298   ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: ",2024-09-26T17:58:23+00:00,2024-09-27T11:49:02+00:00,6,https://github.com/apache/pulsar/pull/23359,23359.0,2024-09-27T11:49:02+00:00,https://github.com/apache/pulsar/pull/23359,0,5,0,5,168,29,0,197,17.844166666666666,type/bug;area/broker;doc-not-needed;cherry-picked/branch-3.3;release/3.3.2,False,True,normal,networking,"[{""filename"": ""pulsar-broker/src/main/java/org/apache/pulsar/broker/loadbalance/extensions/BrokerRegistry.java"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/main/java/org/apache/pulsar/broker/loadbalance/extensions/BrokerRegistryImpl.java"", ""lines_added"": 34, ""lines_deleted"": 23, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/loadbalance/extensions/BrokerRegistryIntegrationTest.java"", ""lines_added"": 124, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/loadbalance/extensions/BrokerRegistryTest.java"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/loadbalance/extensions/ExtensibleLoadManagerImplTest.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
pingcap/tidb,60105,panic: runtime error: invalid memory address or nil pointer dereference when do log restore only,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> log restore only  > /br  restore  point --storage s3://tmp/br-log-backup62f29bd7-1ff6-4d03-afe5-17a29ad33eb9/Log?access-key=xxx&secret-access-key=xxx&endpoint=http%3a%2f%2fminio.pingcap.net%3a9001 > &force-path-style=true --pd http://downstream-pd.brie-acceptance-pitr-tps-7831559-1-66:2379 --restored-ts 456704334025457782 --start-ts 456704078710046815 --concurrency=8 --check-requirements=false  ### 2. What did you expect to see? (Required) log restore success  ### 3. What did you see instead (Required)  > panic: runtime error: invalid memory address or nil pointer dereference > [signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x5ba3247] >  > goroutine 1 [running]: > github.com/pingcap/tidb/br/pkg/task.RunRestore({0x78731d0, 0xc0012411d0}, {0x7894bc0, 0xc001c966b0}, {0x6f26c84, 0xd}, 0xc0018d0f08) >         /workspace/source/tidb/br/pkg/task/restore.go:839 +0x847 > main.runRestoreCommand(0xc001bf0c08, {0x6f26c84, 0xd}) >         /workspace/source/tidb/br/cmd/br/restore.go:80 +0x739 > main.newStreamRestoreCommand.func1(0xc001bf0c08?, {0xc001c03ae0?, 0x4?, 0x6f08b81?}) >         /workspace/source/tidb/br/cmd/br/restore.go:254 +0x1f > github.com/spf13/cobra.(*Command).execute(0xc001bf0c08, {0xc00013c6b0, 0xa, 0xa}) >         /root/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:985 +0xaaa > github.com/spf13/cobra.(*Command).ExecuteC(0xc0018d2908) >         /root/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1117 +0x3ff > github.com/spf13/cobra.(*Command).Execute(...) >         /root/go/pkg/mod/github.com/spf13/cobra@v1.8.1/command.go:1041 > main.main() >         /workspace/source/tidb/br/cmd/br/main.go:37 +0x23a   br.log  > [2025/03/17 05:21:53.127 +00:00] [INFO] [stream.go:359] [""tsoStream.recvLoop ended""] [stream=downstream-pd-0.downstream-pd-peer.brie-acceptance-pitr-tps-7831559-1-66.svc:2379-3] [error=""rpc error: code = Canceled desc = context canceled""] [errorVerbose=""rpc error: code = Canceled desc = context canceled\\ngithub.com/tikv/pd/client/clients/tso.(*tsoStream).recvLoop\\n\\t/root/go/pkg/mod/github.com/tikv/pd/client@v0.0.0-20250305170641-bdd857e5503b/clients/tso/stream.go:427\\nruntime.goexit\\n\\t/root/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.7.linux-amd64/src/runtime/asm_amd64.s:1700""] > [2025/03/17 05:21:53.131 +00:00] [ERROR] [checkpoint.go:421] [""send the error""] [category=checkpoint] [error=""context canceled""] [stack=""github.com/pingcap/tidb/br/pkg/checkpoint.(*CheckpointRunner[...]).sendError\\n\\t/workspace/source/tidb/br/pkg/checkpoint/checkpoint.go:421\\ngithub.com/pingcap/tidb/br/pkg/checkpoint.(*CheckpointRunner[...]).startCheckpointMainLoop.func2\\n\\t/workspace/source/tidb/br/pkg/checkpoint/checkpoint.go:468""]  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() --> Release Version: v9.0.0-beta.1 Edition: Community Git Commit Hash: 7b4717bf180ec94d5d42dc4d5ca3711d34131370 Git Branch: HEAD UTC Build Time: 2025-03-13 03:08:08 GoVersion: go1.23.7 Race Enabled: false Check Table Before Drop: false Store: tikv  /br -V Release Version: v9.0.0-beta.1 Git Commit Hash: 7b4717bf180ec94d5d42dc4d5ca3711d34131370 Git Branch: HEAD Go Version: go1.23.7 UTC Build Time: 2025-03-13 03:09:50 Race Enabled: false ",2025-03-17T06:16:53+00:00,2025-03-17T14:29:37+00:00,3,https://github.com/pingcap/tidb/issues/60105,60116.0,2025-03-17T14:29:36+00:00,https://github.com/pingcap/tidb/pull/60116,0,2,3,5,16,25,0,23,8.211944444444445,type/bug;type/regression;severity/critical;component/br;affects-9.0,False,True,normal,security,"[{""filename"": ""br/pkg/conn/BUILD.bazel"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""br/pkg/conn/conn.go"", ""lines_added"": 0, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/task/restore.go"", ""lines_added"": 8, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""br/tests/br_full_ddl/run.sh"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""br/tests/br_incremental_ddl/run.sh"", ""lines_added"": 6, ""lines_deleted"": 6, ""file_type"": ""other""}]",,False
go-kratos/kratos,3492,fix(registry): add timeout validation,使用 Consul 做服务发现时，如果 gRPC Client 的 timeout 设置为 0，程序会报错 `discovery create watcher overtime`。  原因是：  1. gRPC Client 会将 `timeout` 传给 discovery；  https://github.com/go-kratos/kratos/blob/861493a20524217061cf329eff0b4e39fb9cbfeb/transport/grpc/client.go#L190-L196  2. discovery 的 `Build` 会因为 `timeout` 为 0，立马超时，得到 `ErrWatcherCreateTimeout` 的错误。  https://github.com/go-kratos/kratos/blob/861493a20524217061cf329eff0b4e39fb9cbfeb/transport/grpc/resolver/discovery/builder.go#L97-L102 ,2024-12-18T09:11:46+00:00,2024-12-18T10:20:03+00:00,1,https://github.com/go-kratos/kratos/pull/3492,3492.0,2024-12-18T10:20:03+00:00,https://github.com/go-kratos/kratos/pull/3492,0,3,0,3,22,12,0,34,1.1380555555555556,bug;LGTM;size:M,False,True,normal,networking,"[{""filename"": ""contrib/registry/consul/registry.go"", ""lines_added"": 11, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/resolver/discovery/builder.go"", ""lines_added"": 9, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/resolver/discovery/builder_test.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
pingcap/tidb,60112,"When a subquery contains a UNION and is executed, the requirement for subquery aliases is different with MySQL.","## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> CREATE TABLE `sbtest1` (   `id` int NOT NULL AUTO_INCREMENT,   `k` int NOT NULL DEFAULT '0',   `c` char(120) NOT NULL DEFAULT '',   `pad` char(60) NOT NULL DEFAULT '',   PRIMARY KEY (`id`) /*T![clustered_index] CLUSTERED */,   KEY `k_1` (`k`) ) ; CREATE TABLE `sbtest2` (   `id` int NOT NULL AUTO_INCREMENT,   `k` int NOT NULL DEFAULT '0',   `c` char(120) NOT NULL DEFAULT '',   `pad` char(60) NOT NULL DEFAULT '',   PRIMARY KEY (`id`) /*T![clustered_index] CLUSTERED */,   KEY `k_1` (`k`) ) ;  mysql> select * from (select * from sbtest1 where id=2  ) where id=100; ERROR 1248 (42000): Every derived table must have its own alias mysql> select * from (select * from sbtest1 where id=2  ) a where id=100; Empty set (0.00 sec)  mysql> select * from (select * from sbtest1 where id=2 union select * from sbtest2 ) where id=100; Empty set (0.00 sec)   ### 2. What did you expect to see? (Required)  report grammar error,like ERROR 1248 (42000): Every derived table must have its own alias.  ### 3. What did you see instead (Required) MySQL 8 behavior  mysql> select version(); +-----------+ | version() | +-----------+ | 8.4.2     | +-----------+ 1 row in set (0.01 sec)  mysql> select * from (select * from conflict_records_v1 union select * from conflict_records_v2 )   where id=100; ERROR 1248 (42000): Every derived table must have its own alias mysql> select * from (select * from conflict_records_v1 union select * from conflict_records_v2 ) a  where id=100; Empty set (0.01 sec)   ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  SELECT tidb_version()  ;  | tidb_version()                                                                                                                                                                                                                                 | Release Version: v8.5.1 Edition: Community Git Commit Hash: fea86c8e35ad4a86a5e1160701f99493c2ee547c Git Branch: HEAD UTC Build Time: 2025-01-16 07:38:34",2025-03-17T08:00:41+00:00,2025-03-19T06:29:10+00:00,0,https://github.com/pingcap/tidb/issues/60112,60159.0,2025-03-19T06:29:09+00:00,https://github.com/pingcap/tidb/pull/60159,0,4,4,8,26,8,0,10,46.47444444444445,type/bug;sig/planner;sig/sql-infra;severity/minor,False,True,normal,database,"[{""filename"": ""pkg/executor/test/issuetest/executor_issue_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/test/tiflashtest/tiflash_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/physical_plan_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/preprocess.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integrationtest/r/executor/executor.result"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/r/planner/core/issuetest/planner_issue.result"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/executor/executor.test"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/planner/core/issuetest/planner_issue.test"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}]",,False
apache/pulsar,20984,[fix][broker] Fix get outdated compactedTopicContext after compactionHorizon has been updated,"### Motivation  In #20697, we remove the lock of get compactedTopicContext and asyncReadEntries, but `compactionHorizon` has been updated before  `compactedTopicContext` is updated. In concurrent cases, this may lead to cursor skip messages of newly compacted or getting an NPE when reading compacted entries.  ### Modifications  Ensure `compactionHorizon` is updated after `compactedTopicContext`.   ### Verifying this change  - [x] Make sure that the change passes the CI checks.  *(Please pick either of the following options)*  This change is a trivial rework / code cleanup without any test coverage.  *(or)*  This change is already covered by existing tests, such as *(please describe tests)*.  *(or)*  This change added tests and can be verified as follows:  *(example:)*   - *Added integration tests for end-to-end deployment with large payloads (10MB)*   - *Extended integration test for recovery after broker failure*  ### Does this pull request potentially affect one of the following parts:  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  *If the box was checked, please highlight the changes*  - [ ] Dependencies (add or upgrade a dependency) - [ ] The public API - [ ] The schema - [ ] The default values of configurations - [ ] The threading model - [ ] The binary protocol - [ ] The REST endpoints - [ ] The admin CLI options - [ ] The metrics - [ ] Anything that affects deployment  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: <!-- ENTER URL HERE -->  <!-- After opening this PR, the build in apache/pulsar will fail and instructions will be provided for opening a PR in the PR author's forked repository.  apache/pulsar pull requests should be first tested in your own fork since the  apache/pulsar CI based on GitHub Actions has constrained resources and quota. GitHub Actions provides separate quota for pull requests that are executed in  a forked repository.  The tests will be run in the forked repository until all PR review comments have been handled, the tests pass and the PR is approved by a reviewer. --> ",2023-08-12T10:47:38+00:00,2023-08-16T16:09:40+00:00,1,https://github.com/apache/pulsar/pull/20984,20984.0,2023-08-16T16:09:40+00:00,https://github.com/apache/pulsar/pull/20984,0,2,0,2,74,4,0,78,101.36722222222222,type/bug;area/compaction;area/broker;doc-not-needed;ready-to-test;release/3.1.1;cherry-picked/branch-3.1;release/3.0.11,False,True,normal,configuration,"[{""filename"": ""pulsar-broker/src/main/java/org/apache/pulsar/compaction/CompactedTopicImpl.java"", ""lines_added"": 8, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/compaction/CompactedTopicTest.java"", ""lines_added"": 66, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,60037,auto analyze merge global statistics failed  when some partition statistics are missing,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  ```sql drop table if exists employees3;  CREATE TABLE employees3 (     emp_id INT NOT NULL,     emp_name VARCHAR(25) NOT NULL,     salary int NOT NULL,     dept_id INT NOT NULL,     PRIMARY KEY (emp_id) NONCLUSTERED ) PARTITION BY RANGE(emp_id)  (     PARTITION p0 VALUES LESS THAN (1000),     PARTITION p1 VALUES LESS THAN (4000),     PARTITION p2 VALUES LESS THAN (12000),     PARTITION p3 VALUES LESS THAN (16000),     PARTITION p4 VALUES LESS THAN (20000),     PARTITION p5 VALUES LESS THAN (25000),     PARTITION p6 VALUES LESS THAN (30000),     PARTITION p7 VALUES LESS THAN (35000),     PARTITION p8 VALUES LESS THAN (40000),     PARTITION p9 VALUES LESS THAN (45000),     PARTITION p10 VALUES LESS THAN (50000),     PARTITION p11 VALUES LESS THAN (55000),     PARTITION p12 VALUES LESS THAN (65000),     PARTITION p13 VALUES LESS THAN (75000),     PARTITION p14 VALUES LESS THAN (85000),     PARTITION p15 VALUES LESS THAN (95000),     PARTITION p16 VALUES LESS THAN (105000),     PARTITION p17 VALUES LESS THAN (115000),     PARTITION p18 VALUES LESS THAN (125000),     PARTITION pmax VALUES LESS THAN MAXVALUE );  set session cte_max_recursion_depth=1000000000;  insert into   employees3 WITH RECURSIVE EmployeeGenerator AS (     SELECT       101 AS emp_id,       'Emp00001' AS emp_name,       FLOOR(RAND() * (150000 - 50000) + 50000) AS salary,       FLOOR(RAND() * 3 + 1) AS dept_id     UNION ALL     SELECT       emp_id + 1,       CONCAT('Emp', LPAD(CAST(emp_id - 100 AS CHAR), 5, '0')),       FLOOR(RAND() * (150000 - 50000) + 50000),       FLOOR(RAND() * 3 + 1)     FROM       EmployeeGenerator     WHERE       emp_id < 20100   ) SELECT   * FROM   EmployeeGenerator;  //wait auto analyze   show analyze status;   +--------------+------------+----------------+-------------------------------------------------------------------------+----------------+---------------------+---------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------------+ | Table_schema | Table_name | Partition_name | Job_info                                                                | Processed_rows | Start_time          | End_time            | State    | Fail_reason                                                                                                                                                                                               | Instance       | Process_ID | +--------------+------------+----------------+-------------------------------------------------------------------------+----------------+---------------------+---------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------------+ | test         | employees3 |                | merge global stats for test.employees3's index primary                  |              0 | 2025-03-12 15:00:00 | 2025-03-12 15:00:00 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` index `primary`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3 columns                          |              0 | 2025-03-12 15:00:00 | 2025-03-12 15:00:00 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` column `emp_id`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 | p4             | auto analyze table all columns with 256 buckets, 500 topn, 1 samplerate |           4000 | 2025-03-12 15:00:00 | 2025-03-12 15:00:00 | finished | NULL                                                                                                                                                                                                      | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3's index primary                  |              0 | 2025-03-12 15:00:00 | 2025-03-12 15:00:00 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` index `primary`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3 columns                          |              0 | 2025-03-12 15:00:00 | 2025-03-12 15:00:00 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` column `emp_id`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 | p3             | auto analyze table all columns with 256 buckets, 500 topn, 1 samplerate |           4000 | 2025-03-12 14:59:59 | 2025-03-12 15:00:00 | finished | NULL                                                                                                                                                                                                      | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3's index primary                  |              0 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` index `primary`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3 columns                          |              0 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` column `emp_id`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 | p2             | auto analyze table all columns with 256 buckets, 500 topn, 1 samplerate |           8000 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | finished | NULL                                                                                                                                                                                                      | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3's index primary                  |              0 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` index `primary`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 |                | merge global stats for test.employees3 columns                          |              0 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | failed   | [types:8244]Build global-level stats failed due to missing partition-level column stats: table `employees3` partition `p0` column `emp_id`, please run analyze table to refresh columns of all partitions | 127.0.0.1:4000 |       NULL | | test         | employees3 | p1             | auto analyze table all columns with 256 buckets, 500 topn, 1 samplerate |           3000 | 2025-03-12 14:59:59 | 2025-03-12 14:59:59 | finished | NULL                                                                                                                                                                                                      | 127.0.0.1:4000 |       NULL | +--------------+------------+----------------+-------------------------------------------------------------------------+----------------+---------------------+---------------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+------------+ 12 rows in set (0.00 sec)   ```   ### 2. What did you expect to see? (Required)  merge global stats success  ### 3. What did you see instead (Required)  merge global stats failed  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  v6.5.11 ",2025-03-12T10:08:36+00:00,2025-03-13T04:29:03+00:00,0,https://github.com/pingcap/tidb/issues/60037,60035.0,2025-03-18T14:13:39+00:00,https://github.com/pingcap/tidb/pull/60035,0,2,0,2,52,0,0,52,148.08416666666668,type/bug;sig/planner;severity/major;affects-6.5;affects-7.1;report/customer,False,True,normal,database,"[{""filename"": ""statistics/handle/handle.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""statistics/handle/update_test.go"", ""lines_added"": 42, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3455,fix: return inner Logger from GetLogger for log.WithContext reuse,"fix: Ensure thread safety by eliminating lock copy in GetLogger  <img width=""710"" alt=""企业微信截图_d18e997f-a193-4d6d-9dc4-cb1ee9e86e4d"" src=""https://github.com/user-attachments/assets/357dbbff-21de-49e9-9bcb-63d587ae9580"">  <img width=""678"" alt=""企业微信截图_9782d329-af80-4d24-8f5d-611625c1c40e"" src=""https://github.com/user-attachments/assets/2dcf06f5-a6e3-4095-86f8-7c72ada9212c"">  <!-- 🎉 Thanks for sending a pull request to Kratos! Here are some tips for you:  1. If this is your first time contributing to Kratos, please read our contribution guide: https://go-kratos.dev/en/docs/community/contribution/ 2. Ensure you have added or ran the appropriate tests and lint for your PR, please use `make lint` and `make test` before filing your PR, use `make clean` to tidy your go mod. 3. If the PR is unfinished, you may need to mark it as a WIP(Work In Progress) PR or Draft PR 4. Please use a semantic commits format title, such as `<type>[optional scope]: <description>`, see: https://go-kratos.dev/docs/community/contribution#type 5. at the same time, please note that similar work should be submitted in one PR as far as possible to reduce the workload of reviewers. Do not split a work into multiple PR unless it should. -->  <!-- 🎉 感谢您向 Kratos 发送 PR！以下是一些提示： 如果这是你第一次为 Kratos 贡献，请阅读我们的贡献指南：https://go-kratos.dev/en/docs/community/contribution/ 2、确保您已经为您的 PR 添加或运行了适当的测试和lint，请在提交PR之前使用“make lint”和“make test”，使用“make clean”整理您的 go.mod。 3、如果 PR 未完成，您可能需要将其标记为 WIP（Work In Progress）PR 或 Draft PR 4、请使用语义提交格式标题，如“<类型>[可选范围]：<说明>`，请参阅：https://go-kratos.dev/docs/community/contribution#type 5. 同时请注意，同类的工作请尽量在一个PR中提交，以减轻 review 者的工作负担，不要把一项工作拆分成很多个PR，除非它应该这样做。 -->   #### Description (what this PR does / why we need it): <!-- * The description should include the motivation for this PR or contrast this with previous behavior -->   #### Which issue(s) this PR fixes (resolves / be part of): <!-- * Automatically closes linked issue when PR is merged. * If your PR is not fully resolved the issue, please use `part of #<issue number>` instead.  Usage: `fixes/resolves #<issue number>`, or `fixes/resolves (paste link of issue)`. -->   #### Other special notes for the reviewers: <!-- * Some things that need extra attention for the reviewers * Some additional notes, TODO list, etc. --> ",2024-10-31T09:17:18+00:00,2024-10-31T10:02:07+00:00,1,https://github.com/go-kratos/kratos/pull/3455,3455.0,2024-10-31T10:02:07+00:00,https://github.com/go-kratos/kratos/pull/3455,0,2,0,2,24,2,0,26,0.7469444444444444,bug;LGTM;size:S,False,True,normal,ui,"[{""filename"": ""log/global.go"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""log/global_test.go"", ""lines_added"": 20, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59827,Wrong Ranges for partition pruning,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  ```mysql DROP TABLE IF EXISTS t; CREATE TABLE `t` (   `a` varchar(150) NOT NULL,   `b` varchar(100) NOT NULL,   `c` int NOT NULL DEFAULT '0',   PRIMARY KEY (`a`,`b`) /*T![clustered_index] CLUSTERED */ ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci PARTITION BY LIST COLUMNS(`b`) (PARTITION `p0` VALUES IN ('0'),  PARTITION `p1` VALUES IN ('1'),  PARTITION `p2` VALUES IN ('2'));  explain analyze select * from t where a = 'b' and b = '2'; explain analyze select * from t where a = 'b' and (b = '2'); ```  https://github.com/pingcap/tidb/blob/75695928b955564835e1284dcdc2c7785cfe1d67/pkg/executor/point_get.go#L58-L65  Here a safe guard is added by #49161, trying to prune and see if it succeeded.  But when there is a pair of `()`, it will not go through the fast path, falling back to findbesttask.  In the slow path, before partition pruning processor rule, the juding paths is already generated by https://github.com/pingcap/tidb/blob/75695928b955564835e1284dcdc2c7785cfe1d67/pkg/planner/core/stats.go#L385-L417 ，which is using `DetachCondAndBuildRangeForIndex`.  While it should be processed by the pruning processor, and use `DetachCondAndBuildRangeForPartition` like https://github.com/pingcap/tidb/blob/75695928b955564835e1284dcdc2c7785cfe1d67/pkg/planner/core/rule_partition_processor.go#L737-L751.  This eventually leads to the failure of that safety guard, returning `table dual` instead.  ### 2. What did you expect to see? (Required) Both pruning results the same  ### 3. What did you see instead (Required) Different pruning results (with bad result as well, no rows would be found) ```mysql tidb> explain analyze select * from t where a = 'b' and b = '2'; +-------------+---------+---------+------+------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------+--------+------+ | id          | estRows | actRows | task | access object                                        | execution info                                                                               | operator info | memory | disk | +-------------+---------+---------+------+------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------+--------+------+ | Point_Get_1 | 1.00    | 0       | root | table:t, partition:p2, clustered index:PRIMARY(a, b) | time:35µs, open:375ns, close:1.96µs, loops:1, RU:0.47, Get:{num_rpc:1, total_time:6.42µs} |               | N/A    | N/A  | +-------------+---------+---------+------+------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------+--------+------+ 1 row in set (0.00 sec)  tidb> explain analyze select * from t where a = 'b' and (b = '2'); +-------------+---------+---------+------+--------------------------------------------------------+--------------------------------------------------------+---------------+--------+------+ | id          | estRows | actRows | task | access object                                          | execution info                                         | operator info | memory | disk | +-------------+---------+---------+------+--------------------------------------------------------+--------------------------------------------------------+---------------+--------+------+ | Point_Get_5 | 1.00    | 0       | root | table:t, partition:dual, clustered index:PRIMARY(a, b) | time:1.33µs, open:83ns, close:250ns, loops:1, RU:0.00 |               | N/A    | N/A  | +-------------+---------+---------+------+--------------------------------------------------------+--------------------------------------------------------+---------------+--------+------+ 1 row in set (0.00 sec) ```  ### 4. What is your TiDB version? (Required)  v8.5.1, but should affect all version after v8.0  ",2025-02-28T01:31:15+00:00,2025-03-12T01:56:04+00:00,9,https://github.com/pingcap/tidb/issues/59827,60161.0,2025-03-19T15:15:12+00:00,https://github.com/pingcap/tidb/pull/60161,0,7,3,10,376,57,0,374,469.7325,type/bug;type/regression;sig/sql-infra;severity/critical;component/tablepartition;affects-8.1;report/customer;affects-8.5;affects-9.0,False,True,normal,database,"[{""filename"": ""pkg/executor/builder.go"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/point_get.go"", ""lines_added"": 6, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/casetest/partition/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/planner/core/casetest/partition/partition_pruner_test.go"", ""lines_added"": 110, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/find_best_task.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/plan_cache_partition_table_test.go"", ""lines_added"": 77, ""lines_deleted"": 26, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/point_get_plan.go"", ""lines_added"": 107, ""lines_deleted"": 25, ""file_type"": ""app_code""}, {""filename"": ""pkg/server/conn.go"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/integrationtest/r/planner/core/casetest/partition/partition_pruner.result"", ""lines_added"": 38, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/planner/core/casetest/partition/partition_pruner.test"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""other""}]",pkg,False
go-kratos/kratos,3442,[Question] Middleware appears to use HTTP error codes even when the request was originally gRPC - is this intended?,"While working with the metrics middleware, we noticed that all the error codes showing up in Prometheus were HTTP status codes (400, 504, etc) even though only the gRPC endpoints were being exercised, which surprised us at first, but it looks like all the middlewares behave this way:  - Kratos errors are declared with an HTTP status code [[link](https://github.com/go-kratos/kratos/blob/main/errors/types.go#L6)] - And middlewares use this code field directly ([metrics](https://github.com/go-kratos/kratos/blob/main/middleware/metrics/metrics.go#L125), [logging](https://github.com/go-kratos/kratos/blob/main/middleware/logging/logging.go#L36))  Which left us wondering: is this intentional behavior? It seems like it could be meant to abstract logged error codes from the actual protocol used, maybe favoring HTTP because it's more widely recognized, but we wanted to reach out and check.  We also noticed that OK responses are logged as 0, which would be the correct status code for gRPC but is inconsistent with the HTTP ones and likely just an empty value. ",2024-10-15T18:52:25+00:00,2024-10-23T12:49:09+00:00,3,https://github.com/go-kratos/kratos/issues/3442,1717.0,,https://github.com/go-kratos/kratos/pull/1717,0,8,0,8,91,27,0,118,185.9455555555556,question,False,True,normal,functional,"[{""filename"": ""transport/grpc/client.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/pool.go"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/server.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/transport.go"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/http/client.go"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""transport/http/pool.go"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/http/server.go"", ""lines_added"": 9, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""transport/http/transport.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",grpc;http,True
pingcap/tidb,60144,"objstore: retry on ""server sent GOAWAY"" http2 error","<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #60143  Problem Summary:  ### What changed and how does it work?  Add ""server sent GOAWAY"" to retry error list, since gcp support recommends to retry on this error.  And see https://stackoverflow.com/questions/45209168/http2-server-sent-goaway-and-closed-the-connection-laststreamid-1999 for details.  ### Check List  Tests <!-- At least one of them must be included. -->  - [X] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-03-18T07:41:01+00:00,2025-03-18T08:28:52+00:00,4,https://github.com/pingcap/tidb/pull/60144,60144.0,2025-03-18T08:28:51+00:00,https://github.com/pingcap/tidb/pull/60144,0,2,0,2,3,0,0,3,0.7972222222222223,size/XS;release-note-none;approved;lgtm,False,True,normal,networking,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/gcs_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,60143,import task should retry on GCS error `server sent GOAWAY`,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  When importing data from GCS, sometimes we may encounter errors like `http2: server sent GOAWAY and closed the connection; LastStreamID=XXX, ErrCode=NO_ERROR, debug=""server_shutting_down""`, and this error can be resolved through retry.  But currently, this error is not included in the retry error list.  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  Subtask should retry after meeting this error.  ### 3. What did you see instead (Required)  Subtask failed.  ### 4. What is your TiDB version? (Required)  master  <!-- Paste the output of SELECT tidb_version() -->  ",2025-03-18T07:40:16+00:00,2025-03-18T08:28:52+00:00,1,https://github.com/pingcap/tidb/issues/60143,60144.0,2025-03-18T08:28:51+00:00,https://github.com/pingcap/tidb/pull/60144,0,2,0,2,3,0,0,3,0.8097222222222222,type/bug;severity/moderate,False,True,normal,networking,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/gcs_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,24001,"[Bug] Rate-Limiter fails with a huge spike in a traffic, and publish/consume stuck for a longer time.","### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  `> 3.x`  ### Minimal reproduce step  It sounds like your current token bucket rate-limiter suffers from issues due to eventual consistency, leading to a situation where a sudden traffic spike causes an excessive increase in `pendingConsumedTokens`, pushing the bucket into deep negative values. This, in turn, results in prolonged pauses in publish/dispatch operations.  **Fundamental Issues Identified:**  **1. Eventual Consistency Across Multiple Threads:**  Each thread maintains its own counter for `pendingConsumedTokens`, which gets [aggregated periodically](https://github.com/apache/pulsar/blob/master/pulsar-broker/src/main/java/org/apache/pulsar/broker/qos/AsyncTokenBucket.java#L192). During a spike, [multiple producers concurrently increase pendingConsumedTokens](https://github.com/apache/pulsar/blob/master/pulsar-broker/src/main/java/org/apache/pulsar/broker/qos/AsyncTokenBucket.java#L210), leading to an aggregated count that significantly exceeds the configured rate.  **2. Deep Negative Token Count Problem:**  Since tokens are refreshed at fixed intervals, a large negative value requires multiple refresh cycles to recover. This leads to long pauses, causing the publish/dispatch system to get stuck.  ![Image](https://github.com/user-attachments/assets/4b8e55a3-1e8f-4fa5-aac8-88156280b36a)  **3. Previous RateLimiter Had a Controlled Spike Handling:**  [Previous Rate Limiter](https://github.com/apache/pulsar/pull/634) was able to handle the spike and it doesn't; allow topic or dispatcher to stuck without dispatching messages.  Alternative: **Bound Negative Token Values**: Implement a threshold to prevent tokens from going excessively negative, limiting recovery time.  Let's step back and fix this issue fundamentally without adding any more patches, and we should at least revert to Dispatch Rate-Limiter until we have a matured solution in place because the current Rate-Limiter has certainly changed the behavior when the broker hits the spike in dispatch or publish.  This can be reproduced with below test   ``` void testAsyncToken() throws Exception {         int rate = 2000;         int resolutionTimeNano = 8;         asyncTokenBucket = AsyncTokenBucket.builder().rate(rate).ratePeriodNanos(TimeUnit.SECONDS.toNanos(1)).clock(                 new DefaultMonotonicSnapshotClock(TimeUnit.MILLISECONDS.toNanos(resolutionTimeNano), System::nanoTime))                 .build();          for (int j = 0; j < (1000); j++) {             for (int i = 0; i < (1000); i++) {                 long token = asyncTokenBucket.getTokens();                 if (token < 0) {                     // sleep to allow add more tokens                     Thread.sleep(resolutionTimeNano * 5);                     assertTrue(asyncTokenBucket.getTokens() > 0);                 }                 // calling consumeTokens iteratively to simulate calling this method multiple times from multiple                 // threads                 asyncTokenBucket.consumeTokens(100);             }         }     } ```    ### What did you expect to see?  Rate-Limiter should not be blocked for long time but like previous Rate-Limiter it should handle spike and allow configured rate at every second(or configured rate time)  ### What did you see instead?  Rate-Limiter should blocks publish/consume for long time  ### Anything else?  _No response_  ### Are you willing to submit a PR?  - [x] I'm willing to submit a PR!",2025-02-19T08:19:06+00:00,2025-02-25T09:30:51+00:00,1,https://github.com/apache/pulsar/issues/24001,24002.0,,https://github.com/apache/pulsar/pull/24002,0,2,0,2,30,3,0,33,145.19583333333333,type/bug,False,True,major,configuration,"[{""filename"": ""pulsar-broker/src/main/java/org/apache/pulsar/broker/qos/AsyncTokenBucket.java"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/qos/AsyncTokenBucketTest.java"", ""lines_added"": 24, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3401,kratos new helloworld - does not work,"```kratos new helloworld ```                                                                                                                                                   🚀 Creating service helloworld, layout repo is https://github.com/go-kratos/kratos-layout.git, please wait a moment.  ERROR: Failed to create project(chdir /Users/alexander./.kratos/repo/github.com/go-kratos/kratos-layout@main: permission denied)  Do you have another options for create new project (without access to external source)?",2024-08-20T17:47:42+00:00,2024-08-20T17:57:05+00:00,0,https://github.com/go-kratos/kratos/issues/3401,3267.0,,https://github.com/go-kratos/kratos/pull/3267,0,2,0,2,33,0,0,33,0.1563888888888888,bug,False,True,normal,security,"[{""filename"": ""log/log.go"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""log/log_test.go"", ""lines_added"": 27, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3364,chore: replace the DisableDebugLog method with the PrintDebugLog method,"DisableDebugLog is marked as ""Deprecated: please use PrintDebugLog""",2024-07-12T08:51:34+00:00,2024-08-19T02:56:35+00:00,0,https://github.com/go-kratos/kratos/pull/3364,3364.0,2024-08-19T02:56:34+00:00,https://github.com/go-kratos/kratos/pull/3364,0,1,0,1,1,1,0,2,906.0833333333334,LGTM;size:XS,False,True,normal,functional,"[{""filename"": ""transport/grpc/resolver/discovery/builder_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9235,[Bug] Response codes do not match,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [x] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [x] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  ubuntu  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  ![Image](https://github.com/user-attachments/assets/fd42fdfb-3df4-46c0-a325-5f12e659bc98) Calling the mqadmin interface passes in a non-existent subscription group, but returns a mismatched response code  ### Steps to Reproduce  A non-existent subscription group is used, but the response returned is that the consumer is not online  ### What Did You Expect to See?  Return Subscription group does not exist  ### What Did You See Instead?  null  ### Additional Context  _No response_",2025-03-11T08:42:57+00:00,2025-03-14T06:58:26+00:00,0,https://github.com/apache/rocketmq/issues/9235,9236.0,,https://github.com/apache/rocketmq/pull/9236,0,2,0,2,10,0,0,10,70.25805555555556,,False,True,normal,ui,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessor.java"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""broker/src/test/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessorTest.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,54552,Achieving eventual consistency for statistics synchronization is unattainable.,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  When you have two tidb instance, if you run ```analyze``` on the one tidb.  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  both of tidb can see this stats. ### 3. What did you see instead (Required)  the other tidb instance can be unable to see these stats.  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ",2024-07-10T09:49:37+00:00,2025-03-17T11:21:59+00:00,2,https://github.com/pingcap/tidb/issues/54552,60124.0,,https://github.com/pingcap/tidb/pull/60124,0,12,1,13,1084,16,0,1092,6001.539444444445,type/bug;sig/planner;component/statistics;severity/major;affects-6.5;affects-7.1;affects-7.5;affects-8.1;report/community;affects-8.5,False,True,normal,ui,"[{""filename"": ""pkg/executor/analyze.go"", ""lines_added"": 7, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/analyze_worker.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/test/analyzetest/analyze_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/cache/statscache.go"", ""lines_added"": 35, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/ddl/subscriber.go"", ""lines_added"": 655, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/globalstats/global_stats.go"", ""lines_added"": 32, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/BUILD.bazel"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/statistics/handle/storage/dump_test.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/save.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/stats_read_writer.go"", ""lines_added"": 124, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/stats_read_writer_test.go"", ""lines_added"": 150, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/update.go"", ""lines_added"": 51, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/types/interfaces.go"", ""lines_added"": 9, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,2825,google.protobuf.Empty not generated correctly,"#### What happened: ```proto syntax = ""proto3"";  package helloworld.v1;  import ""google/api/annotations.proto""; import ""google/protobuf/empty.proto"";  option go_package = ""x/api/helloworld/v1;v1""; option java_multiple_files = true; option java_package = ""dev.kratos.api.helloworld.v1""; option java_outer_classname = ""HelloworldProtoV1"";  // The greeting service definition. service Greeter {   // Sends a greeting   rpc SayHello (HelloRequest) returns (google.protobuf.Empty) {     option (google.api.http) = {       get: ""/helloworld/{name}""     };   } }  // The request message containing the user's name. message HelloRequest {   string name = 1; }  // The response message containing the greetings message HelloReply {   string message = 1; } ```  ```go package service  import ( 	""context""  	pb ""x/api/helloworld/v1"" )  type GreeterService struct { 	pb.UnimplementedGreeterServer }  func NewGreeterService() *GreeterService { 	return &GreeterService{} }  func (s *GreeterService) SayHello(ctx context.Context, req *pb.HelloRequest) (*pb.google_protobuf_Empty, error) { 	return &pb.google_protobuf_Empty{}, nil }  ```  Using `kratos proto server` to generate server stub, it doesn't recognize google.protobuf.empty  #### What you expected to happen: `google.protobuf.Empty` to be correctly recognized and use `empty.pb` instead.  #### How to reproduce it (as minimally and precisely as possible): Run -- kratos new helloworld Modify api/helloworld/v1/greeter.proto to above Remove internal/service/greeter.go and run `kratos proto server ./api/helloworld/v1/greeter.proto`  #### Anything else we need to know?: It was previously working, perhaps my local environment messed up some configs with kratos.  #### Environment: - Kratos version (use `kratos -v`): kratos version v2.6.2 - Go version (use `go version`): go version go1.20.3 linux/amd64 - OS (e.g: `cat /etc/os-release`): Debian GNU/Linux 12 (bookworm) - Others:   - protoc-gen-go-http --version: protoc-gen-go-http v2.6.2   - protoc-gen-go --version: protoc-gen-go v1.28.0   - protoc-gen-go-grpc --version: protoc-gen-go-grpc 1.3.0 ",2023-05-10T04:15:51+00:00,2023-06-29T05:18:37+00:00,1,https://github.com/go-kratos/kratos/issues/2825,1717.0,,https://github.com/go-kratos/kratos/pull/1717,0,8,0,8,91,27,0,118,1201.046111111111,bug,False,True,normal,configuration,"[{""filename"": ""transport/grpc/client.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/pool.go"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/server.go"", ""lines_added"": 8, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""transport/grpc/transport.go"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/http/client.go"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""transport/http/pool.go"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/http/server.go"", ""lines_added"": 9, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""transport/http/transport.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",grpc;http,True
apache/pulsar,23859,[fix][client] fix retry topic with exclusive mode.,"### Motivation  - Retry topic relies on the delayed queue feature. Once user call `reconsumeLater` for a message, this message will be produce as a delayed message to the corresponding retry topic.  - Delayed queue feature can only work with shared/key-shared subscription mode, or the delayed message will be dispatched immediately with exclusive/failover mode.  Based on the analysis above, we can come to the conclusion that the consumer of the retry topic must be shared/key-shared.  ### Modifications  Restrict the subscription type of the retry topic to be shared.  ### Verifying this change  - [x] Make sure that the change passes the CI checks.  *(Please pick either of the following options)*  This change added tests and can be verified as follows:  *(example:)*   - *Added integration tests for end-to-end deployment with large payloads (10MB)*   - *Extended integration test for recovery after broker failure*  ### Does this pull request potentially affect one of the following parts:  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  *If the box was checked, please highlight the changes*  - [ ] Dependencies (add or upgrade a dependency) - [ ] The public API - [ ] The schema - [ ] The default values of configurations - [ ] The threading model - [ ] The binary protocol - [ ] The REST endpoints - [ ] The admin CLI options - [ ] The metrics - [ ] Anything that affects deployment  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [x] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [ ] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository:  ",2025-01-17T06:39:21+00:00,2025-02-18T09:59:52+00:00,5,https://github.com/apache/pulsar/pull/23859,23859.0,2025-02-18T09:59:52+00:00,https://github.com/apache/pulsar/pull/23859,0,2,0,2,48,0,0,48,771.3419444444445,type/bug;doc-required;ready-to-test;cherry-picked/branch-3.0;cherry-picked/branch-3.3;cherry-picked/branch-4.0;release/3.0.10;release/3.3.5;release/4.0.3,False,True,normal,configuration,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/RetryTopicTest.java"", ""lines_added"": 40, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ConsumerBase.java"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",client,False
apache/rocketmq,8997,[Bug] 一次请求打到坏的broker把超时时间耗光，后两次打到正常的broker也因过期时间为0，无法成功,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  linux and windows  ### RocketMQ version  4.7.1   ### JDK Version  jdk11  ### Describe the Bug  <img width=""580"" alt=""b2489c47defcd7426ba819008233748"" src=""https://github.com/user-attachments/assets/f9956b46-f3c0-45e0-8969-110e50bbcf16"">  broker-f 突然内存持续上涨，导致服务假死，也未从nameserver摘掉，导致客户端连接超时，虽然mq有三次重试，通过上图发现后两次请求到了正常的broker，应该成功，结果还是失败了  <img width=""588"" alt=""7a5b9cf09a1974768c6bf495e0f801c"" src=""https://github.com/user-attachments/assets/6bc4a795-b5de-4141-8300-ec781581dee6"">   ### Steps to Reproduce  现在我们无法复现broker内存上涨假死的情况，可以制造一个broker访问不通使得连接把耗时耗光  ### What Did You Expect to See?  期望的是，单个请求超时和整体三次重试的超时分开配置  ### What Did You See Instead?  期望的是，单个请求超时和整体三次重试的超时分开配置  ### Additional Context  无",2024-11-28T07:18:50+00:00,2025-03-10T08:03:12+00:00,3,https://github.com/apache/rocketmq/issues/8997,9137.0,2025-03-10T08:03:11+00:00,https://github.com/apache/rocketmq/pull/9137,0,2,0,2,23,2,0,25,2448.7391666666667,,False,True,normal,functional,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/producer/DefaultMQProducerImpl.java"", ""lines_added"": 10, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/producer/DefaultMQProducer.java"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3355,bug in Quick Start,"<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened: when I run the tutorial in https://go-kratos.dev/en/docs/getting-started/start, it comes out an error: $ go generate ./... ```shell panic: runtime error: invalid memory address or nil pointer dereference [recovered]         panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x58b24f]  goroutine 60 [running]: go/types.(*Checker).handleBailout(0xc0001f6600, 0xc000333bd0)         /home/gitpod/go/src/go/types/check.go:367 +0x88 panic({0x69a640?, 0x8be710?})         /home/gitpod/go/src/runtime/panic.go:770 +0x132 go/types.(*StdSizes).Sizeof(0x0, {0x747128, 0x8c2c40})         /home/gitpod/go/src/go/types/sizes.go:228 +0x30f go/types.(*Config).sizeof(...)         /home/gitpod/go/src/go/types/sizes.go:333 go/types.representableConst.func1({0x747128?, 0x8c2c40?})         /home/gitpod/go/src/go/types/const.go:76 +0x9e go/types.representableConst({0x748d60, 0x8b7000}, 0xc0001f6600, 0x8c2c40, 0xc000331fe8)         /home/gitpod/go/src/go/types/const.go:92 +0x192 go/types.(*Checker).representation(0xc0001f6600, 0xc00009d040, 0x8c2c40)         /home/gitpod/go/src/go/types/const.go:256 +0x65 go/types.(*Checker).implicitTypeAndValue(0xc0001f6600, 0xc00009d040, {0x747128, 0x8c2c40})         /home/gitpod/go/src/go/types/expr.go:375 +0x2d7 go/types.(*Checker).convertUntyped(0xc0001f6600, 0xc00009d040, {0x747128, 0x8c2c40})         /home/gitpod/go/src/go/types/const.go:289 +0x3f go/types.(*Checker).matchTypes(0xc0001f6600, 0xc00009d000, 0xc00009d040)         /home/gitpod/go/src/go/types/expr.go:926 +0x79 go/types.(*Checker).binary(0xc0001f6600, 0xc00009d000, {0x747e18, 0xc00031c0f0}, {0x747ea8, 0xc00007e1e0}, {0x7483e8, 0xc00007e200}, 0x28, 0x133)         /home/gitpod/go/src/go/types/expr.go:800 +0x166 go/types.(*Checker).exprInternal(0xc0001f6600, 0x0, 0xc00009d000, {0x747e18, 0xc00031c0f0}, {0x0, 0x0})         /home/gitpod/go/src/go/types/expr.go:1416 +0x206 go/types.(*Checker).rawExpr(0xc0001f6600, 0x0, 0xc00009d000, {0x747e18?, 0xc00031c0f0?}, {0x0?, 0x0?}, 0x0)         /home/gitpod/go/src/go/types/expr.go:979 +0x19e go/types.(*Checker).expr(0xc0001f6600, 0x7474c0?, 0xc00009d000, {0x747e18?, 0xc00031c0f0?})         /home/gitpod/go/src/go/types/expr.go:1513 +0x30 go/types.(*Checker).stmt(0xc0001f6600, 0x0, {0x748208, 0xc00009c2c0})         /home/gitpod/go/src/go/types/stmt.go:570 +0x11f2 go/types.(*Checker).stmtList(0xc0001f6600, 0x0, {0xc00007e360?, 0x0?, 0x0?})         /home/gitpod/go/src/go/types/stmt.go:121 +0x85 go/types.(*Checker).funcBody(0xc0001f6600, 0x747128?, {0xc00030c068?, 0x8c2e20?}, 0xc00009cd40, 0xc00031c180, {0x0?, 0x0?})         /home/gitpod/go/src/go/types/stmt.go:41 +0x331 go/types.(*Checker).funcDecl.func1()         /home/gitpod/go/src/go/types/decl.go:852 +0x3a go/types.(*Checker).processDelayed(0xc0001f6600, 0x0)         /home/gitpod/go/src/go/types/check.go:467 +0x162 go/types.(*Checker).checkFiles(0xc0001f6600, {0xc000316008, 0x1, 0x1})         /home/gitpod/go/src/go/types/check.go:411 +0x1cc go/types.(*Checker).Files(...)         /home/gitpod/go/src/go/types/check.go:372 golang.org/x/tools/go/packages.(*loader).loadPackage(0xc000182000, 0xc00009eab0)         /workspace/go/pkg/mod/golang.org/x/tools@v0.6.0/go/packages/packages.go:1052 +0xa72 golang.org/x/tools/go/packages.(*loader).loadRecursive.func1()         /workspace/go/pkg/mod/golang.org/x/tools@v0.6.0/go/packages/packages.go:851 +0x1a9 sync.(*Once).doSlow(0x0?, 0x0?)         /home/gitpod/go/src/sync/once.go:74 +0xc2 sync.(*Once).Do(...)         /home/gitpod/go/src/sync/once.go:65 golang.org/x/tools/go/packages.(*loader).loadRecursive(0x0?, 0x0?)         /workspace/go/pkg/mod/golang.org/x/tools@v0.6.0/go/packages/packages.go:839 +0x4a golang.org/x/tools/go/packages.(*loader).loadRecursive.func1.1(0x0?)         /workspace/go/pkg/mod/golang.org/x/tools@v0.6.0/go/packages/packages.go:846 +0x26 created by golang.org/x/tools/go/packages.(*loader).loadRecursive.func1 in goroutine 59         /workspace/go/pkg/mod/golang.org/x/tools@v0.6.0/go/packages/packages.go:845 +0x94 exit status 2 cmd/helloworld/wire_gen.go:3: running ""go"": exit status 1 ```  #### What you expected to happen:  #### How to reproduce it (as minimally and precisely as possible): just run follow official tutorial #### Anything else we need to know?:  #### Environment: - Kratos version (use `kratos -v`): - Go version (use `go version`):1.22.2 - OS (e.g: `cat /etc/os-release`):linux/amd64 - Others: ",2024-06-25T06:08:25+00:00,2024-06-25T07:53:28+00:00,1,https://github.com/go-kratos/kratos/issues/3355,2904.0,2023-07-06T11:59:03+00:00,https://github.com/go-kratos/kratos/pull/2904,0,2,0,2,14,0,0,14,-8514.156111111111,bug,False,True,normal,configuration,"[{""filename"": ""transport/grpc/server.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""transport/http/server.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",grpc;http,True
apache/rocketmq,9125,[Bug] Fixed array overreach caused by site error when reading messages from tiered storage,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  ubuntu/centos  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  When searching messages in tiered storage, the array out-of-bounds exception caused by the absence of the corresponding site is not captured and processed ![1736603231353](https://github.com/user-attachments/assets/61e48cf3-afba-4503-8fb8-4d60c159b7cc)   ### Steps to Reproduce  null  ### What Did You Expect to See?  correct handle it  ### What Did You See Instead?  null  ### Additional Context  _No response_",2025-01-11T14:04:28+00:00,2025-03-04T08:00:25+00:00,0,https://github.com/apache/rocketmq/issues/9125,9126.0,,https://github.com/apache/rocketmq/pull/9126,0,2,0,2,8,0,0,8,1241.9325,,False,True,normal,functional,"[{""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/file/FlatAppendFile.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/file/FlatCommitLogFile.java"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",file,False
go-kratos/kratos,3226,"When the App stops, the ctx in the afterStop callback is not available","<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened:  `afterStopFn` 接收基于 App 的 `ctx` 衍生的  `sctx` , 但是在接收停止信号时，就会取消 App 的 `ctx` ，导致 `sctx` 也会被取消，`afterStopFn` 无法执行（如：需要推送系统停止的消息等）  参考:   - https://github.com/go-kratos/kratos/blob/50cad79c96ae6799eff87b8534c58a4a0bb672b1/app.go#L144 - https://github.com/go-kratos/kratos/blob/50cad79c96ae6799eff87b8534c58a4a0bb672b1/app.go#L166  是否应该像 `stopSrv` 的方式，可设置应用停止后，回调函数可设置执行超时，基于 `o.ctx` 创建，不依赖 App.ctx ？？  https://github.com/go-kratos/kratos/blob/50cad79c96ae6799eff87b8534c58a4a0bb672b1/app.go#L105   #### What you expected to happen:  #### How to reproduce it (as minimally and precisely as possible):  #### Anything else we need to know?:  #### Environment: - Kratos version (use `kratos -v`):  - Go version (use `go version`): - OS (e.g: `cat /etc/os-release`): - Others: ",2024-03-09T10:42:07+00:00,2024-06-15T16:01:03+00:00,3,https://github.com/go-kratos/kratos/issues/3226,2365.0,,https://github.com/go-kratos/kratos/pull/2365,0,5,0,5,83,24,0,107,2357.3155555555554,bug,False,True,normal,functional,"[{""filename"": ""log/filter_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""log/global_test.go"", ""lines_added"": 18, ""lines_deleted"": 16, ""file_type"": ""app_code""}, {""filename"": ""log/log_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""log/std.go"", ""lines_added"": 62, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""log/std_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8674,[Bug] Fix the correct consumption pattern when the heartbeat is reported,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  When the heartbeat is reported, the calling function only contains two modes: push/pull, so the judgment of pop consumption mode should be added to the logic of pushImpl, so that the correct consumption mode can be uploaded during the heartbeat ![图片](https://github.com/user-attachments/assets/386ef254-23dc-415b-ab9c-a2cb02d2da31) ![图片](https://github.com/user-attachments/assets/32ec6b4c-53e0-44d3-8528-137a47d4ad06)   ### Steps to Reproduce  null  ### What Did You Expect to See?  null  ### What Did You See Instead?  null  ### Additional Context  _No response_",2024-09-10T09:10:01+00:00,2025-03-04T07:53:29+00:00,0,https://github.com/apache/rocketmq/issues/8674,8675.0,,https://github.com/apache/rocketmq/pull/8675,0,3,0,3,25,0,0,25,4198.724444444444,,False,True,normal,functional,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultMQPushConsumerImpl.java"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/RebalanceImpl.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/RebalancePushImpl.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9217,[Bug] Broker's inflight and available message counts are incorrect when the pop consumer service is enabled,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [x] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [x] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Linux 5.10  ### RocketMQ version  develop branch | before 5.3.2-SNAPSHOT  ### JDK Version  JDK11  ### Describe the Bug  Broker inflight and available messsage count wrong when pop consumer service enable  ### Steps to Reproduce  Enable the pop kv feature  ![Image](https://github.com/user-attachments/assets/2701c445-ffe0-47ea-bfaa-152beab81985)  ### What Did You Expect to See?  ![Image](https://github.com/user-attachments/assets/831b2d4c-7959-404c-b754-c562d3297375)  ### What Did You See Instead?  inflight and available messsage count wrong  ### Additional Context  inflight and available messsage count wrong",2025-03-03T03:39:25+00:00,2025-03-03T06:24:03+00:00,0,https://github.com/apache/rocketmq/issues/9217,9218.0,2025-03-03T06:24:02+00:00,https://github.com/apache/rocketmq/pull/9218,0,1,0,1,3,6,0,9,2.743611111111111,,False,True,normal,functional,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/metrics/ConsumerLagCalculator.java"", ""lines_added"": 3, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
pingcap/tidb,33174,Wrong error on SET DEFAULT ROLE with empty hostnames,"## Bug Report  ### 1. Minimal reproduce step  ```sql DROP USER IF EXISTS 'u1'@''; DROP ROLE IF EXISTS 'r1'@'', 'r2'@'';  CREATE USER 'u1'@''; CREATE ROLE 'r1'@'', 'r2'@''; GRANT 'r1'@'' TO 'u1'@''; GRANT 'r2'@'' TO 'u1'@''; SET DEFAULT ROLE 'r1'@'', 'r2'@'' TO 'u1'@'';  REVOKE 'r1'@'' FROM 'u1'@''; REVOKE 'r2'@'' FROM 'u1'@''; DROP USER 'u1'@''; DROP ROLE  'r1'@'', 'r2'@''; ```  ### 2. What did you expect to see?  ```sql mysql> SET DEFAULT ROLE 'r1'@'', 'r2'@'' TO 'u1'@''; Query OK, 0 rows affected (0.00 sec) ```  ### 3. What did you see instead  ```sql tidb> SET DEFAULT ROLE 'r1'@'', 'r2'@'' TO 'u1'@''; ERROR 3530 (HY000): `r1`@`` is not granted to u1@% ```  ### 4. What is your TiDB version?  ``` tidb_version(): Release Version: v6.0.0-alpha-23-g7c69e74bd Edition: Community Git Commit Hash: 7c69e74bd9dad30dc9d724dc9d0b3fc759973b40 Git Branch: master UTC Build Time: 2022-03-16 20:54:29 GoVersion: go1.17.2 Race Enabled: false TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306 Check Table Before Drop: false ``` ",2022-03-16T23:54:46+00:00,2025-03-17T03:12:56+00:00,0,https://github.com/pingcap/tidb/issues/33174,60082.0,2025-03-17T03:12:54+00:00,https://github.com/pingcap/tidb/pull/60082,0,3,0,3,8,15,0,23,26307.30222222222,type/bug;sig/sql-infra;severity/moderate,False,True,normal,database,"[{""filename"": ""pkg/executor/grant.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/simple.go"", ""lines_added"": 0, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""pkg/privilege/privileges/cache.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9213,[Bug] Fix get the earliest time error when data is clean up in tiered storage ,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [x] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [x] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  [Bug] Fix get earliest time error when data is clean up in tiered storage  ### RocketMQ version  before develop 5.3.2-SNAPSHOT  ### JDK Version  JDK11  ### Describe the Bug  Get earliest time error when data is clean up in tiered storage  ### Steps to Reproduce  Get earliest time error when data is clean up in tiered storage  ![Image](https://github.com/user-attachments/assets/8621e228-296b-4a11-84f3-a1d6c193d4f4)  ### What Did You Expect to See?  No error  ### What Did You See Instead?  Get earliest time success  ### Additional Context  _No response_",2025-02-28T03:29:35+00:00,2025-03-03T01:58:07+00:00,0,https://github.com/apache/rocketmq/issues/9213,9214.0,2025-03-03T01:58:06+00:00,https://github.com/apache/rocketmq/pull/9214,0,4,0,4,22,27,0,49,70.47527777777778,,False,True,normal,database,"[{""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/TieredMessageStore.java"", ""lines_added"": 12, ""lines_deleted"": 14, ""file_type"": ""app_code""}, {""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/core/MessageStoreFetcherImpl.java"", ""lines_added"": 1, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/file/FlatMessageFile.java"", ""lines_added"": 8, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""tieredstore/src/test/java/org/apache/rocketmq/tieredstore/TieredMessageStoreTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,54700,Default value as an expression really only supports default value as function,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  From [the MySQL manual](https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html) there is an example that uses a literal default specified as an expression: ```sql CREATE TABLE t2 (b BLOB DEFAULT ('abc')); ```  In my actual application, I have a similar example where the literal is `'0'`. If you test in MySQL you can also use expressions like `DEFAULT(NOW()+2)`, but these are also parse errors in TiDB.  ### 2. What did you expect to see? (Required)  ``` mysql> CREATE TABLE t2 (b BLOB DEFAULT ('abc')); Query OK, 0 rows affected (0.03 sec)  mysql> SELECT version(); +-----------+ | version() | +-----------+ | 8.0.32    | +-----------+ 1 row in set (0.01 sec) ```  ### 3. What did you see instead (Required)  ``` mysql> CREATE TABLE t2 (b BLOB DEFAULT ('abc')); ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your TiDB version for the right syntax to use line 1 column 38 near ""'abc'))"" ```  ### 4. What is your TiDB version? (Required)  ``` mysql> SELECT tidb_version()\\G *************************** 1. row *************************** tidb_version(): Release Version: v8.2.0-alpha-602-g9794156596 Edition: Community Git Commit Hash: 9794156596b1bbd1270f7776fb5bcad59267c25d Git Branch: HEAD UTC Build Time: 2024-07-17 02:29:36 GoVersion: go1.21.10 Race Enabled: false Check Table Before Drop: false Store: tikv 1 row in set (0.00 sec) ``` ",2024-07-18T02:07:11+00:00,2025-03-15T22:17:53+00:00,2,https://github.com/pingcap/tidb/issues/54700,54706.0,,https://github.com/pingcap/tidb/pull/54706,0,2,2,4,6550,6506,0,13046,5780.178333333333,type/bug;sig/sql-infra;severity/moderate,False,True,normal,database,"[{""filename"": ""pkg/expression/integration_test/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/expression/integration_test/integration_test.go"", ""lines_added"": 23, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/parser/parser.go"", ""lines_added"": 6518, ""lines_deleted"": 6505, ""file_type"": ""app_code""}, {""filename"": ""pkg/parser/parser.y"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
go-kratos/kratos,3174,Unable to customize http.ResponseWriter because of wrong implementation of Kratos' http.wrapper.Result(),"<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened:  i implemented `ResponseWriter` of `net/http` and use it in a custom Filter, but in my custom ResponseEncoder, the type assertion code below will fail:  `customResponseWriter := w.(*CustomResponseWriter)`  finally, i found that, in Kratos' `http.wrapper.Result()`, `wrapper.res` (which is right actually my custom ResponseWriter) is not used, instead, `wrapper.w` is used:  ``` func (c *wrapper) Result(code int, v interface{}) error { 	c.w.WriteHeader(code) 	return c.router.srv.enc(&c.w, c.req, v) } ```  and what's more, we can see any other method of the wrapper, like `JSON()` or `XML()`, they just use `wrapper.res`:  ``` func (c *wrapper) JSON(code int, v interface{}) error { 	c.res.Header().Set(""Content-Type"", ""application/json"") 	c.res.WriteHeader(code) 	return json.NewEncoder(c.res).Encode(v) }  func (c *wrapper) XML(code int, v interface{}) error { 	c.res.Header().Set(""Content-Type"", ""application/xml"") 	c.res.WriteHeader(code) 	return xml.NewEncoder(c.res).Encode(v) } ```  so, i'm wondering, the `Result()` method of `wrapper` should use `wrapper.res` too, just like other methods.  #### What you expected to happen:  `http.wrapper.Result()` changed to:  ``` func (c *wrapper) Result(code int, v interface{}) error { 	c.res.WriteHeader(code) 	return c.router.srv.enc(c.res, c.req, v) } ```  #### How to reproduce it (as minimally and precisely as possible):  #### Anything else we need to know?:  #### Environment: - Kratos version (use `kratos -v`): 2.1 - Go version (use `go version`): 1.20 - OS (e.g: `cat /etc/os-release`): macOS - Others: ",2024-01-27T04:09:38+00:00,2024-04-24T16:31:15+00:00,4,https://github.com/go-kratos/kratos/issues/3174,3189.0,,https://github.com/go-kratos/kratos/pull/3189,0,2,0,2,87,27,0,114,2124.360277777778,bug,False,True,normal,configuration,"[{""filename"": ""transport/http/context.go"", ""lines_added"": 11, ""lines_deleted"": 16, ""file_type"": ""app_code""}, {""filename"": ""transport/http/context_test.go"", ""lines_added"": 76, ""lines_deleted"": 11, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7859,[Bug]  Logic for checking MessageConst.PROPERTY_CRC32 in CommitLog#asyncPutMessage has been removed due to a logic error,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  windows  ### RocketMQ version  master  ### JDK Version  jdk8  ### Describe the Bug  ![image](https://github.com/apache/rocketmq/assets/15797831/8ce584f7-2850-4f58-80b8-147be87c7b27)   ### Steps to Reproduce  code   ### What Did You Expect to See?  remove  ### What Did You See Instead?  add  ### Additional Context  _No response_",2024-02-26T14:56:50+00:00,2025-03-01T00:11:42+00:00,2,https://github.com/apache/rocketmq/issues/7859,7860.0,,https://github.com/apache/rocketmq/pull/7860,0,1,0,1,1,1,0,2,8841.247777777779,stale,False,True,normal,performance,"[{""filename"": ""store/src/main/java/org/apache/rocketmq/store/CommitLog.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/pulsar,23952,[Bug] Deadlock in PersistentSubscription.close / NamespaceService.unloadNamespaceBundle / PulsarService.closeAsync countered in tests,"### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  master branch  ### Minimal reproduce step  No steps to reproduce, faced in tests ``` Found one Java-level deadlock: ============================= ""main"":   waiting to lock monitor 0x00007fd520fe6f10 (object 0x000010003425f350, a org.apache.pulsar.broker.service.persistent.PersistentSubscrip tion),   which is held by ""PulsarTestContext-executor-OrderedExecutor-0-0""  ""PulsarTestContext-executor-OrderedExecutor-0-0"":   waiting to lock monitor 0x00007fd4f400dd00 (object 0x000010003425fd70, a org.apache.pulsar.broker.service.persistent.PersistentDispatch erSingleActiveConsumer),   which is held by ""broker-topic-workers-OrderedExecutor-0-0""  ""broker-topic-workers-OrderedExecutor-0-0"":   waiting to lock monitor 0x00007fd7a406f4a0 (object 0x000010003427f678, a org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead),   which is held by ""PulsarTestContext-executor-OrderedExecutor-0-0""  Java stack information for the threads listed above: =================================================== ""main"": 	at org.apache.pulsar.broker.service.persistent.PersistentSubscription.close(PersistentSubscription.java) 	- waiting to lock <0x000010003425f350> (a org.apache.pulsar.broker.service.persistent.PersistentSubscription) 	at org.apache.pulsar.broker.service.persistent.PersistentTopic.lambda$close$56(PersistentTopic.java:1697) 	at org.apache.pulsar.broker.service.persistent.PersistentTopic$$Lambda/0x00007fd54cb397a8.accept(Unknown Source) 	at java.util.concurrent.ConcurrentHashMap.forEach(java.base@21.0.6/ConcurrentHashMap.java:1603) 	at org.apache.pulsar.broker.service.persistent.PersistentTopic.lambda$close$57(PersistentTopic.java:1697) 	at org.apache.pulsar.broker.service.persistent.PersistentTopic$$Lambda/0x00007fd54cb39358.accept(Unknown Source) 	at java.util.concurrent.CompletableFuture.uniAcceptNow(java.base@21.0.6/CompletableFuture.java:757) 	at java.util.concurrent.CompletableFuture.uniAcceptStage(java.base@21.0.6/CompletableFuture.java:735) 	at java.util.concurrent.CompletableFuture.thenAccept(java.base@21.0.6/CompletableFuture.java:2214) 	at org.apache.pulsar.broker.service.persistent.PersistentTopic.close(PersistentTopic.java:1688) 	at org.apache.pulsar.broker.service.BrokerService.lambda$unloadServiceUnit$116(BrokerService.java:2371) 	at org.apache.pulsar.broker.service.BrokerService$$Lambda/0x00007fd54cb76aa8.apply(Unknown Source) 	at java.util.concurrent.CompletableFuture.uniComposeStage(java.base@21.0.6/CompletableFuture.java:1187) 	at java.util.concurrent.CompletableFuture.thenCompose(java.base@21.0.6/CompletableFuture.java:2341) 	at org.apache.pulsar.broker.service.BrokerService.lambda$unloadServiceUnit$118(BrokerService.java:2371) 	at org.apache.pulsar.broker.service.BrokerService$$Lambda/0x00007fd54cb6f800.accept(Unknown Source) 	at java.util.concurrent.ConcurrentHashMap.forEach(java.base@21.0.6/ConcurrentHashMap.java:1603) 	at org.apache.pulsar.broker.service.BrokerService.unloadServiceUnit(BrokerService.java:2341) 	at org.apache.pulsar.broker.service.BrokerService.unloadServiceUnit(BrokerService.java:2314) 	at org.apache.pulsar.broker.namespace.OwnedBundle.lambda$handleUnloadRequest$0(OwnedBundle.java:138) 	at org.apache.pulsar.broker.namespace.OwnedBundle$$Lambda/0x00007fd54cb6f5c8.apply(Unknown Source) 	at java.util.concurrent.CompletableFuture.uniComposeStage(java.base@21.0.6/CompletableFuture.java:1187) 	at java.util.concurrent.CompletableFuture.thenCompose(java.base@21.0.6/CompletableFuture.java:2341) 	at org.apache.pulsar.broker.namespace.OwnedBundle.handleUnloadRequest(OwnedBundle.java:138) 	at org.apache.pulsar.broker.namespace.NamespaceService.unloadNamespaceBundle(NamespaceService.java:848) ... 	at org.apache.pulsar.broker.namespace.NamespaceService.unloadNamespaceBundle(NamespaceService.java:839) ... 	at org.apache.pulsar.broker.namespace.NamespaceService.unloadNamespaceBundle(NamespaceService.java:830) 	at org.apache.pulsar.broker.service.BrokerService.lambda$unloadNamespaceBundlesGracefully$30(BrokerService.java:999) 	at org.apache.pulsar.broker.service.BrokerService$$Lambda/0x00007fd54cb6ef58.accept(Unknown Source) 	at java.lang.Iterable.forEach(java.base@21.0.6/Iterable.java:75) 	at org.apache.pulsar.broker.service.BrokerService.unloadNamespaceBundlesGracefully(BrokerService.java:992) 	at org.apache.pulsar.broker.service.BrokerService.unloadNamespaceBundlesGracefully(BrokerService.java:962) 	at org.apache.pulsar.broker.PulsarService.closeAsync(PulsarService.java:525) ... 	at org.apache.pulsar.broker.PulsarService.closeAsync(PulsarService.java:509) 	at org.apache.pulsar.broker.PulsarService.close(PulsarService.java:484) ...   ""PulsarTestContext-executor-OrderedExecutor-0-0"": 	at org.apache.pulsar.broker.service.AbstractDispatcherSingleActiveConsumer.disconnectActiveConsumers(AbstractDispatcherSingleActiveConsumer.java) 	- waiting to lock <0x000010003425fd70> (a org.apache.pulsar.broker.service.persistent.PersistentDispatcherSingleActiveConsumer) 	at org.apache.pulsar.broker.service.persistent.PersistentSubscription.resetCursor(PersistentSubscription.java:856) 	- locked <0x000010003425f350> (a org.apache.pulsar.broker.service.persistent.PersistentSubscription) 	at org.apache.pulsar.broker.service.persistent.PersistentSubscription$6.findEntryComplete(PersistentSubscription.java:824) 	at org.apache.pulsar.broker.service.persistent.PersistentMessageFinder.findEntryComplete(PersistentMessageFinder.java:162) 	at org.apache.bookkeeper.mledger.impl.OpFindNewest.readEntryComplete(OpFindNewest.java:133) 	at org.apache.bookkeeper.mledger.impl.cache.RangeEntryCacheImpl$1.readEntriesComplete(RangeEntryCacheImpl.java:241) 	at org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead.readEntriesComplete(PendingReadsManager.java:253) 	- locked <0x000010003427f678> (a org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead) 	at org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead.lambda$attach$0(PendingReadsManager.java:232) 	at org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead$$Lambda/0x00007fd54cb0fc60.run(Unknown Source) 	at org.apache.bookkeeper.common.util.SingleThreadExecutor.safeRunTask(SingleThreadExecutor.java:137) 	at org.apache.bookkeeper.common.util.SingleThreadExecutor.run(SingleThreadExecutor.java:107) 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 	at java.lang.Thread.runWith(java.base@21.0.6/Thread.java:1596) 	at java.lang.Thread.run(java.base@21.0.6/Thread.java:1583) ""broker-topic-workers-OrderedExecutor-0-0"": 	at org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead.addListener(PendingReadsManager.java) 	- waiting to lock <0x000010003427f678> (a org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager$PendingRead) 	at org.apache.bookkeeper.mledger.impl.cache.PendingReadsManager.readEntries(PendingReadsManager.java:430) ... 	at org.apache.pulsar.broker.service.persistent.PersistentDispatcherSingleActiveConsumer.readMoreEntries(PersistentDispatcherSingleActiveConsumer.java:387) 	- locked <0x000010003425fd70> (a org.apache.pulsar.broker.service.persistent.PersistentDispatcherSingleActiveConsumer) ... ``` full dead lock details: https://gist.github.com/lhotari/135bb1a5a045d00c19cf374fca1ff8f7#file-threaddump15633_2025-02-08_00-txt-L1327-L1542 full thread dump: https://gist.github.com/lhotari/135bb1a5a045d00c19cf374fca1ff8f7 analysis: https://jstack.review/?https://gist.github.com/lhotari/135bb1a5a045d00c19cf374fca1ff8f7   ### What did you expect to see?  no deadlocks  ### What did you see instead?  there was a deadlock in closing PulsarService  ### Anything else?  _No response_  ### Are you willing to submit a PR?  - [ ] I'm willing to submit a PR!",2025-02-08T18:53:52+00:00,2025-02-11T15:14:19+00:00,2,https://github.com/apache/pulsar/issues/23952,23958.0,2025-02-11T15:14:18+00:00,https://github.com/apache/pulsar/pull/23958,0,1,0,1,42,29,0,71,68.34055555555555,type/bug,False,True,normal,functional,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/cache/PendingReadsManager.java"", ""lines_added"": 42, ""lines_deleted"": 29, ""file_type"": ""app_code""}]",,False
pingcap/tidb,57960,The initialization of the priority queue is too slow when TiDB contains a large number of tables,## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> million tables restart tidb node ![Image](https://github.com/user-attachments/assets/15de9f44-3135-4e58-909c-291586dc6345)  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  Release Version: v8.5.0-alpha-279-g9812d85d0d Edition: Community Git Commit Hash: 9812d85d0d259547cf1dae88abbc7c406c56f935 Git Branch: HEAD UTC Build Time: 2024-12-03 17:04:33 GoVersion: go1.23.3 Race Enabled: false Check Table Before Drop: false Store: tikv,2024-12-04T03:50:58+00:00,2025-03-14T10:50:05+00:00,1,https://github.com/pingcap/tidb/issues/57960,58825.0,2025-03-14T10:50:04+00:00,https://github.com/pingcap/tidb/pull/58825,0,14,1,15,179,134,0,312,2406.985,type/bug;sig/planner;component/statistics;severity/major;affects-8.5,False,True,normal,ui,"[{""filename"": ""br/pkg/gluetidb/glue.go"", ""lines_added"": 1, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pkg/domain/domain.go"", ""lines_added"": 5, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/meta/meta.go"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/logical_plans_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/mock.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/session/session.go"", ""lines_added"": 3, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/autoanalyze.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/priorityqueue/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/statistics/handle/autoanalyze/priorityqueue/queue.go"", ""lines_added"": 93, ""lines_deleted"": 62, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/priorityqueue/queue_ddl_handler_test.go"", ""lines_added"": 27, ""lines_deleted"": 27, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/priorityqueue/queue_test.go"", ""lines_added"": 19, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/refresher/refresher.go"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/autoanalyze/refresher/refresher_test.go"", ""lines_added"": 9, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/handle.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/handletest/handle_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3284,[bugfix] compatile both map[kratos] and map.kratos for a map type in the quer…,"compatile the old way of using map.kratos  as the defintion of  map's value. the post version change the usage of  parameter defintion in the URL query value, ONLY support the using 'map[kratos]' to convey the value, which resulting in the incompatibility thirt party projects. This PR patches the flaw.",2024-04-16T03:47:33+00:00,2024-04-16T12:39:13+00:00,0,https://github.com/go-kratos/kratos/pull/3284,3284.0,2024-04-16T12:39:13+00:00,https://github.com/go-kratos/kratos/pull/3284,0,1,0,1,6,10,0,16,8.86111111111111,LGTM;size:S,False,True,normal,database,"[{""filename"": ""encoding/form/proto_decode.go"", ""lines_added"": 6, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7837,[Bug] Receipt handle auto-renew scheduled task is not started,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  all  ### RocketMQ version  develop  ### JDK Version  _No response_  ### Describe the Bug  The receipt handle auto-renew scheduled task is not started, resulting in duplicate consumption after handle expired.  <img width=""2139"" alt=""image"" src=""https://github.com/apache/rocketmq/assets/103550934/136c6355-3e94-4b49-a679-5cfc11ae8211"">   ### Steps to Reproduce  /  ### What Did You Expect to See?  /  ### What Did You See Instead?  /  ### Additional Context  _No response_",2024-02-20T04:52:48+00:00,2025-02-23T00:11:05+00:00,2,https://github.com/apache/rocketmq/issues/7837,7838.0,,https://github.com/apache/rocketmq/pull/7838,0,3,0,3,85,0,0,85,8851.304722222223,stale,False,True,normal,functional,"[{""filename"": ""proxy/src/main/java/org/apache/rocketmq/proxy/processor/DefaultMessagingProcessor.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""proxy/src/main/java/org/apache/rocketmq/proxy/processor/ReceiptHandleProcessor.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""proxy/src/test/java/org/apache/rocketmq/proxy/processor/ReceiptHandleProcessorTest.java"", ""lines_added"": 83, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,57325,Cancel ADD INDEX job with DXF is slow,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  1. enable `tidb_ddl_enable_fast_reorg` and `tidb_enable_dist_task` 2. set `tidb_ddl_reorg_max_write_speed` (introduced in https://github.com/pingcap/tidb/pull/57145) to a small speed like `1kb` 3. add index in session 1, which would cost some time 4. cancel the add index job in session 2  ### 2. What did you expect to see? (Required)  The job can be canceled in time  ### 3. What did you see instead (Required)  It takes much time for the job state to transfer from 'canceling' to 'rollback done'  ### 4. What is your TiDB version? (Required)  https://github.com/pingcap/tidb/tree/f973f194b21a8e79d42be3a9d4b2db441073f51c with https://github.com/pingcap/tidb/pull/57145 ",2024-11-12T13:46:32+00:00,2024-11-29T02:32:55+00:00,0,https://github.com/pingcap/tidb/issues/57325,60088.0,,https://github.com/pingcap/tidb/pull/60088,0,7,0,7,1284,29,0,1313,396.7730555555556,type/bug;severity/major;affects-6.5;component/ddl;affects-7.5;affects-8.5,False,True,normal,ui,"[{""filename"": ""pkg/ddl/backfilling.go"", ""lines_added"": 64, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ddl_test.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ddl_worker_test.go"", ""lines_added"": 0, ""lines_deleted"": 24, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/index.go"", ""lines_added"": 20, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/job_worker.go"", ""lines_added"": 1111, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/reorg.go"", ""lines_added"": 13, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/realtikvtest/addindextest1/disttask_test.go"", ""lines_added"": 73, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9176,[Bug] POP Msg failed when enable Authorization of ACL 2.0,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [x] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [x] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  mac os  ### RocketMQ version  5.3.x   ### JDK Version  jdk 1.8  ### Describe the Bug  PopMessagingProcessors.processRequest() will failed when configure `authorizationEnabled=true`, So  client can't consume message. The root cause is that incorrect caching was applied during the decoding process of the RequestHeader,  lack of the required `bornTime` field  ![Image](https://github.com/user-attachments/assets/ce6457ac-3950-4d59-9bb4-103b3fadde2a)  ![Image](https://github.com/user-attachments/assets/b13c97a6-5e05-462f-8077-5ec1ca47440c)  ### Steps to Reproduce  1. enable authorization 2. start a grpc client    ### What Did You Expect to See?  pop and consume message   ### What Did You See Instead?  can't POP message   ### Additional Context  _No response_",2025-02-12T05:31:59+00:00,2025-02-21T06:17:04+00:00,0,https://github.com/apache/rocketmq/issues/9176,9211.0,2025-02-28T01:52:46+00:00,https://github.com/apache/rocketmq/pull/9211,0,1,0,1,0,1,0,1,380.3463888888889,,False,True,normal,configuration,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/PopMessageProcessor.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/pulsar,23955,[fix][ml] Fix memory leak due to duplicated RangeCache value retain operations ,"### Motivation  https://github.com/apache/pulsar/pull/23903 introduces a memory leak issue in `RangeCache#removeEntry`.  ```diff -        Value value = entryWrapper.getValue(key); +        Value value = getValueMatchingEntry(entry); ```  Unlike `entryWrapper.getValue`, `getValueMatchingEntry` will increase the reference count of `entry`'s value.  ### Modifications  - Remove the duplicated retain operation in `RangeCache#removeEntry`. Add some API notes for the private methods that might increase the reference count of the value - Apply the reference count validation for eviction on a `RangeCache` (`RangeCacheTest.customTimeExtraction`).  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository:",2025-02-10T14:03:44+00:00,2025-02-10T18:17:54+00:00,6,https://github.com/apache/pulsar/pull/23955,23955.0,2025-02-10T18:17:54+00:00,https://github.com/apache/pulsar/pull/23955,0,2,0,2,62,67,0,129,4.236111111111111,type/bug;doc-not-needed;cherry-picked/branch-3.0;cherry-picked/branch-3.3;cherry-picked/branch-4.0;release/3.0.10;release/3.3.5;release/4.0.3,False,True,normal,ui,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/util/RangeCache.java"", ""lines_added"": 33, ""lines_deleted"": 63, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/util/RangeCacheTest.java"", ""lines_added"": 29, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59944,Unexpcted blocked scheduler in DXF if doCleanupTask takes long time,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> ``` SQL 	sm.schedulerWG.RunWithLog(func() { 		defer func() { 			scheduler.Close() 			sm.delScheduler(task.ID) 			if allocateSlots { 				sm.slotMgr.unReserve(basicTask, reservedExecID) 			} 			handle.NotifyTaskChange() 			sm.logger.Info(""task scheduler exit"", zap.Int64(""task-id"", task.ID)) 		}() 		metrics.UpdateMetricsForRunTask(task) 		scheduler.ScheduleTask() 		sm.finishCh <- struct{}{} 	}) ``` finishCh may be blocked. If more than 2x concurrency tasks are put into finishCh and it still can handle any result. New task would be blocked because no slot is available.   ### 2. What did you expect to see? (Required) N/A ### 3. What did you see instead (Required) N/A ### 4. What is your TiDB version? (Required) master <!-- Paste the output of SELECT tidb_version() -->  ",2025-03-06T13:08:25+00:00,2025-03-14T05:41:35+00:00,0,https://github.com/pingcap/tidb/issues/59944,59945.0,2025-03-14T05:41:34+00:00,https://github.com/pingcap/tidb/pull/59945,0,2,1,3,39,1,0,39,184.5525,type/bug;severity/moderate;component/ddl,False,True,normal,database,"[{""filename"": ""pkg/disttask/framework/scheduler/scheduler_manager.go"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/realtikvtest/addindextest1/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/realtikvtest/addindextest1/disttask_test.go"", ""lines_added"": 33, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3110,errors had been ignore or covered when too much err happend,"<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened: when too much err happened ,error info has been ignored.  由于err变量是同一个，后续的err会把之前执行过程中的error信息覆盖掉，导致错误信息被掩盖。 而且由于是FIFO的方式执行hook function；后续执行过程中不管有没有发生err，都会覆盖掉此前出现的err。 #### What you expected to happen: avoid error info being ignore when too much err happened . 如果出现大量的err，在对变量重复赋值之前打印出err信息，将此前执行过程中的err暴露出来。 #### How to reproduce it (as minimally and precisely as possible):  ```` for _, fn := range a.opts.afterStop { 		if err != nil { 			log.Warnf(""%s"", err) 		}  		fnErr := fn(sctx) 		if fnErr != nil { 			err = fnErr 		} 	} ````   ```` for _, fn := range a.opts.beforeStop { 		if err != nil { 			log.Warnf(""%s"", err) 		} 		fnErr := fn(sctx) 		if fnErr != nil { 			err = fnErr 		} 	} ```` #### Anything else we need to know?:  #### Environment: - Kratos version (use `kratos -v`):lastest - Go version (use `go version`):1.21.5 - OS (e.g: `cat /etc/os-release`): - Others: ",2023-12-08T06:29:36+00:00,2024-03-16T16:03:46+00:00,1,https://github.com/go-kratos/kratos/issues/3110,1567.0,2021-10-20T13:43:45+00:00,https://github.com/go-kratos/kratos/pull/1567,0,3,2,5,639,263,0,831,-18688.764166666668,bug,False,True,normal,functional,"[{""filename"": ""encoding/form/form_test.go"", ""lines_added"": 32, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""internal/complex/complex.pb.go"", ""lines_added"": 0, ""lines_deleted"": 242, ""file_type"": ""app_code""}, {""filename"": ""internal/complex/complex.proto"", ""lines_added"": 0, ""lines_deleted"": 19, ""file_type"": ""other""}, {""filename"": ""internal/testdata/complex/complex.pb.go"", ""lines_added"": 555, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/testdata/complex/complex.proto"", ""lines_added"": 52, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
pingcap/tidb,60051,DATA RACE in the br/pkg/restore/snap_client.TestConcurrency(),## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ``` ================== WARNING: DATA RACE Read at 0x00c0087baf78 by goroutine 12030:   github.com/pingcap/tidb/br/pkg/restore/snap_client.TestConcurrency()       br/pkg/restore/snap_client/pitr_collector_test.go:305 +0x911   github.com/pingcap/failpoint.parseTerm()       external/com_github_pingcap_failpoint/terms.go:149 +0x233   github.com/pingcap/failpoint.parse()       external/com_github_pingcap_failpoint/terms.go:126 +0xa5   github.com/pingcap/failpoint.newTerms()       external/com_github_pingcap_failpoint/terms.go:98 +0x3e   github.com/pingcap/failpoint.(*Failpoint).EnableCall()       external/com_github_pingcap_failpoint/failpoint.go:94 +0x124   github.com/pingcap/failpoint.(*Failpoints).EnableCall()       external/com_github_pingcap_failpoint/failpoints.go:150 +0x296   github.com/pingcap/failpoint.EnableCall()       external/com_github_pingcap_failpoint/failpoints.go:270 +0x358   github.com/pingcap/tidb/br/pkg/restore/snap_client.TestConcurrency()       br/pkg/restore/snap_client/pitr_collector_test.go:280 +0x277   testing.tRunner()       GOROOT/src/testing/testing.go:1690 +0x226   testing.(*T).Run.gowrap1()       GOROOT/src/testing/testing.go:1743 +0x44 Previous write at 0x00c0087baf78 by goroutine 12032:   github.com/pingcap/tidb/br/pkg/restore/snap_client.TestConcurrency.func2()       br/pkg/restore/snap_client/pitr_collector_test.go:295 +0x1a4 Goroutine 12030 (running) created at:   testing.(*T).Run()       GOROOT/src/testing/testing.go:1743 +0x825   testing.runTests.func1()       GOROOT/src/testing/testing.go:2168 +0x85   testing.tRunner()       GOROOT/src/testing/testing.go:1690 +0x226   testing.runTests()       GOROOT/src/testing/testing.go:2166 +0x8be   testing.(*M).Run()       GOROOT/src/testing/testing.go:2034 +0xf17   github.com/pingcap/tidb/br/pkg/restore/snap_client_test.TestMain()       br/pkg/restore/snap_client/main_test.go:48 +0x83d   github.com/pingcap/tidb/pkg/sessionctx/variable.(*SysVar).ValidateWithRelaxedValidation()       pkg/sessionctx/variable/variable.go:285 +0x242   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:143 +0xa24   fmt.Sscanf()       GOROOT/src/fmt/scan.go:114 +0x18e   github.com/pingcap/tidb/pkg/sessionctx/variable.parseByteSize()       pkg/sessionctx/variable/varsutil.go:319 +0x1d   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func263()       pkg/sessionctx/variable/sysvar.go:1411 +0x44   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:144 +0xa94   github.com/pingcap/tidb/pkg/sessionctx/variable.parseSchemaCacheSize()       pkg/sessionctx/variable/varsutil.go:551 +0x17c   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func622()       pkg/sessionctx/variable/sysvar.go:3310 +0x67   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:144 +0xa94   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func621()       pkg/sessionctx/variable/sysvar.go:3302 +0x52   github.com/pingcap/tidb/pkg/sessionctx/variable.(*SysVar).ValidateWithRelaxedValidation()       pkg/sessionctx/variable/variable.go:285 +0x242   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:143 +0xa24   github.com/pingcap/tidb/pkg/domain.(*Domain).LoadSysVarCacheLoop()       pkg/domain/domain.go:2053 +0x93   github.com/pingcap/tidb/pkg/session.bootstrapSessionImpl()       pkg/session/session.go:3553 +0x864   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func622()       pkg/sessionctx/variable/sysvar.go:3310 +0x67   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:144 +0xa94   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func621()       pkg/sessionctx/variable/sysvar.go:3302 +0x52   github.com/pingcap/tidb/pkg/sessionctx/variable.(*SysVar).ValidateWithRelaxedValidation()       pkg/sessionctx/variable/variable.go:285 +0x242   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:143 +0xa24   fmt.Sscanf()       GOROOT/src/fmt/scan.go:114 +0x18e   github.com/pingcap/tidb/pkg/sessionctx/variable.parseByteSize()       pkg/sessionctx/variable/varsutil.go:319 +0x1d   github.com/pingcap/tidb/pkg/sessionctx/variable.init.func263()       pkg/sessionctx/variable/sysvar.go:1411 +0x44   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCache()       pkg/domain/sysvar_cache.go:144 +0xa94   github.com/pingcap/tidb/pkg/domain.(*Domain).rebuildSysVarCacheIfNeeded()       pkg/domain/sysvar_cache.go:51 +0x1bb   github.com/pingcap/tidb/pkg/domain.(*Domain).GetSessionCache()       pkg/domain/sysvar_cache.go:62 +0x4a   github.com/pingcap/tidb/pkg/session.(*session).loadCommonGlobalVariablesIfNeeded()       pkg/session/session.go:3964 +0x2bc   github.com/pingcap/tidb/pkg/session.(*session).ExecuteStmt()       pkg/session/session.go:2029 +0x17a   github.com/pingcap/tidb/pkg/session.(*session).ExecuteInternal()       pkg/session/session.go:1540 +0x3af   github.com/pingcap/tidb/pkg/domain.(*Domain).LoadPrivilegeLoop()       pkg/domain/domain.go:1981 +0x102   github.com/pingcap/tidb/pkg/session.bootstrapSessionImpl()       pkg/session/session.go:3546 +0x807   github.com/pingcap/tidb/pkg/session.BootstrapSession()       pkg/session/session.go:3422 +0x25b   github.com/pingcap/tidb/br/pkg/mock.NewCluster()       br/pkg/mock/mock_cluster.go:76 +0x237   github.com/pingcap/tidb/br/pkg/restore/snap_client_test.TestMain()       br/pkg/restore/snap_client/main_test.go:40 +0x7a4   main.main()       bazel-out/k8-fastbuild/bin/br/pkg/restore/snap_client/snap_client_test_/testmain.go:175 +0x593 Goroutine 12032 (finished) created at:   github.com/pingcap/tidb/br/pkg/restore/snap_client.TestConcurrency()       br/pkg/restore/snap_client/pitr_collector_test.go:292 +0x3d6   github.com/pingcap/failpoint.parseTerm()       external/com_github_pingcap_failpoint/terms.go:149 +0x233   github.com/pingcap/failpoint.parse()       external/com_github_pingcap_failpoint/terms.go:126 +0xa5   github.com/pingcap/failpoint.newTerms()       external/com_github_pingcap_failpoint/terms.go:98 +0x3e   github.com/pingcap/failpoint.(*Failpoint).EnableCall()       external/com_github_pingcap_failpoint/failpoint.go:94 +0x124   github.com/pingcap/failpoint.(*Failpoints).EnableCall()       external/com_github_pingcap_failpoint/failpoints.go:150 +0x296   github.com/pingcap/failpoint.EnableCall()       external/com_github_pingcap_failpoint/failpoints.go:270 +0x358   github.com/pingcap/tidb/br/pkg/restore/snap_client.TestConcurrency()       br/pkg/restore/snap_client/pitr_collector_test.go:280 +0x277   testing.tRunner()       GOROOT/src/testing/testing.go:1690 +0x226   testing.(*T).Run.gowrap1()       GOROOT/src/testing/testing.go:1743 +0x44 ==================  ```  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ,2025-03-13T07:25:04+00:00,2025-03-14T04:58:17+00:00,0,https://github.com/pingcap/tidb/issues/60051,60078.0,2025-03-14T04:58:16+00:00,https://github.com/pingcap/tidb/pull/60078,0,1,0,1,8,3,0,11,21.55333333333333,type/bug;severity/minor;affects-9.0,False,True,normal,database,"[{""filename"": ""br/pkg/restore/snap_client/pitr_collector_test.go"", ""lines_added"": 8, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9177,[Bug] Fix unstable tests in AdaptiveLockTest.testAdaptiveLock,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [x] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [x] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  ubuntu  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  ![Image](https://github.com/user-attachments/assets/baedaf49-e09e-43c0-8a73-d1220d15a836) Occasional failures, erratic testing  ### Steps to Reproduce  null  ### What Did You Expect to See?  Stable test  ### What Did You See Instead?  null  ### Additional Context  _No response_",2025-02-12T05:32:38+00:00,2025-02-12T08:44:35+00:00,0,https://github.com/apache/rocketmq/issues/9177,9178.0,2025-02-12T08:44:34+00:00,https://github.com/apache/rocketmq/pull/9178,0,1,0,1,5,3,0,8,3.198888888888889,,False,True,normal,functional,"[{""filename"": ""store/src/test/java/org/apache/rocketmq/store/lock/AdaptiveLockTest.java"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,2883,[Question] kratosgrpc.DialInsecure报错 failed to build resolver: discovery create watcher overtime ,"使用nacos作为注册中心， gRPC客户端使用一下代码建立连接时，等待10s后报`failed to build resolver: discovery create watcher overtime `错误   ```go // CreateUser : rpc调用， 创建用户 func (r *tplInnerRepo) CreateUser(ctx context.Context, in *tplinner.CreateUserRequest) ( 	reply *tplinner.CreateUserReply, err error) { 	r.h.WithContext(ctx).Infof(""(r *tplInnerRepo) CreateUser, %+v"", in)  	ins, err := r.data.dis.GetService(ctx, ""be_tpl_golang_inner_svr.grpc"") 	if err != nil { 		r.h.WithContext(ctx).Errorf(""r.data.dis.GetService error %+v"") 		return reply, err 	} 	r.h.WithContext(ctx).Infof(""instance: %+v"", ins) 	r.h.WithContext(ctx).Infof(""endpoint: %s"", ins[0].Endpoints[0])  	conn, err := kratosgrpc.DialInsecure(context.Background(), 		kratosgrpc.WithEndpoint(""discovery:///be_tpl_golang_inner_svr.grpc""), 		// kratosgrpc.WithEndpoint(""127.0.0.1:32112""), 		kratosgrpc.WithDiscovery(r.data.dis), 		kratosgrpc.WithLogger(r.log), 		// kratosgrpc.WithUnaryInterceptor(r.logUnaryRequestDetails), 		// kratosgrpc.WithPrintDiscoveryDebugLog(true), 		kratosgrpc.WithTimeout(time.Second*10), 	) 	if err != nil { 		r.h.WithContext(ctx).Errorf(""grpc.DialInsecure error, %+v"", err) 		return reply, err 	} 	defer conn.Close() 	r.h.WithContext(ctx).Infof(""%+v"", conn.GetState())  	c := tplinner.NewUserClient(conn) 	reply, err = c.CreateUser(ctx, &tplinner.CreateUserRequest{ 		RequestID: in.RequestID, 		UserName:  in.UserName, 	})  	if err != nil { 		r.h.WithContext(ctx).Errorf(""c.CreateUser error, %+v"", err) 		return reply, err 	} 	r.h.WithContext(ctx).Infof(""c.CreateUser success, %+v"", reply) 	return reply, err } ```  补充信息： 1. GetService可以正常获取instance 2. 服务端正常，使用gprccurl可以正常访问 3. 把`kratosgrpc.WithEndpoint(""discovery:///be_tpl_golang_inner_svr.grpc"")`换成`kratosgrpc.WithEndpoint(""127.0.0.1:32112""),`也是正常的。   ——————————补充信息 2023年06月21日10:14:46 —————————————— 看源码， DialInsecure会调用dial(ctx, true, opts...) dial() 中会 通过 `discovery.NewBuilder() `创建 `resolver.Builder` 实例 之后进入grpc官方库的函数 `grpc.DialContext(ctx, options.endpoint, grpcOpts...) ` ==》 `rWrapper, err := newCCResolverWrapper(cc, resolverBuilder)` ==> `	ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo)` 这里的rb.Build 应该是之前创建的`resolver.Builder`, 也就相当于进入了Build的代码如下：  ```  func (b *builder) Build(target resolver.Target, cc resolver.ClientConn, _ resolver.BuildOptions) (resolver.Resolver, error) { 	watchRes := &struct { 		err error 		w   registry.Watcher 	}{}  	done := make(chan struct{}, 1) 	ctx, cancel := context.WithCancel(context.Background()) 	go func() { 		w, err := b.discoverer.Watch(ctx, strings.TrimPrefix(target.URL.Path, ""/"")) 		watchRes.w = w 		watchRes.err = err 		close(done) 	}()  	var err error 	select { 	case <-done: 		err = watchRes.err 	case <-time.After(b.timeout): 		err = errors.New(""discovery create watcher overtime"") 	} 	if err != nil { 		cancel() 		return nil, err 	}  	r := &discoveryResolver{ 		w:           watchRes.w, 		cc:          cc, 		ctx:         ctx, 		cancel:      cancel, 		insecure:    b.insecure, 		debugLog:    b.debugLog, 		subsetSize:  b.subsetSize, 		selecterKey: uuid.New().String(), 	} 	go r.watch() 	return r, nil } ``` 这里会阻塞调用 `w, err := b.discoverer.Watch(ctx, strings.TrimPrefix(target.URL.Path, ""/"")) `  **看上去 `discoverer.Watch()` 一直到超时了还没有返回，不知道是什么原因**      ",2023-06-20T13:31:55+00:00,2023-06-21T07:24:15+00:00,3,https://github.com/go-kratos/kratos/issues/2883,1892.0,2022-03-18T06:58:33+00:00,https://github.com/go-kratos/kratos/pull/1892,0,1,0,1,3,0,0,3,-11022.556111111113,question,False,True,normal,networking,"[{""filename"": ""contrib/registry/consul/client.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,4689,NumberFormatException exception,"Caused by: org.apache.rocketmq.client.exception.MQBrokerException: CODE: 1  DESC: java.lang.NumberFormatException: For input string: ""3sw81-NzMzMjYwYTliYWViNGNiZmE1MTNkNzBhMGFkNWJiMjEuMTM3LjE2NTg3MzI0MTE1MDUwMDMz-NzMzMjYwYTliYWViNGNiZmE1MTNkNzBhMGFkNWJiMjEuMTM3LjE2NTg3MzI0MTE1MDUwMDMy-3-aGlob25vcmFwaS1kZXY=-aGlob25vcmFwaS1kZXYtNjhjOWY0NDRjZC12bWtwYg==-e0dFVH0vZGVtby9kZXY=-MTAuMTIwLjEuMTI5Ojk4NzY="", java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) BROKER: 10.120.1.129:10911 For more information, please visit the url, http://rocketmq.apache.org/docs/faq/ 	at org.apache.rocketmq.client.impl.MQClientAPIImpl.processSendResponse(MQClientAPIImpl.java:668) ~[rocketmq-client-4.9.2.jar!/:4.9.2] 	at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessageSync(MQClientAPIImpl.java:507) ~[rocketmq-client-4.9.2.jar!/:4.9.2] 	at org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage$original$1r6gufJZ(MQClientAPIImpl.java:489) ~[rocketmq-client-4.9.2.jar!/:4.9.2]   rocketmq version  4.9.2   全局搜索了4.9.2的源码没发现这个异常信息，疑问这里在哪里输出的。 ",2022-07-25T09:39:08+00:00,2024-06-16T00:09:13+00:00,10,https://github.com/apache/rocketmq/issues/4689,7952.0,2024-03-24T10:58:45+00:00,https://github.com/apache/rocketmq/pull/7952,0,1,0,1,3,1,0,4,14593.326944444443,type/question;stale,False,True,normal,functional,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessor.java"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
go-kratos/kratos,3181,Test failed on different time zone.,"<!-- Please answer these questions before submitting your issue. Thanks! For questions please use one of our forums: https://go-kratos.dev/docs/getting-started/faq --> #### What happened:  Test failed. Logs like below:  ```sh go test -run ^TestEncodeValues$ github.com/go-kratos/kratos/v2/encoding/form  --- FAIL: TestEncodeValues (0.00s)     /Users/kvii/workspace/github.com/kvii/kratos/encoding/form/proto_encode_test.go:49: want: a=19...&timestamp=1970-01-01T00%3A00%3A20.000000002Z..., got: a=19...&timestamp=1970-01-01T08%3A00%3A20.000000002Z... FAIL FAIL	github.com/go-kratos/kratos/v2/encoding/form	1.340s FAIL ```  Note that timestamps ""1970-01-01**T00**"" and ""1970-01-01**T08**"" are different.  #### What you expected to happen:  Test passed.  #### How to reproduce it (as minimally and precisely as possible):  `go test -run ^TestEncodeValues$ github.com/go-kratos/kratos/v2/encoding/form`.  #### Anything else we need to know?:  I'll fix it soon.  #### Environment: - Kratos version (use `kratos -v`): v2.7.2 - Go version (use `go version`): go1.21.6 darwin/arm64 - OS (e.g: `cat /etc/os-release`): Mac OS Sonoma 14.3 - Others: ",2024-01-31T07:53:26+00:00,2024-02-02T02:38:21+00:00,1,https://github.com/go-kratos/kratos/issues/3181,3183.0,2024-02-02T02:38:20+00:00,https://github.com/go-kratos/kratos/pull/3183,0,2,0,2,10,0,0,10,42.748333333333335,bug,False,True,normal,functional,"[{""filename"": ""encoding/form/form_test.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""encoding/form/proto_encode_test.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7519,[Bug] rocketmq-spring自定义sendMessageHook的sendMessageAfter被执行多次,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  win & centos7  ### RocketMQ version  - `org.apache.rocketmq:rocketmq-spring-boot-starter:2.2.3`   ### JDK Version  jdk8  ### Describe the Bug  发送异步数据时发现z自定义hook中的方法被调用多次，打印了多次日志  ### Steps to Reproduce  `rocketMQTemplate.asyncSend(....)`  ### What Did You Expect to See?  没有重复调用的日志  ### What Did You See Instead?  出现了重复调用打印的日志  ### Additional Context  [讨论](https://github.com/apache/rocketmq/discussions/7518#discussion-5796798)",2023-10-31T08:09:10+00:00,2025-02-05T00:10:09+00:00,5,https://github.com/apache/rocketmq/issues/7519,4678.0,2022-08-17T02:25:42+00:00,https://github.com/apache/rocketmq/pull/4678,0,2,0,2,56,42,0,98,-10565.724444444444,stale,False,True,normal,functional,"[{""filename"": ""store/src/main/java/org/apache/rocketmq/store/MappedFileQueue.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""store/src/main/java/org/apache/rocketmq/store/logfile/DefaultMappedFile.java"", ""lines_added"": 55, ""lines_deleted"": 41, ""file_type"": ""app_code""}]",store;logfile,True
pingcap/tidb,59798,potential memory leak chances in `tableTimerStoreCore.takeSession`,"## Bug Report  As the code displayed below:  https://github.com/pingcap/tidb/blob/f689bd646557f6891a4a79fdb98955810986e5f3/pkg/timer/tablestore/store.go#L374-L381  When `Rollback` fails it calls `Close` to release the resource. However, the `Close` method does not call `infosync.DeleteInternalSession` and it will still cause memory leak here.  We can use `pool.Destroy` to release a session: https://github.com/pingcap/tidb/pull/59546/files#diff-54aadf83518cc9c682c4e7462a4755a38515c67b40e92bd4202884c7d1950ab8R35 ",2025-02-27T04:02:40+00:00,2025-03-13T15:09:45+00:00,1,https://github.com/pingcap/tidb/issues/59798,59799.0,2025-03-13T15:09:43+00:00,https://github.com/pingcap/tidb/pull/59799,0,8,1,9,109,43,0,150,347.1175,type/bug;sig/sql-infra;severity/major;affects-8.1;impact/leak;affects-8.5,False,True,normal,ui,"[{""filename"": ""pkg/timer/BUILD.bazel"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/timer/store_intergartion_test.go"", ""lines_added"": 51, ""lines_deleted"": 16, ""file_type"": ""app_code""}, {""filename"": ""pkg/timer/tablestore/sql_test.go"", ""lines_added"": 23, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pkg/timer/tablestore/store.go"", ""lines_added"": 11, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pkg/ttl/ttlworker/job_manager.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/ttl/ttlworker/job_manager_integration_test.go"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pkg/ttl/ttlworker/session_integration_test.go"", ""lines_added"": 11, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/ttl/ttlworker/session_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ttl/ttlworker/task_manager_integration_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7801,[Bug] DefaultMQPullConsumer first update offset fail,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  OS: mac14.3  ### RocketMQ version  branch: 4.9.x  ### JDK Version  JDK1.8  ### Describe the Bug  This is my pull message cod ```java public static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException {         DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(""pullTopicTest_consumerGroup"");         consumer.setNamesrvAddr(""127.0.0.1:9876"");         consumer.start();         Set<MessageQueue> messageQueues = consumer.fetchSubscribeMessageQueues(""pulltopictest"");          for (MessageQueue messageQueue : messageQueues) {             long offset = consumer.fetchConsumeOffset(messageQueue, true);             System.out.println(offset);             if (offset < 0) {                 continue;             }             PullResult result = consumer.pull(messageQueue, ""*"", offset, 1, 3000);             System.out.println(result);             long i = result.getNextBeginOffset();             if (result.getPullStatus() == PullStatus.FOUND) {                 System.out.println(i);                 consumer.updateConsumeOffset(messageQueue, i); //                offset = consumer.fetchConsumeOffset(messageQueue, true); //                System.out.println(offset);             }         }         Thread.sleep(10000);         consumer.shutdown();     } ```  This code is a simple pull message ，The final sleep is to wait for the consumer to submit the location to the server. But No matter how long you wait in the end，offset not update to broker  ###   ### Steps to Reproduce  print ``` 9 # fetchSubscribeMessageQueues PullResult [pullStatus=FOUND, nextBeginOffset=10, minOffset=0, maxOffset=10, msgFoundList=1] 10 # getNextBeginOffset ```   Execute discovery multiple times fetchSubscribeMessageQueues always get offset 9  ### What Did You Expect to See?  Each execution will get the latest offset  ### What Did You See Instead?  If you set up a scheduled task to start the consumer once a day to pull a message, and then submit the offset，this offset cant update to broker。you will get the same offset in next time  ### Additional Context  _No response_",2024-01-30T07:49:19+00:00,2025-02-02T00:10:32+00:00,2,https://github.com/apache/rocketmq/issues/7801,7802.0,,https://github.com/apache/rocketmq/pull/7802,0,3,0,3,54,6,0,60,8848.353611111112,stale,False,True,normal,functional,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/consumer/DefaultMQPullConsumer.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultMQPullConsumerImpl.java"", ""lines_added"": 46, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/factory/MQClientInstance.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59877,"TiFlash join may got wrong result if fine grained join is enabled, and multiple join key has different types","## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> ``` mysql> create table t1(id bigint, v1 int); Query OK, 0 rows affected (0.04 sec)  mysql> create table t2(id bigint unsigned, v1 int); Query OK, 0 rows affected (0.05 sec)  mysql> create table t3(id bigint, v1 int); Query OK, 0 rows affected (0.05 sec)  mysql> insert into t1 values(10001, 1); Query OK, 1 row affected (0.01 sec)  mysql> insert into t2 values(10001, 2); Query OK, 1 row affected (0.00 sec)  mysql> insert into t3 values(10001, 3); Query OK, 1 row affected (0.01 sec)  mysql> set tidb_broadcast_join_threshold_size=0; Query OK, 0 rows affected (0.00 sec)  mysql> set tidb_broadcast_join_threshold_count=0; Query OK, 0 rows affected (0.00 sec)  mysql> alter table t1 set tiflash replica 1; Query OK, 0 rows affected (0.06 sec)  mysql> alter table t2 set tiflash replica 1; Query OK, 0 rows affected (0.04 sec)  mysql> alter table t3 set tiflash replica 1; Query OK, 0 rows affected (0.06 sec)  mysql> set tidb_enforce_mpp=1; Query OK, 0 rows affected (0.01 sec)  mysql> set tiflash_fine_grained_shuffle_stream_count=0; Query OK, 0 rows affected (0.00 sec)  mysql>  select /*+ hash_join_build(t3) */ count(*) from t1 straight_join t2 on t1.id = t2.id straight_join t3 on t1.id = t3.id; +----------+ | count(*) | +----------+ |        0 | +----------+ 1 row in set (0.03 sec)  mysql> set tiflash_fine_grained_shuffle_stream_count=-1; Query OK, 0 rows affected (0.00 sec)  mysql>  select /*+ hash_join_build(t3) */ count(*) from t1 straight_join t2 on t1.id = t2.id straight_join t3 on t1.id = t3.id; +----------+ | count(*) | +----------+ |        1 | +----------+ 1 row in set (0.07 sec) ``` ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ",2025-03-04T03:12:23+00:00,2025-03-05T08:40:40+00:00,9,https://github.com/pingcap/tidb/issues/59877,59913.0,2025-03-07T12:37:24+00:00,https://github.com/pingcap/tidb/pull/59913,0,2,1,3,73,5,0,76,81.41694444444444,type/bug;sig/execution;severity/critical;affects-7.1;affects-7.5;affects-8.1;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/executor/test/tiflashtest/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/executor/test/tiflashtest/tiflash_test.go"", ""lines_added"": 56, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/optimizer.go"", ""lines_added"": 16, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
apache/pulsar,23903,[fix][broker Fix bug in RangeCache where different instance of the key wouldn't ever match,"Fixes #23900  ### Motivation  During work on PR #23901, a severe issue was discovered in the RangeCache implementation. Cache lookups consistently fail when the key used to look up an entry differs from the instance used to put the entry into the cache.  One detail of the current broker cache implementation is that it is necessary to configure `cacheEvictionByMarkDeletedPosition=true` so that cached entries don't get evicted by the read position in the managed ledger.  This problematic solution was introduced in PR #22789, which aimed to address race conditions in the RangeCache and prevent the use of already recycled object instances. The solution underwent subsequent refactoring in PRs #22814 and #22818. While the value wrapper solution ensures consistency between cached entries and their corresponding keys, the original design primarily focused on the `getRange` and `removeRange` methods.  In these method implementations, the ConcurrentSkipListMap's subMap method ensures the original key remains available, preventing the lookup problem from manifesting. This explains why the issue remained undetected for so long, as caching functions correctly in most scenarios.  The impact primarily affects single reads, such as initial readings of replay queue entries. Single reads have never added entries to the cache, a bug originally reported as part of #23504 and subsequently moved to #23900.  Issue #23900 represents a regression. Prior to the changes introduced in PRs #22789, #22814, and #22818, replay queue entries might have been cached due to previous reads adding entries to the cache. However, the functionality to add replay queue reads to the cache has never been implemented.  ### Modifications  - add unit test to verify that an entry can be looked up with a different key instance - fix the issue in RangeCache implementation  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->",2025-01-28T00:31:48+00:00,2025-01-28T23:23:52+00:00,4,https://github.com/apache/pulsar/pull/23903,23903.0,2025-01-28T23:23:52+00:00,https://github.com/apache/pulsar/pull/23903,1,6,0,7,543,32,9,566,22.86777777777778,doc-not-needed;ready-to-test;cherry-picked/branch-3.0;cherry-picked/branch-3.3;cherry-picked/branch-4.0;release/3.0.10;release/3.3.5;release/4.0.3,False,True,critical,configuration,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/util/RangeCache.java"", ""lines_added"": 84, ""lines_deleted"": 25, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerBkTest.java"", ""lines_added"": 110, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/util/RangeCacheTest.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/KeySharedSubscriptionBrokerCacheTest.java"", ""lines_added"": 308, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/resources/log4j2.xml"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""pulsar-transaction/coordinator/src/test/java/org/apache/pulsar/transaction/coordinator/impl/TxnLogBufferedWriterTest.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-transaction/coordinator/src/test/java/org/apache/pulsar/transaction/coordinator/test/MockedBookKeeperTestCase.java"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",client,False
pingcap/tidb,20710,Optimizer does not consider the cost of other condition in Index Join,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> ```sql mysql> create table t(a int, b int, index (a)); Query OK, 0 rows affected (0.01 sec) ```  SQL is  ```sql select * from t t1 inner join t t2 on t1.a = t2.a and t1.b = t2.b; ``` If Index Join is used, currently **_t1.a = t2.a_** is put in the equality condition, and **_t1.b = t2.b_** is put in other conditions. If the selectivity of **_t1.a = t2.a_** is high, there will be quite a lot of rows that need to be filtered with **_t1.b = t2.b_**. This part of the cost will be very large. ```sql mysql> explain select /*+ INL_HASH_JOIN(t1) */ * from t t1 inner join t t2 on t1.a = t2.a and t1.b = t2.b; +-------------------------------+----------+-----------+----------------------+-------------------------------------------------------------------------------------------------------------+ | id                            | estRows  | task      | access object        | operator info                                                                                               | +-------------------------------+----------+-----------+----------------------+-------------------------------------------------------------------------------------------------------------+ | IndexHashJoin_30              | 12475.01 | root      |                      | inner join, inner:IndexLookUp_27, outer key:test.t.a, inner key:test.t.a, other cond:eq(test.t.b, test.t.b) | | ├─TableReader_41(Build)       | 9980.01  | root      |                      | data:Selection_40                                                                                           | | │ └─Selection_40              | 9980.01  | cop[tikv] |                      | not(isnull(test.t.a)), not(isnull(test.t.b))                                                                | | │   └─TableFullScan_39        | 10000.00 | cop[tikv] | table:t2             | keep order:false, stats:pseudo                                                                              | | └─IndexLookUp_27(Probe)       | 1.25     | root      |                      |                                                                                                             | |   ├─Selection_25(Build)       | 1.25     | cop[tikv] |                      | not(isnull(test.t.a))                                                                                       | |   │ └─IndexRangeScan_23       | 1.25     | cop[tikv] | table:t1, index:a(a) | range: decided by [eq(test.t.a, test.t.a)], keep order:false, stats:pseudo                                  | |   └─Selection_26(Probe)       | 1.25     | cop[tikv] |                      | not(isnull(test.t.b))                                                                                       | |     └─TableRowIDScan_24       | 1.25     | cop[tikv] | table:t1             | keep order:false, stats:pseudo                                                                              | +-------------------------------+----------+-----------+----------------------+-------------------------------------------------------------------------------------------------------------+ 9 rows in set (0.01 sec) ```  If Hash Join is used, **_t1.a = t2.a and t1.b = t2.b_** will be put in equal condition. There is no such problem.  ```sql mysql> explain select * from t t1 inner join t t2 on t1.a = t2.a and t1.b = t2.b; +------------------------------+----------+-----------+---------------+-------------------------------------------------------------------+ | id                           | estRows  | task      | access object | operator info                                                     | +------------------------------+----------+-----------+---------------+-------------------------------------------------------------------+ | HashJoin_40                  | 12475.01 | root      |               | inner join, equal:[eq(test.t.a, test.t.a) eq(test.t.b, test.t.b)] | | ├─TableReader_61(Build)      | 9980.01  | root      |               | data:Selection_60                                                 | | │ └─Selection_60             | 9980.01  | cop[tikv] |               | not(isnull(test.t.a)), not(isnull(test.t.b))                      | | │   └─TableFullScan_59       | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                    | | └─TableReader_54(Probe)      | 9980.01  | root      |               | data:Selection_53                                                 | |   └─Selection_53             | 9980.01  | cop[tikv] |               | not(isnull(test.t.a)), not(isnull(test.t.b))                      | |     └─TableFullScan_52       | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                    | +------------------------------+----------+-----------+---------------+-------------------------------------------------------------------+ 7 rows in set (0.01 sec) ```  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ",2020-10-29T06:41:34+00:00,2020-11-19T09:17:54+00:00,3,https://github.com/pingcap/tidb/issues/20710,59970.0,,https://github.com/pingcap/tidb/pull/59970,0,1,2,3,122,1,0,9,506.6055555555556,type/bug;priority/release-blocker;sig/planner;severity/critical;affects-6.5;affects-7.1;affects-7.5;affects-8.1;report/customer;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/planner/core/exhaust_physical_plans.go"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integrationtest/r/planner/core/issuetest/planner_issue.result"", ""lines_added"": 69, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/planner/core/issuetest/planner_issue.test"", ""lines_added"": 45, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
apache/rocketmq,9119,[Bug] Invoke async should handle raw exception instead of CompletionException,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  OS: CentOS 6.9  ### RocketMQ version  branch: (develop|tag 5.3.1) version: 5.3.1  ### JDK Version  JDK: 1.8.0_202  ### Describe the Bug  NettyRemotingAbstract.invokeAsyncImpl should handle raw exception instead of CompletionException: ![image](https://github.com/user-attachments/assets/577964cc-1334-4950-84de-4d646beee6a4)  Because there are a lot of codes using raw exception for judging, such as: [MQClientAPIImpl](https://github.com/apache/rocketmq/blob/release-5.3.1/client/src/main/java/org/apache/rocketmq/client/impl/MQClientAPIImpl.java#L738), [DefaultMQPushConsumerImpl](https://github.com/apache/rocketmq/blob/release-5.3.1/client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultMQPushConsumerImpl.java#L438). [](url)[](url)  ### Steps to Reproduce  ![image](https://github.com/user-attachments/assets/426602cf-30d7-4c69-95f6-db48641052fa) Raw exception will be wrap by CompletionException when execute future.completeExceptionally(exception).  ### What Did You Expect to See?  Raw exception.  ### What Did You See Instead?  Wrapped exception: CompletionException.  ### Additional Context  _No response_",2025-01-10T07:06:14+00:00,2025-01-17T03:44:33+00:00,0,https://github.com/apache/rocketmq/issues/9119,9120.0,2025-01-17T03:44:32+00:00,https://github.com/apache/rocketmq/pull/9120,0,1,0,1,2,1,0,3,164.63833333333332,,False,True,normal,functional,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/netty/NettyRemotingAbstract.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/pulsar,23900,"[Bug] Redelivery messages (replay queue reads) don't get read from cache even when they exist in cache, causing extra bookkeeper reads","### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  Since Pulsar 3.0.6, 3.2.4, 3.3.1, 4.0.0  ### Minimal reproduce step  Replay queue reads cache cache misses in all cases, even when the entry is already available in the cache. One detail of the current broker cache implementation is that it is necessary to configure `cacheEvictionByMarkDeletedPosition=true` so that cached entries don't get evicted by the read position in the managed ledger. (There's a separate issue #16421 about by-passing the cache when `cacheEvictionByMarkDeletedPosition=true` isn't used.)  ### What did you expect to see?  Replay queue reads should get read from the cache.  ### What did you see instead?  No caching  ### Anything else?  This was previously reported as part of #23504. In addition, there's another issue that when reads are performed, the returned entries aren't added to the cache.  ### Are you willing to submit a PR?  - [x] I'm willing to submit a PR!",2025-01-27T19:35:56+00:00,2025-01-28T23:23:53+00:00,0,https://github.com/apache/pulsar/issues/23900,23903.0,2025-01-28T23:23:52+00:00,https://github.com/apache/pulsar/pull/23903,1,6,0,7,543,32,9,566,27.79888888888889,type/bug,False,True,normal,configuration,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/util/RangeCache.java"", ""lines_added"": 84, ""lines_deleted"": 25, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerBkTest.java"", ""lines_added"": 110, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/util/RangeCacheTest.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/KeySharedSubscriptionBrokerCacheTest.java"", ""lines_added"": 308, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/resources/log4j2.xml"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""pulsar-transaction/coordinator/src/test/java/org/apache/pulsar/transaction/coordinator/impl/TxnLogBufferedWriterTest.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-transaction/coordinator/src/test/java/org/apache/pulsar/transaction/coordinator/test/MockedBookKeeperTestCase.java"", ""lines_added"": 5, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",client,False
pingcap/tidb,59894,*: fix incorrect handling IterAllTables,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59912  Problem Summary: We used number order to split subtasks, which is wrong.  ### What changed and how does it work? Use string order to split subtasks.  ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-03-04T09:49:36+00:00,2025-03-12T13:56:39+00:00,16,https://github.com/pingcap/tidb/pull/59894,59894.0,2025-03-12T13:56:39+00:00,https://github.com/pingcap/tidb/pull/59894,0,2,0,2,42,17,0,59,196.1175,size/M;release-note-none;approved;lgtm;needs-cherry-pick-release-8.5,False,True,normal,database,"[{""filename"": ""pkg/ddl/db_test.go"", ""lines_added"": 10, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/meta/meta.go"", ""lines_added"": 32, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",,False
apache/pulsar,23705,[Bug] Pulsar Function processing time doesn't get properly recorded for asynchronous functions,"### Search before asking  - [X] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [X] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  any released version  ### Minimal reproduce step  In the code, it can be seen that asynchronous functions don't get handled properly. `stats.processTimeEnd()` gets called immediately when the result is returned: https://github.com/apache/pulsar/blob/6fe8100b1fd5d37a6e1bf33803a8904fa3879321/pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/JavaInstanceRunnable.java#L337-L355   ### What did you expect to see?  asynchronous functions would also be handled  ### What did you see instead?  asynchronous functions have invalid processing time stats  ### Anything else?  The current processing metric for async functions includes the time for doing the async calls and the waiting time when the concurrency limit is reached. The metric is useful for this purpose.  It doesn't tell the actual end-to-completion processing time which contains the async processing time.  ### Are you willing to submit a PR?  - [ ] I'm willing to submit a PR!",2024-12-10T15:48:41+00:00,2025-01-28T16:03:13+00:00,0,https://github.com/apache/pulsar/issues/23705,23811.0,2025-01-28T16:03:11+00:00,https://github.com/apache/pulsar/pull/23811,1,9,0,10,142,37,5,174,1176.2416666666666,type/bug;release/4.0.3,False,True,normal,functional,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/io/PulsarFunctionE2ETest.java"", ""lines_added"": 95, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/JavaExecutionResult.java"", ""lines_added"": 1, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/JavaInstance.java"", ""lines_added"": 6, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/JavaInstanceRunnable.java"", ""lines_added"": 7, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/stats/ComponentStatsManager.java"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/stats/FunctionStatsManager.java"", ""lines_added"": 2, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/stats/SinkStatsManager.java"", ""lines_added"": 1, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/java/org/apache/pulsar/functions/instance/stats/SourceStatsManager.java"", ""lines_added"": 1, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""pulsar-functions/instance/src/main/resources/findbugsExclude.xml"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""pulsar-functions/instance/src/test/java/org/apache/pulsar/functions/instance/JavaInstanceRunnableTest.java"", ""lines_added"": 23, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59912,IterAllTables may get wrong result,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> Previously, we assumed that kv is in number order, so we split the subtask by 0-maxDBID. But in fact, the kv is in string number. We splited the subtask in the wrong way.  ### 2. What did you expect to see? (Required) N/A  ### 3. What did you see instead (Required) N/A ### 4. What is your TiDB version? (Required) master <!-- Paste the output of SELECT tidb_version() -->  ",2025-03-05T08:36:22+00:00,2025-03-12T13:56:41+00:00,0,https://github.com/pingcap/tidb/issues/59912,59894.0,2025-03-12T13:56:39+00:00,https://github.com/pingcap/tidb/pull/59894,0,2,0,2,42,17,0,59,173.33805555555554,type/bug;severity/moderate;component/ddl,False,True,normal,ui,"[{""filename"": ""pkg/ddl/db_test.go"", ""lines_added"": 10, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/meta/meta.go"", ""lines_added"": 32, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",,False
pingcap/tidb,60024,br: pitr meta kv parsing refactor fail to parse value in write cf,"## Bug Report only on master since in development. still create an issue to track it.  found a meta kv parsing issue with short value, need to make is consistent with the previous logic  ",2025-03-11T23:28:29+00:00,2025-03-12T10:47:06+00:00,1,https://github.com/pingcap/tidb/issues/60024,60039.0,2025-03-17T16:33:33+00:00,https://github.com/pingcap/tidb/pull/60039,0,1,0,1,5,3,0,8,137.08444444444444,type/bug;severity/major;component/br;affects-9.0,False,True,normal,functional,"[{""filename"": ""br/pkg/stream/table_mapping.go"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",stream,False
apache/rocketmq,4273,DLedgerRpcNettyService can not communicate with each other when tls enabled,"RocketMQ version: 4.9.3 enableDLegerCommitLog=true  I set -Dtls.enable=true -Dtls.server.mode=enforcing -Dtls.config.file=C:\\Software\\rocketmq-4.9.2\\conf\\tls.properties in BrokerStartup command line, but DLedgerRpcNettyService can not communicate with each other.   I notice that in DLedgerRpcNettyService, a NettyRemotingServer(remotingServer) was build with a new NettyServerConfig, and a NettyRemotingClient(remotingClient) was build with a new NettyClientConfig, but remotingServer load the tls conf(tls.server.mode) from system propeties and remotingClient load the tls conf(useTLS) from NettyServerConfig which is default false and can not set, so remotingServer is tls enabled but remotingClient is not enabled.  ",2022-05-10T08:01:57+00:00,2023-05-14T00:08:07+00:00,3,https://github.com/apache/rocketmq/issues/4273,4274.0,,https://github.com/apache/rocketmq/pull/4274,0,1,0,1,2,2,0,4,8848.102777777778,type/bug;stale,False,True,normal,configuration,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/netty/NettyRemotingClient.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7619,"[Bug] no messages are generated, the TOOLS_CONSUMER consumer group has been consuming messages quickly.","### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos8  ### RocketMQ version  4.9.7  ### JDK Version  1.8  ### Describe the Bug   The dashboard interface shows that a large number of messages were consumed today, but no messages are actually generated at this time. Check stat.log, the information is as follows： dashboard界面显示今日消费了大量消息，此时实际上并没有消息产生，查看stat.log,信息如下：  2023-12-06 13:45:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 293665 AVGRT: 0.02 2023-12-06 13:45:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3817645 TPS: 63627.42 AVGPT: 13.00 2023-12-06 13:46:00 INFO - [GROUP_GET_SIZE] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 4164735474 TPS: 69412257.90 AVGPT: 14073.00 2023-12-06 13:46:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 295940 AVGRT: 0.02 2023-12-06 13:46:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3847181 TPS: 64119.68 AVGPT: 13.00 2023-12-06 13:47:00 INFO - [GROUP_GET_SIZE] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 4200016485 TPS: 70000274.75 AVGPT: 14073.00 2023-12-06 13:47:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 298444 AVGRT: 0.02 2023-12-06 13:47:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3879798 TPS: 64663.30 AVGPT: 13.00 2023-12-06 13:48:00 INFO - [GROUP_GET_SIZE] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 4123220124 TPS: 68720335.40 AVGPT: 14073.00 2023-12-06 13:48:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 292988 AVGRT: 0.02 2023-12-06 13:48:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3808857 TPS: 63480.95 AVGPT: 13.00 2023-12-06 13:49:00 INFO - [GROUP_GET_SIZE] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3597157311 TPS: 59952621.85 AVGPT: 14073.00 2023-12-06 13:49:00 INFO - [GROUP_GET_FALL_TIME] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats Every 5 Minutes, Value: -5634663807447474569 2023-12-06 13:49:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 255609 AVGRT: 0.02 2023-12-06 13:49:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3322891 TPS: 55381.52 AVGPT: 13.00 2023-12-06 13:49:00 INFO - [GROUP_GET_FALL_SIZE] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats Every 5 Minutes, Value: 152237 2023-12-06 13:50:00 INFO - [GROUP_GET_SIZE] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 4196582673 TPS: 69943044.55 AVGPT: 14073.00 2023-12-06 13:50:00 INFO - [GROUP_GET_LATENCY] [2@sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, TIMES: 298200 AVGRT: 0.02 2023-12-06 13:50:00 INFO - [GROUP_GET_NUMS] [sms_batch_send_topic@TOOLS_CONSUMER] Stats In One Minute, SUM: 3876613 TPS: 64610.22 AVGPT: 13.00  ### Steps to Reproduce  Unable to reproduce  ### What Did You Expect to See?  Know what causes this problem  ### What Did You See Instead?  Know what causes this problem  ### Additional Context  _No response_",2023-12-06T07:49:49+00:00,2025-01-13T00:11:10+00:00,4,https://github.com/apache/rocketmq/issues/7619,4795.0,2022-08-07T09:44:42+00:00,https://github.com/apache/rocketmq/pull/4795,0,1,0,1,1,0,0,1,-11662.085277777778,stale,False,True,normal,ui,"[{""filename"": ""tools/src/main/java/org/apache/rocketmq/tools/command/controller/GetControllerMetaDataSubCommand.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,23880,[Bug] pulsar-admin functions list without specifying tenant and namespace return a NPE error,### Search before asking  - [x] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Read release policy  - [x] I understand that unsupported versions don't get bug fixes. I will attempt to reproduce the issue on a supported version of Pulsar client and Pulsar broker.   ### Version  [e5bd774](https://github.com/apache/pulsar/commit/e5bd77419e91d1602731cd0c1d02a738e1b7ebc7)  ### Minimal reproduce step  1. start pulsar in standalone mode:  ``` bin/pulsar standalone ```  2. list functions:  ``` bin/pulsar-admin functions list  null  Reason: java.lang.NullPointerException: path is 'null'. ```  ### What did you expect to see?  an empty list since no functions exist:  ``` ```  ### What did you see instead?  an error:  ``` null  Reason: java.lang.NullPointerException: path is 'null'. ```  ### Anything else?  _No response_  ### Are you willing to submit a PR?  - [x] I'm willing to submit a PR!,2025-01-23T02:56:04+00:00,2025-01-24T04:11:01+00:00,0,https://github.com/apache/pulsar/issues/23880,23881.0,2025-01-24T04:11:00+00:00,https://github.com/apache/pulsar/pull/23881,0,2,0,2,23,0,0,23,25.24888888888889,type/bug,False,True,normal,functional,"[{""filename"": ""pulsar-client-tools-test/src/test/java/org/apache/pulsar/admin/cli/CmdFunctionsTest.java"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client-tools/src/main/java/org/apache/pulsar/admin/cli/CmdFunctions.java"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59402,TSO retrieval failure during read TS validation caused by residual txn-scope code,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  Related to #58838   ### 1. Minimal reproduce step (Required)  It can be triggered by either stale read or `tidb_replica_read=closest-replicas`, but not for `closest-adaptive`.  ```go func TestTxnScopeAndValidateReadTs(t *testing.T) { 	defer config.RestoreFunc()() 	config.UpdateGlobal(func(conf *config.Config) { 		conf.Labels = map[string]string{ 			""zone"": ""bj"", 		} 	})  	store := realtikvtest.CreateMockStoreAndSetup(t) 	tk := testkit.NewTestKit(t, store) 	tk.MustExec(""use test"") 	tk.MustExec(""create table t1 (id int primary key);"") 	time.Sleep(time.Second)  	// stale read will fail 	//tk.MustQuery(""select * from t1 AS OF TIMESTAMP NOW() where id = 1;"").Check(testkit.Rows())  	// replica read 	// fail 	//tk.MustExec(""set @@tidb_replica_read = 'closest-replicas';"")  	// succeed 	//tk.MustExec(""set @@tidb_replica_read = 'follower';"")  	// succeed, because of https://github.com/pingcap/tidb/blob/f684d41f3e9e17777f6e19e2f9852457eed00ca6/pkg/sessiontxn/isolation/base.go#L178 	//tk.MustExec(""set @@tidb_replica_read = 'closest-adaptive';"") 	 	tk.MustExec(""begin"") 	tk.MustQuery(""select * from t1 where id = 1;"").Check(testkit.Rows()) } ```  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  Test passes.  ### 3. What did you see instead (Required)  error: ``` fail to validate read timestamp: [PD:client:ErrClientGetTSO]get TSO failed, unknown dc-location bj to the client ```  ### 4. What is your TiDB version? (Required)  LTS versions in [v6.5, v8.5] which has picked https://github.com/tikv/client-go/pull/1513, for example v8.5.1  <!-- Paste the output of SELECT tidb_version() -->  ",2025-02-11T08:11:43+00:00,2025-02-20T03:17:13+00:00,1,https://github.com/pingcap/tidb/issues/59402,60033.0,2025-03-13T03:01:53+00:00,https://github.com/pingcap/tidb/pull/60033,0,1,4,5,36,28,0,9,714.8361111111111,type/bug;sig/transaction;type/regression;severity/critical;affects-6.5;affects-7.1;affects-7.5;affects-8.1;affects-8.5;affects-9.0,False,True,normal,configuration,"[{""filename"": ""DEPS.bzl"", ""lines_added"": 18, ""lines_deleted"": 18, ""file_type"": ""other""}, {""filename"": ""go.mod"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 6, ""lines_deleted"": 6, ""file_type"": ""other""}, {""filename"": ""pkg/executor/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/executor/set.go"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,58600,fatal error: sync: unlock of unlocked mutex (v8.5.0 stmtctx.go:803),"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  I'm sorry, but I'm not sure how to reproduce the issue. It occurs sporadically on multiple running TiDB processes.  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  fatal error on https://github.com/pingcap/tidb/blob/d13e52ed6e22cc5789bed7c64c861578cd2ed55b/pkg/sessionctx/stmtctx/stmtctx.go#L800-L804 .  These errors always occur on the same line.  ``` fatal error: sync: unlock of unlocked mutex  goroutine 322590843 [running]: sync.fatal({0x67ac522?, 0x0?}) 	/usr/local/go/src/runtime/panic.go:1031 +0x18 sync.(*Mutex).unlockSlow(0xc0414d09c8, 0xffffffff) 	/usr/local/go/src/sync/mutex.go:231 +0x35 sync.(*Mutex).Unlock(...) 	/usr/local/go/src/sync/mutex.go:225 github.com/pingcap/tidb/pkg/sessionctx/stmtctx.(*StatementContext).AffectedRows(0xa6f0580?) 	/workspace/source/tidb/pkg/sessionctx/stmtctx/stmtctx.go:803 +0x9e github.com/pingcap/tidb/pkg/util.(*ProcessInfo).ToRow(0xc040ae1860, 0xc003726ee0) 	/workspace/source/tidb/pkg/util/processinfo.go:152 +0x87 github.com/pingcap/tidb/pkg/executor.(*memtableRetriever).setDataForProcessList(0xc084d3a8c0, {0x7114e40, 0xc06e015208}) 	/workspace/source/tidb/pkg/executor/infoschema_reader.go:1843 +0x25a github.com/pingcap/tidb/pkg/executor.(*memtableRetriever).setDataForClusterProcessList(0xc084d3a8c0, {0x7114e40, 0xc06e015208}) 	/workspace/source/tidb/pkg/executor/infoschema_reader.go:1816 +0x26 github.com/pingcap/tidb/pkg/executor.(*memtableRetriever).retrieve(0xc084d3a8c0, {0x707dc08, 0xc07adb7c20}, {0x7114e40, 0xc06e015208}) 	/workspace/source/tidb/pkg/executor/infoschema_reader.go:179 +0xef3 github.com/pingcap/tidb/pkg/executor.(*MemTableReaderExec).Next(0xc0c5628300, {0x707dc08, 0xc07adb7c20}, 0xc040a6d090) 	/workspace/source/tidb/pkg/executor/memtable_reader.go:136 +0x29c github.com/pingcap/tidb/pkg/executor/internal/exec.Next({0x707dc08, 0xc07adb7c20}, {0x70b4620, 0xc0c5628300}, 0xc040a6d090) 	/workspace/source/tidb/pkg/executor/internal/exec/executor.go:456 +0x29f github.com/pingcap/tidb/pkg/executor.(*SelectionExec).Next(0xc040a55080, {0x707dc08, 0xc07adb7c20}, 0xc040a6d0e0) 	/workspace/source/tidb/pkg/executor/select.go:721 +0xeb github.com/pingcap/tidb/pkg/executor/internal/exec.Next({0x707dc08, 0xc07adb7c20}, {0x70b47d0, 0xc040a55080}, 0xc040a6d0e0) 	/workspace/source/tidb/pkg/executor/internal/exec/executor.go:456 +0x29f github.com/pingcap/tidb/pkg/executor.(*CoprocessorDAGHandler).HandleRequest(0xc019f318b8, {0x707dc08?, 0xc07adb7770?}, 0xc040a56480) 	/workspace/source/tidb/pkg/executor/coprocessor.go:90 +0x356 github.com/pingcap/tidb/pkg/server.(*rpcServer).handleCopRequest(0xc003655b90, {0x707dc08, 0xc07adb7770}, 0xc040a56480) 	/workspace/source/tidb/pkg/server/rpc_server.go:216 +0x215 github.com/pingcap/tidb/pkg/server.(*rpcServer).Coprocessor(0xc003655b90, {0x707dc08, 0xc07adb7770}, 0xc040a56480) 	/workspace/source/tidb/pkg/server/rpc_server.go:105 +0x96 github.com/pingcap/kvproto/pkg/tikvpb._Tikv_Coprocessor_Handler({0x6700b60, 0xc003655b90}, {0x707dc08, 0xc07adb7770}, 0xc057e61400, 0x0) 	/root/go/pkg/mod/github.com/pingcap/kvproto@v0.0.0-20240924080114-4a3e17f5e62d/pkg/tikvpb/tikvpb.pb.go:3526 +0x1a6 google.golang.org/grpc.(*Server).processUnaryRPC(0xc003eff000, {0x707dc08, 0xc07adb76e0}, {0x70a1f40, 0xc018578180}, 0xc040a5f9e0, 0xc003655d10, 0x9f0d6d0, 0x0) 	/root/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1369 +0xdf8 google.golang.org/grpc.(*Server).handleStream(0xc003eff000, {0x70a1f40, 0xc018578180}, 0xc040a5f9e0) 	/root/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1780 +0xe8b google.golang.org/grpc.(*Server).serveStreams.func2.1() 	/root/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1019 +0x7f created by google.golang.org/grpc.(*Server).serveStreams.func2 in goroutine 1510 	/root/go/pkg/mod/google.golang.org/grpc@v1.63.2/server.go:1030 +0x125 ```   ### 4. What is your TiDB version? (Required)  ``` Release Version: v8.5.0 Edition: Community Git Commit Hash: d13e52ed6e22cc5789bed7c64c861578cd2ed55b Git Branch: HEAD UTC Build Time: 2024-12-18 02:26:06 GoVersion: go1.23.3 Race Enabled: false Check Table Before Drop: false Store: tikv ```  ",2024-12-29T15:08:27+00:00,2025-03-12T07:49:56+00:00,24,https://github.com/pingcap/tidb/issues/58600,60004.0,2025-03-12T07:49:54+00:00,https://github.com/pingcap/tidb/pull/60004,0,3,1,4,127,40,0,162,1744.690833333333,type/bug;sig/sql-infra;severity/critical;impact/panic;affects-9.0,False,True,normal,database,"[{""filename"": ""pkg/sessionctx/stmtctx/BUILD.bazel"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/sessionctx/stmtctx/stmtctx.go"", ""lines_added"": 108, ""lines_deleted"": 38, ""file_type"": ""app_code""}, {""filename"": ""pkg/sessionctx/stmtctx/stmtctx_test.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/sessionctx/variable/session.go"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59558,storage/s3: add retry for h2 disconnection errors,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59557  Problem Summary: See the issue.  ### What changed and how does it work? This PR retry when encountered possible H2 disconnection messages.   ### Check List  Tests <!-- At least one of them must be included. -->  - [ ] Unit test - [ ] Integration test - [x] Manual test (add detailed scripts or steps below) We verified this internally in a Tencent Cloud COS storage. - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note Now, br tolerances HTTP/2 GOAWAY messages / server cutting down connection. ``` ",2025-02-14T09:26:35+00:00,2025-02-18T19:04:53+00:00,13,https://github.com/pingcap/tidb/pull/59558,59558.0,2025-02-18T19:04:53+00:00,https://github.com/pingcap/tidb/pull/59558,0,1,0,1,56,5,0,61,105.63833333333334,release-note;size/M;needs-cherry-pick-release-6.5;approved;lgtm,False,True,normal,networking,"[{""filename"": ""br/pkg/storage/s3.go"", ""lines_added"": 56, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
pingcap/tidb,46324,etcd client leak when a table uses AUTO_ID_CACHE=1 and auto_increment ,## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required) nowhere close this etcd client https://github.com/pingcap/tidb/blob/1769f3a1ac3674b9267dca55bf1fc261ac804325/meta/autoid/autoid.go#L588 <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required) this code is introduced in tidb version >= 6.4.0 <!-- Paste the output of SELECT tidb_version() -->  ,2023-08-22T14:50:25+00:00,2023-11-06T05:54:42+00:00,1,https://github.com/pingcap/tidb/issues/46324,49049.0,2023-11-30T15:41:49+00:00,https://github.com/pingcap/tidb/pull/49049,0,1,0,1,1,0,0,1,2400.8566666666666,type/bug;sig/sql-infra;severity/major;affects-6.4;affects-6.5;affects-6.6;affects-7.0;affects-7.1;affects-7.2;fixes-6.5.6;fixes-7.1.3;fixes-7.5.0;report/customer,False,True,normal,ui,"[{""filename"": ""ddl/ddl.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9080,[Bug] get message from tiered store from cache too large,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Ubuntu 20.0  ### RocketMQ version  5.3.1  ### JDK Version  1.8.0  ### Describe the Bug   get message from tiered store from cache too large  ### Steps to Reproduce  1. open tiered storage ``` messageStorePlugIn=org.apache.rocketmq.tieredstore.TieredMessageStore tieredBackendServiceProvider=org.apache.rocketmq.tieredstore.provider.posix.PosixFileSegment tieredStoreFilePath=/xxx ``` 2. send message size 4M using benchmark  ``` ./producer.sh -n localhost:9876 -t test -s 4194304 ```  3. set force fetch from tiered storage ``` tieredStorageLevel=FORCE ``` 4. consumer pull message twice, the first time ok ,second time fail. error message: io.netty.handler.codec.TooLongFrameException：Adjusted frame length exceeds 16777216: 40016599 discarded    ### What Did You Expect to See?  consumer pull message success  ### What Did You See Instead?  consumer pull message twice, the first time ok ,second time fail. error message:  io.netty.handler.codec.TooLongFrameException：Adjusted frame length exceeds 16777216: 40016599 discarded   ### Additional Context  _No response_",2024-12-25T12:44:05+00:00,2024-12-26T06:49:01+00:00,0,https://github.com/apache/rocketmq/issues/9080,9086.0,2024-12-30T07:21:26+00:00,https://github.com/apache/rocketmq/pull/9086,0,1,0,1,2,3,0,5,114.6225,,False,True,normal,functional,"[{""filename"": ""tieredstore/src/main/java/org/apache/rocketmq/tieredstore/core/MessageStoreFetcherImpl.java"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8686,[Bug] Network jitter encountered while rebalancing results in repeated consumption or errors in pop mode,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  When pop consumption mode is enabled for topic-a, the client sets isRebanlance=false ![293b0204-1fda-4d5f-80f1-14042da21dc2](https://github.com/user-attachments/assets/5c84d71a-7552-4bdb-be2c-e5fb9f41219f) ![1501d98d-a6d2-4ca3-86dd-aedc51d70e3e](https://github.com/user-attachments/assets/dfc89547-09d1-481e-808e-8fa1cf7e5510) ![39ae3686-1817-4c6a-855b-20ab8f7723b6](https://github.com/user-attachments/assets/cbdc844b-768a-4ff8-a608-a5a42c67464c) ![5f8f2576-dd0b-4b52-92d4-b4dbdb78aecd](https://github.com/user-attachments/assets/6d3b6555-1773-45e1-8a11-cd30c2f141c5) Client rebalancing does not make pop consumption pattern judgment  ### Steps to Reproduce  The setMessageRequestMode interface of MQAdmin is called, the broker is network isolated from the client, the client is set isRebanlance=false, the network is restored after some time, and it is found that there is no pop consumption on the topic  ### What Did You Expect to See?  Topics are consumed in pop mode  ### What Did You See Instead?  Topics are not consumed in pop mode  ### Additional Context  _No response_",2024-09-12T03:13:15+00:00,2024-12-25T06:38:59+00:00,0,https://github.com/apache/rocketmq/issues/8686,8958.0,2024-12-25T06:38:57+00:00,https://github.com/apache/rocketmq/pull/8958,0,1,0,1,1,52,0,53,2499.4283333333333,,False,True,normal,networking,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/RebalanceImpl.java"", ""lines_added"": 1, ""lines_deleted"": 52, ""file_type"": ""app_code""}]",,False
pingcap/tidb,57418,table: fix wrong handle comparation in index double write,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #57414  Problem Summary:  ### What changed and how does it work?  Unlike other indices, global index encodes the value of its key-value pairs with a partition ID:  https://github.com/pingcap/tidb/blob/2ca497e23e1488a1675dfe5793296b8b367c261b/pkg/tablecodec/tablecodec.go#L1630-L1649  So during the double write process, we may do comparison between partition handles and non-partition handles.  https://github.com/pingcap/tidb/blob/2ca497e23e1488a1675dfe5793296b8b367c261b/pkg/table/tables/index.go#L419-L432  For example, in #57414, `h` is the handle of global index `i1` whose type is `PartitionHandle` while `oh` is `IntHandle` of the current index. Thus, `!h.Equal(oh)` will return false.    ### Check List  Tests <!-- At least one of them must be included. -->  - [X] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2024-11-15T09:23:46+00:00,2024-11-19T02:44:41+00:00,6,https://github.com/pingcap/tidb/pull/57418,57418.0,2024-11-19T02:44:41+00:00,https://github.com/pingcap/tidb/pull/57418,0,5,0,5,41,9,0,50,89.34861111111111,size/M;release-note-none;ok-to-test;approved;lgtm;needs-cherry-pick-release-8.5,False,True,normal,database,"[{""filename"": ""pkg/ddl/index.go"", ""lines_added"": 1, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/index_modify_test.go"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kv/key.go"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/kv/key_test.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/table/tables/index.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7671,[Bug] Producer connection is incorrect when use otel statistics,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos7  ### RocketMQ version  5.1.5.SNAPSHOT  ### JDK Version  jdk1.8  ### Describe the Bug  the producer connections is actually include the group of ""CLIENT_INNER_PRODUCER"" which doesn't want to be counted to user. Chinese version： 生产者的数量统计结果包含了CLIENT_INNER_PRODUCER 这个内部组的生产者，对用户展示的结果不符合实际启动的生产者数量  ### Steps to Reproduce  1. use otel metrics minitor producer connections 2. when start ""N"" consumer client and no producer client   ### What Did You Expect to See?  producer connection is zero  ### What Did You See Instead?  producer connection is ""N"" ,the same with consumer connections  ### Additional Context  _No response_",2023-12-18T06:51:19+00:00,2024-12-22T00:11:09+00:00,2,https://github.com/apache/rocketmq/issues/7671,7673.0,,https://github.com/apache/rocketmq/pull/7673,0,1,0,1,11,3,0,14,8873.330555555556,stale,False,True,normal,networking,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/metrics/BrokerMetricsManager.java"", ""lines_added"": 11, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
apache/pulsar,23797,[fix][broker] Continue using the next provider for authentication if one fails,"### Motivation  When using Pulsar admin version 2.10.x with JWT authentication and a broker running 3.0.x (forked from Apache Pulsar) configured with both Kerberos and JWT authentication providers.  When a request is without the authentication method name, the broker iterates through each authentication provider to authenticate the request, if authentication data is valid, the broker acts on the request. In this scenario, I ensured that the JWT provider was configured correctly and the token was valid. However, I still encountered an authentication error.  The root cause is that the `AuthenticationProviderList` only catches the `AuthenticationException` exception, If an authentication provider throws an exception of a different type, the authentication process will be terminated.  ### Modifications  - `AuthenticationProviderList` catches the `Exception` exception, and then uses the `AuthenticationException` to wrap the original exception.  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->",2024-12-31T09:45:13+00:00,2025-01-02T07:14:12+00:00,4,https://github.com/apache/pulsar/pull/23797,23797.0,2025-01-02T07:14:12+00:00,https://github.com/apache/pulsar/pull/23797,0,2,0,2,154,35,0,189,45.48305555555555,type/bug;doc-not-needed;area/authn;ready-to-test;cherry-picked/branch-3.0;cherry-picked/branch-3.3;cherry-picked/branch-4.0;release/3.0.9;release/3.3.4;release/4.0.2,False,True,normal,configuration,"[{""filename"": ""pulsar-broker-common/src/main/java/org/apache/pulsar/broker/authentication/AuthenticationProviderList.java"", ""lines_added"": 26, ""lines_deleted"": 34, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker-common/src/test/java/org/apache/pulsar/broker/authentication/AuthenticationProviderListTest.java"", ""lines_added"": 128, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",authentication,False
apache/pulsar,20635,[Bug] pulsar keep creating dead letter queue producer and exceed the maximum limit,"### Search before asking  - [X] I searched in the [issues](https://github.com/apache/pulsar/issues) and found nothing similar.   ### Version 2.11   ### Minimal reproduce step Use pulsar java client library to create a consumer with dlq prodcer.  1. after application starts, stop bookie 2. produce some message to the queue and trigger the consumer 3.  check the log.  It seems keep creating dead letter queue producer, and eventually hit the maximum limit  ### What did you expect to see?  extra producer should not be created if there is an issue on pulsar  ### What did you see instead?  created over 10000 producers and eventually exceed the limits  ### Anything else? see logs  ``` -- Starting Pulsar producer perf with config: {""topicName"":""persistent://public/default/my-topic"",""producerName"":null,""sendTimeoutMs"":30000,""blockIfQueueFull"":false,""maxPendingMessages"":0,""maxPendingMessagesAcrossPartitions"":0,""messageRoutingMode"":""RoundRobinPartition"",""hashingScheme"":""JavaStringHash"",""cryptoFailureAction"":""FAIL"",""batchingMaxPublishDelayMicros"":1000,""batchingPartitionSwitchFrequencyByPublishDelay"":10,""batchingMaxMessages"":1000,""batchingMaxBytes"":131072,""batchingEnabled"":true,""chunkingEnabled"":false,""chunkMaxMessageSize"":-1,""compressionType"":""NONE"",""initialSequenceId"":null,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""multiSchema"":true,""accessMode"":""Shared"",""lazyStartPartitionedProducers"":false,""properties"":{},""initialSubscriptionName"":null} -- Pulsar client config: {""serviceUrl"":""pulsar://pulsar:6650"",""authPluginClassName"":null,""authParams"":null,""authParamMap"":null,""operationTimeoutMs"":30000,""lookupTimeoutMs"":30000,""statsIntervalSeconds"":60,""numIoThreads"":1,""numListenerThreads"":1,""connectionsPerBroker"":1,""connectionMaxIdleSeconds"":180,""useTcpNoDelay"":true,""useTls"":false,""tlsKeyFilePath"":null,""tlsCertificateFilePath"":null,""tlsTrustCertsFilePath"":null,""tlsAllowInsecureConnection"":false,""tlsHostnameVerificationEnable"":false,""concurrentLookupRequest"":5000,""maxLookupRequest"":50000,""maxLookupRedirects"":20,""maxNumberOfRejectedRequestPerConnection"":50,""keepAliveIntervalSeconds"":30,""connectionTimeoutMs"":10000,""requestTimeoutMs"":60000,""initialBackoffIntervalNanos"":100000000,""maxBackoffIntervalNanos"":60000000000,""enableBusyWait"":false,""listenerName"":null,""useKeyStoreTls"":false,""sslProvider"":null,""tlsKeyStoreType"":""JKS"",""tlsKeyStorePath"":null,""tlsKeyStorePassword"":null,""tlsTrustStoreType"":""JKS"",""tlsTrustStorePath"":null,""tlsTrustStorePassword"":null,""tlsCiphers"":[],""tlsProtocols"":[],""memoryLimitBytes"":67108864,""proxyServiceUrl"":null,""proxyProtocol"":null,""enableTransaction"":true,""dnsLookupBindAddress"":null,""dnsLookupBindPort"":0,""socks5ProxyAddress"":null,""socks5ProxyUsername"":null,""socks5ProxyPassword"":null} [persistent://public/default/dlq] [null] Creating producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/dlq] [pulsar-dev-5-10032] Created producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/my-topic] failed to get schema : org.apache.pulsar.client.api.PulsarClientException: {""errorMsg"":""org.apache.pulsar.broker.service.schema.exceptions.SchemaException: Bookie handle is not available - ledger=35399 - operation=Failed to read entry - entry=0"",""reqId"":4059106653410978868, ""remote"":""pulsar/10.204.70.7:6650"", ""local"":""/10.204.68.122:44372""} Dead letter producer exception with topic: persistent://public/default/dlq Starting Pulsar producer perf with config: {""topicName"":""persistent://public/default/dlq"",""producerName"":null,""sendTimeoutMs"":30000,""blockIfQueueFull"":false,""maxPendingMessages"":0,""maxPendingMessagesAcrossPartitions"":0,""messageRoutingMode"":""RoundRobinPartition"",""hashingScheme"":""JavaStringHash"",""cryptoFailureAction"":""FAIL"",""batchingMaxPublishDelayMicros"":1000,""batchingPartitionSwitchFrequencyByPublishDelay"":10,""batchingMaxMessages"":1000,""batchingMaxBytes"":131072,""batchingEnabled"":true,""chunkingEnabled"":false,""chunkMaxMessageSize"":-1,""compressionType"":""NONE"",""initialSequenceId"":null,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""multiSchema"":true,""accessMode"":""Shared"",""lazyStartPartitionedProducers"":false,""properties"":{},""initialSubscriptionName"":null} Pulsar client config: {""serviceUrl"":""pulsar://pulsar:6650"",""authPluginClassName"":null,""authParams"":null,""authParamMap"":null,""operationTimeoutMs"":30000,""lookupTimeoutMs"":30000,""statsIntervalSeconds"":60,""numIoThreads"":1,""numListenerThreads"":1,""connectionsPerBroker"":1,""connectionMaxIdleSeconds"":180,""useTcpNoDelay"":true,""useTls"":false,""tlsKeyFilePath"":null,""tlsCertificateFilePath"":null,""tlsTrustCertsFilePath"":null,""tlsAllowInsecureConnection"":false,""tlsHostnameVerificationEnable"":false,""concurrentLookupRequest"":5000,""maxLookupRequest"":50000,""maxLookupRedirects"":20,""maxNumberOfRejectedRequestPerConnection"":50,""keepAliveIntervalSeconds"":30,""connectionTimeoutMs"":10000,""requestTimeoutMs"":60000,""initialBackoffIntervalNanos"":100000000,""maxBackoffIntervalNanos"":60000000000,""enableBusyWait"":false,""listenerName"":null,""useKeyStoreTls"":false,""sslProvider"":null,""tlsKeyStoreType"":""JKS"",""tlsKeyStorePath"":null,""tlsKeyStorePassword"":null,""tlsTrustStoreType"":""JKS"",""tlsTrustStorePath"":null,""tlsTrustStorePassword"":null,""tlsCiphers"":[],""tlsProtocols"":[],""memoryLimitBytes"":67108864,""proxyServiceUrl"":null,""proxyProtocol"":null,""enableTransaction"":true,""dnsLookupBindAddress"":null,""dnsLookupBindPort"":0,""socks5ProxyAddress"":null,""socks5ProxyUsername"":null,""socks5ProxyPassword"":null} [persistent://public/default/dlq] [null] Creating producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/dlq] [pulsar-dev-5-10033] Created producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/my-topic] failed to get schema : org.apache.pulsar.client.api.PulsarClientException: {""errorMsg"":""org.apache.pulsar.broker.service.schema.exceptions.SchemaException: Bookie handle is not available - ledger=35399 - operation=Failed to read entry - entry=0"",""reqId"":4059106653410978872, ""remote"":""pulsar/10.204.70.7:6650"", ""local"":""/10.204.68.122:44372""} Dead letter producer exception with topic: persistent://public/default/dlq Starting Pulsar producer perf with config: {""topicName"":""persistent://public/default/dlq"",""producerName"":null,""sendTimeoutMs"":30000,""blockIfQueueFull"":false,""maxPendingMessages"":0,""maxPendingMessagesAcrossPartitions"":0,""messageRoutingMode"":""RoundRobinPartition"",""hashingScheme"":""JavaStringHash"",""cryptoFailureAction"":""FAIL"",""batchingMaxPublishDelayMicros"":1000,""batchingPartitionSwitchFrequencyByPublishDelay"":10,""batchingMaxMessages"":1000,""batchingMaxBytes"":131072,""batchingEnabled"":true,""chunkingEnabled"":false,""chunkMaxMessageSize"":-1,""compressionType"":""NONE"",""initialSequenceId"":null,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""multiSchema"":true,""accessMode"":""Shared"",""lazyStartPartitionedProducers"":false,""properties"":{},""initialSubscriptionName"":null} Pulsar client config: {""serviceUrl"":""pulsar://pulsar:6650"",""authPluginClassName"":null,""authParams"":null,""authParamMap"":null,""operationTimeoutMs"":30000,""lookupTimeoutMs"":30000,""statsIntervalSeconds"":60,""numIoThreads"":1,""numListenerThreads"":1,""connectionsPerBroker"":1,""connectionMaxIdleSeconds"":180,""useTcpNoDelay"":true,""useTls"":false,""tlsKeyFilePath"":null,""tlsCertificateFilePath"":null,""tlsTrustCertsFilePath"":null,""tlsAllowInsecureConnection"":false,""tlsHostnameVerificationEnable"":false,""concurrentLookupRequest"":5000,""maxLookupRequest"":50000,""maxLookupRedirects"":20,""maxNumberOfRejectedRequestPerConnection"":50,""keepAliveIntervalSeconds"":30,""connectionTimeoutMs"":10000,""requestTimeoutMs"":60000,""initialBackoffIntervalNanos"":100000000,""maxBackoffIntervalNanos"":60000000000,""enableBusyWait"":false,""listenerName"":null,""useKeyStoreTls"":false,""sslProvider"":null,""tlsKeyStoreType"":""JKS"",""tlsKeyStorePath"":null,""tlsKeyStorePassword"":null,""tlsTrustStoreType"":""JKS"",""tlsTrustStorePath"":null,""tlsTrustStorePassword"":null,""tlsCiphers"":[],""tlsProtocols"":[],""memoryLimitBytes"":67108864,""proxyServiceUrl"":null,""proxyProtocol"":null,""enableTransaction"":true,""dnsLookupBindAddress"":null,""dnsLookupBindPort"":0,""socks5ProxyAddress"":null,""socks5ProxyUsername"":null,""socks5ProxyPassword"":null} [persistent://public/default/dlq] [null] Creating producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/dlq] [pulsar-dev-5-10034] Created producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/my-topic] failed to get schema : org.apache.pulsar.client.api.PulsarClientException: {""errorMsg"":""org.apache.pulsar.broker.service.schema.exceptions.SchemaException: Bookie handle is not available - ledger=35399 - operation=Failed to read entry - entry=0"",""reqId"":4059106653410978876, ""remote"":""pulsar/10.204.70.7:6650"", ""local"":""/10.204.68.122:44372""} Dead letter producer exception with topic: persistent://public/default/dlq Starting Pulsar producer perf with config: {""topicName"":""persistent://public/default/dlq"",""producerName"":null,""sendTimeoutMs"":30000,""blockIfQueueFull"":false,""maxPendingMessages"":0,""maxPendingMessagesAcrossPartitions"":0,""messageRoutingMode"":""RoundRobinPartition"",""hashingScheme"":""JavaStringHash"",""cryptoFailureAction"":""FAIL"",""batchingMaxPublishDelayMicros"":1000,""batchingPartitionSwitchFrequencyByPublishDelay"":10,""batchingMaxMessages"":1000,""batchingMaxBytes"":131072,""batchingEnabled"":true,""chunkingEnabled"":false,""chunkMaxMessageSize"":-1,""compressionType"":""NONE"",""initialSequenceId"":null,""autoUpdatePartitions"":true,""autoUpdatePartitionsIntervalSeconds"":60,""multiSchema"":true,""accessMode"":""Shared"",""lazyStartPartitionedProducers"":false,""properties"":{},""initialSubscriptionName"":null} Pulsar client config: {""serviceUrl"":""pulsar://pulsar:6650"",""authPluginClassName"":null,""authParams"":null,""authParamMap"":null,""operationTimeoutMs"":30000,""lookupTimeoutMs"":30000,""statsIntervalSeconds"":60,""numIoThreads"":1,""numListenerThreads"":1,""connectionsPerBroker"":1,""connectionMaxIdleSeconds"":180,""useTcpNoDelay"":true,""useTls"":false,""tlsKeyFilePath"":null,""tlsCertificateFilePath"":null,""tlsTrustCertsFilePath"":null,""tlsAllowInsecureConnection"":false,""tlsHostnameVerificationEnable"":false,""concurrentLookupRequest"":5000,""maxLookupRequest"":50000,""maxLookupRedirects"":20,""maxNumberOfRejectedRequestPerConnection"":50,""keepAliveIntervalSeconds"":30,""connectionTimeoutMs"":10000,""requestTimeoutMs"":60000,""initialBackoffIntervalNanos"":100000000,""maxBackoffIntervalNanos"":60000000000,""enableBusyWait"":false,""listenerName"":null,""useKeyStoreTls"":false,""sslProvider"":null,""tlsKeyStoreType"":""JKS"",""tlsKeyStorePath"":null,""tlsKeyStorePassword"":null,""tlsTrustStoreType"":""JKS"",""tlsTrustStorePath"":null,""tlsTrustStorePassword"":null,""tlsCiphers"":[],""tlsProtocols"":[],""memoryLimitBytes"":67108864,""proxyServiceUrl"":null,""proxyProtocol"":null,""enableTransaction"":true,""dnsLookupBindAddress"":null,""dnsLookupBindPort"":0,""socks5ProxyAddress"":null,""socks5ProxyUsername"":null,""socks5ProxyPassword"":null} [persistent://public/default/dlq] [null] Creating producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/dlq] [pulsar-dev-5-10035] Created producer on cnx [id: 0x910a07bb, L:/10.204.68.122:45104 - R:pulsar/10.204.67.17:6650] [persistent://public/default/my-topic] failed to get schema : org.apache.pulsar.client.api.PulsarClientException: {""errorMsg"":""org.apache.pulsar.broker.service.schema.exceptions.SchemaException: Bookie handle is not available - ledger=35399 - operation=Failed to read entry - entry=0"",""reqId"":4059106653410978880, ""remote"":""pulsar/10.204.70.7:6650"", ""local"":""/10.204.68.122:44372""} Dead letter producer exception with topic: persistent://public/default/dlq  ```  ### Are you willing to submit a PR?  - [ ] I'm willing to submit a PR!",2023-06-23T15:48:25+00:00,2025-01-09T19:10:50+00:00,6,https://github.com/apache/pulsar/issues/20635,23824.0,2025-01-09T19:10:49+00:00,https://github.com/apache/pulsar/pull/23824,1,4,0,5,423,164,4,583,13587.373333333331,type/bug;Stale;triage/lhotari/important,False,True,normal,configuration,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/DeadLetterTopicTest.java"", ""lines_added"": 93, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/RetryTopicTest.java"", ""lines_added"": 120, ""lines_deleted"": 46, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ConsumerImpl.java"", ""lines_added"": 198, ""lines_deleted"": 117, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/MultiTopicsConsumerImpl.java"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/resources/findbugsExclude.xml"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""config""}]",client,False
apache/rocketmq,9002,"[Bug] In the master-slave replication scenario, since the replication is based on bytes, the slave data will often be temporarily incomplete.","### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  all  ### RocketMQ version  develop  ### JDK Version  _No response_  ### Describe the Bug   auto In the master-slave replication scenario, since the replication is based on bytes, the slave data will often be temporarily incomplete.  <img width=""1090"" alt=""企业微信截图_06609cd0-8897-4495-a555-0e08fdf059ea"" src=""https://github.com/user-attachments/assets/c11da3b9-e81d-41b5-a852-f8c7e969f727"">   ### Steps to Reproduce  Use master-slave, then replicate and observe the slave's log.  ### What Did You Expect to See?  The slave node does not report an error when reputting.  ### What Did You See Instead?  The slave's reput thread will often report errors, but retry will succeed.  ### Additional Context  none",2024-11-29T01:37:57+00:00,2024-12-19T03:44:33+00:00,0,https://github.com/apache/rocketmq/issues/9002,9003.0,2024-12-19T03:44:32+00:00,https://github.com/apache/rocketmq/pull/9003,0,1,0,1,7,0,0,7,482.1097222222222,,False,True,normal,database,"[{""filename"": ""store/src/main/java/org/apache/rocketmq/store/CommitLog.java"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,22423,[admin][broker] Fix force delete subscription not working,"Fixes https://github.com/apache/pulsar/issues/22404  ### Motivation  Fix force delete subscription not working on persistent topics.  ### Modifications  <!-- Describe the modifications you've done. -->  ### Verifying this change  - [ ] Make sure that the change passes the CI checks.  *(Please pick either of the following options)*  This change is a trivial rework / code cleanup without any test coverage.  *(or)*  This change is already covered by existing tests, such as *(please describe tests)*.  *(or)*  This change added tests and can be verified as follows:  *(example:)*   - *Added integration tests for end-to-end deployment with large payloads (10MB)*   - *Extended integration test for recovery after broker failure*  ### Does this pull request potentially affect one of the following parts:  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  *If the box was checked, please highlight the changes*  - [ ] Dependencies (add or upgrade a dependency) - [ ] The public API - [ ] The schema - [ ] The default values of configurations - [ ] The threading model - [ ] The binary protocol - [ ] The REST endpoints - [ ] The admin CLI options - [ ] The metrics - [ ] Anything that affects deployment  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository:  ",2024-04-04T07:18:33+00:00,2024-04-04T15:08:45+00:00,2,https://github.com/apache/pulsar/pull/22423,22423.0,2024-04-04T15:08:45+00:00,https://github.com/apache/pulsar/pull/22423,0,2,0,2,32,3,0,35,7.836666666666667,type/bug;area/admin;doc-not-needed;ready-to-test;cherry-picked/branch-3.0;cherry-picked/branch-3.2;release/3.2.3;release/3.0.5,False,True,normal,configuration,"[{""filename"": ""pulsar-broker/src/main/java/org/apache/pulsar/broker/admin/impl/PersistentTopicsBase.java"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/broker/admin/PersistentTopicsTest.java"", ""lines_added"": 30, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7617,"[Bug] Condition returned by method ""testStateAndRecover"" in class org.apache.rocketmq.store.timer.TimerMessageStoreTest was not fulfilled within 5 seconds.","### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  UBuntu 20.04  ### RocketMQ version  development  ### JDK Version  _No response_  ### Describe the Bug  ``` org.awaitility.core.ConditionTimeoutException: Condition returned by method ""testStateAndRecover"" in class org.apache.rocketmq.store.timer.TimerMessageStoreTest was not fulfilled within 5 seconds. 	at org.apache.rocketmq.store.timer.TimerMessageStoreTest.testStateAndRecover(TimerMessageStoreTest.java:430)  ```  ### Steps to Reproduce  mvn run  ### What Did You Expect to See?  Test should pass  ### What Did You See Instead?  ``` Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project rocketmq-store: There are test failures.  ```  ### Additional Context  _No response_",2023-12-05T13:45:57+00:00,2024-12-08T00:11:52+00:00,2,https://github.com/apache/rocketmq/issues/7617,7618.0,,https://github.com/apache/rocketmq/pull/7618,0,2,0,2,2,2,0,4,8842.431944444445,stale,False,True,normal,networking,"[{""filename"": ""common/src/test/java/org/apache/rocketmq/common/utils/ConcurrentHashMapUtilsTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""store/src/test/java/org/apache/rocketmq/store/timer/TimerMessageStoreTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8995,[Bug]  Pull consumers should be found when querying consumersByWho,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  pull consumers can't be found when querying consumers  ### Steps to Reproduce  pull consumers can be found when querying consumers  ### What Did You Expect to See?  pull consumers can be found when querying consumers  ### What Did You See Instead?  null  ### Additional Context  _No response_",2024-11-28T06:44:37+00:00,2024-12-05T09:45:59+00:00,0,https://github.com/apache/rocketmq/issues/8995,8996.0,,https://github.com/apache/rocketmq/pull/8996,0,1,0,1,12,0,0,12,171.02277777777778,,False,True,normal,database,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/client/ConsumerManager.java"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
thingsboard/thingsboard,12751,[Input node] Use correct queue name from msg,"## Pull Request description  Input rule node always uses Main queue for enqueue!  https://thingsboard-portal.atlassian.net/browse/PROD-5658  ## General checklist  - [ ] You have reviewed the guidelines [document](https://docs.google.com/document/d/1wqcOafLx5hth8SAg4dqV_LV3un3m5WYR8RdTJ4MbbUM/edit?usp=sharing). - [ ] [Labels](https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/managing-labels#about-labels) that classify your pull request have been added. - [ ] The [milestone](https://docs.github.com/en/issues/using-labels-and-milestones-to-track-work/about-milestones) is specified and corresponds to fix version.   - [ ] Description references specific [issue](https://github.com/thingsboard/thingsboard/issues). - [ ] Description contains human-readable scope of changes. - [ ] Description contains brief notes about what needs to be added to the documentation. - [ ] No merge conflicts, commented blocks of code, code formatting issues. - [ ] Changes are backward compatible or upgrade script is provided. - [ ] Similar PR is opened for PE version to simplify merge. Crosslinks between PRs added. Required for internal contributors only.    ## Front-End feature checklist  - [ ] Screenshots with affected component(s) are added. The best option is to provide 2 screens: before and after changes; - [ ] If you change the widget or other API, ensure it is backward-compatible or upgrade script is present. - [ ] Ensure new API is documented [here](https://github.com/thingsboard/thingsboard-ui-help)  ## Back-End feature checklist  - [ ] Added corresponding unit and/or integration test(s). Provide written explanation in the PR description if you have failed to add tests. - [ ] If new dependency was added: the dependency tree is checked for conflicts. - [ ] If new service was added: the service is marked with corresponding @TbCoreComponent, @TbRuleEngineComponent, @TbTransportComponent, etc. - [ ] If new REST API was added: the RestClient.java was updated, issue for [Python REST client](https://github.com/thingsboard/thingsboard-python-rest-client) is created. - [ ] If new yml property was added: make sure a description is added (above or near the property).    ",2025-02-25T14:18:07+00:00,2025-02-26T09:12:52+00:00,0,https://github.com/thingsboard/thingsboard/pull/12751,12751.0,2025-02-26T09:12:52+00:00,https://github.com/thingsboard/thingsboard/pull/12751,0,2,0,2,57,6,0,63,18.9125,bug;Rule Engine,False,True,normal,ui,"[{""filename"": ""application/src/main/java/org/thingsboard/server/actors/ruleChain/DefaultTbContext.java"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""application/src/test/java/org/thingsboard/server/actors/rule/DefaultTbContextTest.java"", ""lines_added"": 54, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",application,False
pingcap/tidb,59560,Memory leak risk in session pool usage in stats sync load,## Bug Report  The exact reproduction steps are still unknown. But we observed memory leaks in production clusters:   ![Image](https://github.com/user-attachments/assets/5553ba2b-b532-4f98-9a76-1a4ed14bfaac)   The code in question is similar to what we met in https://github.com/pingcap/tidb/issues/54022:  v7.5.3:  https://github.com/pingcap/tidb/blob/3c599cd04777a0b26f1856880a428d2e1fff1bc7/pkg/statistics/handle/handle_hist.go#L278-L281   master:  https://github.com/pingcap/tidb/blob/72a1145688e5c4cab368a75875cb9eb704b3583e/pkg/statistics/handle/syncload/stats_syncload.go#L303-L306,2025-02-15T11:19:17+00:00,2025-02-18T13:48:28+00:00,1,https://github.com/pingcap/tidb/issues/59560,59671.0,,https://github.com/pingcap/tidb/pull/59671,0,20,1,21,9905,0,0,9851,74.48638888888888,type/bug;sig/planner;severity/major;affects-7.5;affects-8.1;report/customer;impact/leak;affects-8.5,False,True,normal,performance,"[{""filename"": ""domain/infosync/info.go"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""domain/session_pool_test.go"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/bindinfo/global_handle.go"", ""lines_added"": 548, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/bindinfo/global_handle_test.go"", ""lines_added"": 918, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/bindinfo/tests/cross_db_binding_test.go"", ""lines_added"": 347, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/notifier/testkit_test.go"", ""lines_added"": 598, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/domain/domain.go"", ""lines_added"": 3498, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/rule_collect_plan_stats.go"", ""lines_added"": 456, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/bootstrap.go"", ""lines_added"": 834, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/handle.go"", ""lines_added"": 252, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/lockstats/lock_stats.go"", ""lines_added"": 311, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/storage/read.go"", ""lines_added"": 845, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/syncload/stats_syncload.go"", ""lines_added"": 629, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/util/BUILD.bazel"", ""lines_added"": 54, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/statistics/handle/util/pool.go"", ""lines_added"": 66, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/util/util.go"", ""lines_added"": 289, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/util/util_test.go"", ""lines_added"": 74, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/util/session_pool.go"", ""lines_added"": 131, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""server/server.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""testkit/mocksessionmanager.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""util/processinfo.go"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",server,False
apache/rocketmq,9014,[Bug] clusterAclConfigVersion command execution failed,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  mac  ### RocketMQ version  develop  ### JDK Version  jdk1.8  ### Describe the Bug <img width=""943"" alt=""image"" src=""https://github.com/user-attachments/assets/3ada1849-7fa4-49d1-88a3-f297750f0440"">  <img width=""934"" alt=""image"" src=""https://github.com/user-attachments/assets/e7e2a69f-29a0-45b3-98f2-687aaf399dff""> The clusterAclConfigVersion command fails to execute and a timeout exception is reported  ### Steps to Reproduce  1. Set broker's aclEnable=false 2. Execute clusterAclConfigVersion command  ### What Did You Expect to See?  Normal operation, no timeout error.  ### What Did You See Instead?  Timeout exception  ### Additional Context  _No response_",2024-12-02T10:50:34+00:00,2024-12-03T01:56:42+00:00,0,https://github.com/apache/rocketmq/issues/9014,9017.0,2024-12-03T01:56:41+00:00,https://github.com/apache/rocketmq/pull/9017,0,1,0,1,6,0,0,6,15.101944444444444,,False,True,normal,configuration,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessor.java"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,9006,[Bug] Delayed messages that exceed the default storage expiration time cannot be uploaded to tiered storage,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  CentOS Linux 8  ### RocketMQ version  version: 5.1.4  ### JDK Version  java version ""1.8.0_221"" Java(TM) SE Runtime Environment (build 1.8.0_221-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode)  ### Describe the Bug  1) 延时消息的延时时间超过本地存储的过期时间，达到过期时间时就会被清理掉，而此时因为延时消息没有被投递到真实的 Topic，所以就会发现多级存储也没有对应的延时消息，最后消费者就无法消费这条延时消息 2) 这样就会导致即使配置了多级存储，本地存储的过期时间还会受制于延时消息的延时时间，这样就还是没有办法减少本地存储的过期时间  (1) If the delay time of a delayed message exceeds the default storage expiration time, the message will be cleaned up when it expires. However, since the delayed message hasn’t been delivered to the actual topic yet, there will be no corresponding delayed message in the tiered storage. As a result, the consumer won’t be able to consume this delayed message. (2)This means that even though tiered storage is configured, the expiration time of the default storage is still affected by the delay time of the message. Therefore, it's not possible to reduce the expiration time of the local storage.  ### Steps to Reproduce  1) 本地存储设置过期时间为 1H，多级存储设置过期时间为 6H，删除过期消息的时间为 1H 后 2) 分别发送 1 条延时消息等级为 17 (1H) 和等级为 18 (2H) 的延时消息到延时消息的 Topic 3) 然后持续发送大量普通消息到普通消息的 Topic，确保会生成多个 CommitLog，确保过期时可以删除延时消息 4) 等待 1H 后启动消费者消费延时消息，会发现延时等级为 17 的消息可以消费，但是延时等级为 18 的消息无法消费  (1) The default storage expiration time is set to 1 hour, while the tiered storage expiration time is set to 6 hours. Expired messages are deleted 1 hour after expiration. (2) A delayed message with level 17 (1 hour delay) and another with level 18 (2 hours delay) are sent to the delayed message topic. (3) At the same time, a large number of regular messages are sent to the normal message topic to ensure multiple CommitLogs are generated, ensuring that expired delayed messages can be deleted. (4) After waiting for 1 hour, a consumer is started to consume the delayed messages. It is observed that the delayed message with level 17 (1 hour delay) can be consumed, but the message with level 18 (2 hours delay) cannot be consumed.  ### What Did You Expect to See?  1) 梳理源码发现是延时消息的对应的系统 Topic 没有上传到多级存储，最终导致本地延时消息过期后，也没法把延时消息投递真实的 Topic 2) 此外，在投递真正的延时消息时只会在本地存储查询，不会去多级存储查，所以只将系统 Topic 的消息上传到多级存储是不够的，还需要支持在投递延时消息时从多级存储拉取  (1) After reviewing the source code, it was found that the system topic corresponding to the delayed message was not uploaded to the tiered storage. As a result, when the local delayed message expires, it cannot be delivered to the real topic.  (2) Additionally, when delivering actual delayed messages, the system only queries the default storage and does not check the tiered storage. Therefore, it’s not enough to simply upload the system topic messages to tiered storage; there also needs to be support for fetching delayed messages from tiered storage during delivery.  ### What Did You See Instead?  1) 这个 Bug 会导致即使配置了多级存储，本地存储的过期时间还会受制于延时消息的延时时间，这样就还是没有办法减少本地存储的过期时间  (1) This bug causes the default storage expiration time to still be dependent on the delay time of the delayed messages, even if tiered storage is configured. As a result, it is not possible to reduce the expiration time of the default storage.  ### Additional Context  _No response_",2024-11-29T05:43:48+00:00,2024-12-02T07:17:57+00:00,0,https://github.com/apache/rocketmq/issues/9006,6477.0,2023-03-27T05:40:39+00:00,https://github.com/apache/rocketmq/pull/6477,0,1,0,1,1,0,0,1,-14712.0525,,False,True,normal,configuration,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/EndTransactionProcessor.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59841,IMPORT INTO: invalid kv range when use global sort,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required) import into 200TB data with global sort <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required) import successfully ### 3. What did you see instead (Required) [2025/02/26 10:28:15.265 +00:00] [WARN] [scheduler.go:453] [""generate part of subtasks failed""] [task-id=30001] [task-type=ImportInto] [allocated-slots=true] [error=""invalid kv range, startKey: 74800000000000006a5f728000000001714882, endKey: 74800000000000006a5f7280000000003e0ab2""] ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() --> master ",2025-02-28T06:31:49+00:00,2025-03-06T11:01:06+00:00,0,https://github.com/pingcap/tidb/issues/59841,59971.0,2025-03-10T04:03:33+00:00,https://github.com/pingcap/tidb/pull/59971,0,2,0,2,16,1,0,17,237.5288888888889,type/bug;severity/moderate,False,True,normal,database,"[{""filename"": ""pkg/lightning/backend/external/iter.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/lightning/backend/external/split_test.go"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8984,[Bug] Broker switch enableMixedMessageType doesn't work,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  all  ### RocketMQ version  develop  ### JDK Version  1.8  ### Describe the Bug  Broker switch enableMixedMessageType doesn't work. MIXED type topic can be created even if this switch is off.  ### Steps to Reproduce  1. Set enableMixedMessageType to false. 2. Try create MIXED message type topic.  ### What Did You Expect to See?  Creation failed.  ### What Did You See Instead?  Creation succeeded.  ### Additional Context  _No response_",2024-11-25T12:38:50+00:00,2024-12-02T02:01:52+00:00,0,https://github.com/apache/rocketmq/issues/8984,8986.0,2024-12-02T02:01:51+00:00,https://github.com/apache/rocketmq/pull/8986,0,2,0,2,61,10,0,71,157.38361111111112,,False,True,normal,functional,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessor.java"", ""lines_added"": 19, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""broker/src/test/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessorTest.java"", ""lines_added"": 42, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7526,"[Bug] The interface definition of queryMessage is not clear, and the query result may exceed the maxNum passed.","### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  none  ### RocketMQ version  4.x, 5.x  ### JDK Version  _No response_  ### Describe the Bug  When multiple brokers are involved, the internal implementation of this SDK is to query simultaneously and aggregate the results. In this way, each broker may return up to 64 records. Currently, there is no truncation in place for the final result, so it may exceed the parameter setting.  ### Steps to Reproduce  review the code  ### What Did You Expect to See?  return size <=maxNum  ### What Did You See Instead?  return size > maxNum  ### Additional Context  _No response_",2023-11-02T02:50:47+00:00,2024-12-02T00:11:19+00:00,3,https://github.com/apache/rocketmq/issues/7526,3718.0,2023-07-21T01:31:57+00:00,https://github.com/apache/rocketmq/pull/3718,0,8,0,8,1133,175,0,1308,-2497.313888888889,stale,False,True,normal,database,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/MQClientManager.java"", ""lines_added"": 20, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/producer/DefaultMQProducerImpl.java"", ""lines_added"": 36, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/producer/DefaultMQProducer.java"", ""lines_added"": 339, ""lines_deleted"": 162, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/producer/MQProducer.java"", ""lines_added"": 14, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/producer/ProduceAccumulator.java"", ""lines_added"": 510, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/test/java/org/apache/rocketmq/client/producer/DefaultMQProducerTest.java"", ""lines_added"": 37, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""client/src/test/java/org/apache/rocketmq/client/producer/ProduceAccumulatorTest.java"", ""lines_added"": 176, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""common/src/main/java/org/apache/rocketmq/common/message/MessageBatch.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,46093,Duplicate entry when using BR to restore a NONCLUSTERED AUTO_ID_CACHE=1 table,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  1. prepare  ```sql use test; create table t (a int primary key nonclustered auto_increment, b int) auto_id_cache = 1; insert into t (b) values (1), (2), (3); show table t next_row_id; -- _TIDB_ROWID = 30001, AUTO_INCREMENT = 4 ```  2. backup & restore  ```sql backup table t to 'local:///tmp/br'; drop table t; restore table t from 'local:///tmp/br'; ```  ### 2. What did you expect to see? (Required)  (explained below)  ### 3. What did you see instead (Required)  3. the next_row_id are wrong ```sql show table t next_row_id; -- _TIDB_ROWID = 1, AUTO_INCREMENT = 4001 ```  4. try insert more rows, result in duplicate keys of `_tidb_rowid` ```sql insert into t (b) values (4), (5), (6); -- ERROR 1062 (23000): Duplicate entry '1' for key 't.PRIMARY' ```  5. after `_tidb_rowid` increases to the available zone, we see the primary key's auto-inc values have a gap ```sql insert into t (b) values (7), (8), (9); select * from t; ``` ``` +------+------+ | a    | b    | +------+------+ |    1 |    1 | |    2 |    2 | |    3 |    3 | | 4004 |    7 | | 4005 |    8 | | 4006 |    9 | +------+------+ ```  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ``` Release Version: v7.1.1 Edition: Community Git Commit Hash: cf441574864be63938524e7dfcf7cc659edc3dd8 Git Branch: heads/refs/tags/v7.1.1 UTC Build Time: 2023-07-19 10:20:53 GoVersion: go1.20.6 Race Enabled: false TiKV Min Version: 6.2.0-alpha Check Table Before Drop: false Store: tikv ```",2023-08-15T05:38:50+00:00,2023-08-18T03:47:02+00:00,7,https://github.com/pingcap/tidb/issues/46093,46338.0,2023-10-07T10:09:23+00:00,https://github.com/pingcap/tidb/pull/46338,0,3,2,5,170,8,0,51,1276.5091666666667,type/bug;sig/sql-infra;severity/critical;component/br;affects-6.4;affects-6.5;affects-6.6;affects-7.0;affects-7.1;affects-7.2;affects-7.3;fixes-6.5.5,False,True,normal,database,"[{""filename"": ""br/pkg/backup/client.go"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/tests/br_autoid/run.sh"", ""lines_added"": 51, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""br/tests/run_group.sh"", ""lines_added"": 76, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""ddl/ddl_api.go"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""parser/model/model.go"", ""lines_added"": 23, ""lines_deleted"": 8, ""file_type"": ""app_code""}]",ddl,False
apache/rocketmq,7590,[Bug] An unstable unit test:  ConcurrentHashMapUtilsTest.computeIfAbsent,"### Before Creating the Bug Report  - [x] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  ubuntu 20.04  ### RocketMQ version  develop  ### JDK Version  _No response_  ### Describe the Bug  failed detail :  ``` java.lang.IllegalStateException: Recursive update 	at org.apache.rocketmq.common.utils.ConcurrentHashMapUtilsTest.lambda$3(ConcurrentHashMapUtilsTest.java:39) 	at org.apache.rocketmq.common.utils.ConcurrentHashMapUtilsTest.computeIfAbsent(ConcurrentHashMapUtilsTest.java:39) ```  ### Steps to Reproduce  by running  ``` mvn test ```  ### What Did You Expect to See?  Test should pass  ### What Did You See Instead?   ``` [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project rocketmq-common: There are test failures.  ```  ### Additional Context  _No response_",2023-11-28T19:22:10+00:00,2024-12-01T00:13:04+00:00,3,https://github.com/apache/rocketmq/issues/7590,7591.0,,https://github.com/apache/rocketmq/pull/7591,0,1,0,1,1,1,0,2,8836.848333333333,stale,False,True,normal,functional,"[{""filename"": ""common/src/test/java/org/apache/rocketmq/common/utils/ConcurrentHashMapUtilsTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8991,[Bug] send Heartbeat v2 bug,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Runtime platform environment Ubuntu 22.04.2 LTS  ### RocketMQ version  5.3.2-SNAPSHOT  ### JDK Version  _No response_  ### Describe the Bug  1.8  ### Steps to Reproduce  prepareHeartbeatData should not be set by default subscriptionDataSet data ![image](https://github.com/user-attachments/assets/0e61c960-6573-4751-abe3-238591782f50)  What we actually need is heartbeatDataWithSub with subscription data and heartbeatDataWithoutSub without subscription data. ![image](https://github.com/user-attachments/assets/0e9c27a2-2122-412f-a346-a0046e081225)    ### What Did You Expect to See?  no  ### What Did You See Instead?  no  ### Additional Context  _No response_",2024-11-27T09:04:47+00:00,2024-11-28T02:31:45+00:00,0,https://github.com/apache/rocketmq/issues/8991,8992.0,2024-11-28T02:31:23+00:00,https://github.com/apache/rocketmq/pull/8992,0,1,0,1,0,1,0,1,17.44333333333333,,False,True,normal,database,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/factory/MQClientInstance.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/pulsar,22393,"[fix][client] Fix client side memory leak when call MessageImpl.create and fix imprecise client-side metrics: pendingMessagesUpDownCounter, pendingBytesUpDownCounter, latencyHistogram","### Motivation  **Issue 1:** logs is below<sup>[1]</sup> - create a Message manually. - call `ProducerBase.sendAsync(Message)` - release the message that was created manually after the send is completed.   - (Highlight)  the variable `Message.payload` is `null` after calling `Message.release`.   - Pulsar client will get an NPE at this line https://github.com/apache/pulsar/blob/master/pulsar-client/src/main/java/org/apache/pulsar/client/impl/ProducerImpl.java#L421, and leads a client-side memory leak, eventually get an OOM error.  **Issue 2:** - create a Message manually. - call `ProducerBase.sendAsync(msg1)` - call `ProducerBase.sendAsync(msg2)` - release the message that was created manually after the send is completed.   - (Highlight) `msg2.payload` was retained again when calling `interceptor.onSendAcknowledgement`, leading to this payload not being released. This retention is unnecessary. See https://github.com/apache/pulsar/blob/master/pulsar-client/src/main/java/org/apache/pulsar/client/impl/ProducerImpl.java#L430  **[1]**: ``` 2024-04-02T14:37:27,801 - WARN  - [pulsar-client-io-35-4:ProducerImpl] - [persistent://my-property/my-ns/tp-7c4d35f4-0c0b-4415-b565-e88722b4879a] [test-0-0] Got exception while completing the callback for msg 1: java.lang.NullPointerException: Cannot invoke ""io.netty.buffer.ByteBuf.release()"" because the return value of ""org.apache.pulsar.client.impl.MessageImpl.getDataBuffer()"" is null 	at org.apache.pulsar.client.impl.ProducerImpl$1.sendComplete(ProducerImpl.java:421) ~[classes/:?] 	at org.apache.pulsar.client.impl.ProducerImpl$OpSendMsg.sendComplete(ProducerImpl.java:1594) ~[classes/:?] 	at org.apache.pulsar.client.impl.ProducerImpl.ackReceived(ProducerImpl.java:1277) ~[classes/:?] 	at org.apache.pulsar.client.impl.ClientCnx.handleSendReceipt(ClientCnx.java:485) ~[classes/:?] 	at org.apache.pulsar.common.protocol.PulsarDecoder.channelRead(PulsarDecoder.java:236) ~[classes/:?] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346) ~[netty-codec-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318) ~[netty-codec-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.handler.flush.FlushConsolidationHandler.channelRead(FlushConsolidationHandler.java:152) ~[netty-handler-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.108.Final.jar:4.1.108.Final] 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.108.Final.jar:4.1.108.Final] 	at java.lang.Thread.run(Thread.java:833) ~[?:?] ```  ### The scope that the issue affects Regarding issue-2 that was described in Motivation, it will happen if batch sending is enabled(without calling the API `ProducerBase.sendAsync(Message)`).  But it will not cause an OOM because all the message payload created by the public API will be built this way:  - `ByteBuffer jdkBuffer = ByteBuffer.wrap( byte[] )` - `ByteBuf nettyByteBuf = Unpooled.wrappedBuffer(jdkBuffer)`  So there is no memory leak because the UnpooledByteBuf will not cause issues even if it is eventually not being released.  Note: If someone builds messages by `public static MessageImpl<T> create(ByteBuf payload)` and enables batch-send, they will encounter issue 2, but we did not provide an API to send a message typed `org.apache.pulsar.client.api.Message`, so no worry about this. (Highlight)It only affects the users who use `ProducerImpl` or `ProducerBase`  ### Modifications  - Fix the memory leak   - which is caused by issue 1.   - which is caused by issue 1. - Fix the inaccurate metrics:   - `pendingMessagesUpDownCounter`   - `pendingBytesUpDownCounter`   - `latencyHistogram`   ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: x",2024-04-02T07:07:33+00:00,2024-04-07T01:28:05+00:00,9,https://github.com/apache/pulsar/pull/22393,22393.0,2024-04-07T01:28:05+00:00,https://github.com/apache/pulsar/pull/22393,0,2,0,2,213,66,0,279,114.34222222222222,type/bug;doc-not-needed;cherry-picked/branch-2.10;cherry-picked/branch-2.11;ready-to-test;cherry-picked/branch-3.0;category/reliability;cherry-picked/branch-3.2;release/2.10.7;release/2.11.5;release/3.2.3;release/3.0.5,False,True,normal,database,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/api/SimpleProducerConsumerTest.java"", ""lines_added"": 144, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ProducerImpl.java"", ""lines_added"": 69, ""lines_deleted"": 66, ""file_type"": ""app_code""}]",client,False
pingcap/tidb,52198,incorrect query result using range partition,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> ``` SQL CREATE TABLE `tde5cf9ab` (   `col_30` json NOT NULL,   `col_31` json DEFAULT NULL,   `col_32` tinyblob NOT NULL,   `col_33` json DEFAULT NULL,   `col_34` varbinary(487) DEFAULT '=HIwIv',   `col_35` bigint(20) unsigned NOT NULL,   UNIQUE KEY `idx_19` (`col_35`,`col_34`(2),`col_32`(4)),   PRIMARY KEY (`col_35`,`col_32`(1)) /*T![clustered_index] CLUSTERED */ ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin PARTITION BY RANGE (`col_35`) (PARTITION `p0` VALUES LESS THAN (0),  PARTITION `p1` VALUES LESS THAN (14464107896361403404),  PARTITION `p2` VALUES LESS THAN (15474042590907512091),  PARTITION `p3` VALUES LESS THAN (15569789118590293107),  PARTITION `p4` VALUES LESS THAN (18241271968789361726)); INSERT INTO `tde5cf9ab` VALUES('[\\""Wz95PLRQliLnqOJGrZvtn6QcHdkxwyCI1fSiemUMuA2ghYHm6jBNEhEPIlIMMEMz\\"", \\""GnUTig6aDuaTQxMHstD9JOiyVsMhEkDjmtwaUEVgiLL0reTKVSLSKsU4M1arDHaq\\""]','[\\""1vfZndKPtTkAOKFPQTKwSGGuYffnjTp7SILcVFGxLTA92YXeus0BRsIcY3kOc9VO\\""]',x'2569517e3d4d704833644e5f2474466b','[0.4646976615861493, 0.9582927604035303, 0.8908512637173415, 0.8300351357548978, 0.6887815225915662]',x'7374674835743d7a31364e5f68',0),('[\\""u5JTCUFl754vdvEmaBkSHkyLVHWHxLGWWAY6c8KAi4KR0mp9Ab4Y3J85S3fEciCt\\"", \\""dfJAjmFMLb70UlMq4OfaT7yftdHf60GL1Abhnyk7bG0FrppVnnIhYku4t47esr9D\\"", \\""qpRAR3qH3auqpbckbhsFefPgPXP1EIdEwwJnCet0XgxIDKOhJRmfbNRkpEMoz2TG\\"", \\""DAoKZ1u7AKkE6TqEspAyiPHkAS5agfZQpwHc6dCMIpUIYZiEbbAvBvTALAhCNDFH\\"", \\""oRQ4Aa0qjcLjVNGGBlwapfcegwF5Ptx7gVXzjOuN0hKs9w1UcTY76Bh5VcGE1WA7\\""]','[\\""HBcaDJIRVoKChADMHZQdayCtI7OndTYG9Jof6ZS2Xil2xTDnnepfigmKxeQWtlhA\\"", \\""htuR2wKGpH4IzV8EpESYw5Dnof3Nwi82jHqlPgTLkK4h0C7CbbdEG3t0JDDKSUrL\\""]',x'5e6a72704534402d3d4279543436692573','[0.17975531139017437, 0.6062629203033233, 0.8599170333050206, 0.6715847819290429]',x'65534a2937365444777578',1567779204212634758),('[\\""bDfqRcm0l11U6i9eVYvh8MpjGbTN9Gca1XOZd9tFbjesUcz3mfozwfbuKhQTp8Yy\\"", \\""y4spfXHabhDgNuZcvpmkIRmifuMbis4ACwERwFlPd8zgbhDao1DRu7KCEOoiOnZy\\""]','[\\""mGnKFNVsbxkL9NtWUD7RgoHCrWKHteHRmWIXqT5wnAvw9vHGUO7jXQRrwHuFie46\\"", \\""me1oaqvBYRU3ynvnucfndpqg3hSHDMCKGxgngL2GkgS7hh6BYxuR24d0C7agbah0\\"", \\""hxBpysKZIpnT6OdF7rye0yU6z53PGgd6yZxgHBpUPAhh8LReQtJ4TPEmtfVvAymI\\""]',x'','[0.5007290180382064, 0.4657942932163316, 0.04236859852868916]',x'2455',2396287314775314828),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',2684824597501773396),('[\\""yxH9jpm5YK3yTORfHYpuwkh72ncXGWNGrzsWAdz0W9T7faOQeLW2tOsZiBjUNfO9\\"", \\""1nCJv3aZsIYdMCtDS7rBynyU85Nagj2wSf6YsfGFk4QQ6vgFPzdFYzS26adNhesZ\\"", \\""6GrzbuH5ZsdnPoFBq1UNhfyXITjcvPArfwrMrg98IOjewi1N1A08qre3xSrfxa5B\\"", \\""hHKUq5H6bmgwmlr9yg26ixNd5lv38vpzEaIcVHqBoB9DgOqpL5y2hrfj4nHkNV6w\\"", \\""EST1SKjQZPQJqo0zAPuvMpm6x8AgURwpReIQL5nxDKwKkRyr7xSIxe1iNwHwqryg\\""]','[\\""Wd2O0UF3sAwfarduwMGUApoV6mgXTl0XKLH21zT618l6FE50YbGp3VYIUdRMLInr\\"", \\""thuGkVfKFdH8ubBvLYVvnBlPwSxe4Uu1GO4gBP7wTgRlu1hXO0wLw4H2J1h0Acqu\\"", \\""f925VFkYcnCOZPjXUdUBuRNpbs63wRrqaH8SGMj4XWDdxKoppfq97D7St7sHtGpI\\""]',x'2641326c5f4b6f','[0.1655914412460885]',x'72533728462146365965',2771059760938128257),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',4158046210744577520),('[\\""0zibTVWeikmgdXgdqg9jISmp6oDhYj78JBubXPcSuIr6lhqSkNdte8NT1C1bvcFA\\"", \\""QYO5zzajsMXcxS60rlVLGC4iOwyZNxsUOTMA5Rpe3r8aN4ABV7fYR6MlJWzkwz6p\\"", \\""lVGdBE1xiv0h4wcJac5Fks6rVTQiX6uJgrAdTjqwYzEcdDIGZqNPBdXHLQat8SDs\\"", \\""0Z8vWwyKTsJXNWnXUKTGadWKU1He6hhpvO2n9JM2BpTlqa2vlqIWhYJnjwhCNgwd\\"", \\""PtALIb8AFWahBJf8KnRB3BZBUH16GiJUku4RUPL6wjwBDAN37w55kucP6Vck48CD\\""]','[\\""YJD1tDsMWnFIKCska9b9KAZ57hC9G2gbqQ7E4yNd3qIy4mRREbENNlRT7ztkyq3A\\""]',x'5770597250774e6534712372','[0.292880490089004, 0.7567811197142162, 0.4579668859908117]',x'302d214e5e424e67',5118930515042638219),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',7276349901697860502),('[\\""sGR526YdamJtUSOYwZEKdTxnNnmmLNBnhLk90dFPLHUocfzv0AQZPo9eDaAe8QAL\\"", \\""K700JUnSJ4lB4Vip9e6ItNuicTmj9kjcECbXNE2YfQC5UwU6FjkGKNXI0TwUCCZz\\""]','[\\""UneqxjxBAxUlfRbSwZdwFLkLzhwpRfgDi3cX0raT7MPFHLj2nxauh1svVZgmJZ4G\\"", \\""SI3TKHJsiidlzKSC3YVgU803QcYSlsU4Dhx7rvmkKny86Mf8heNMVerAQpUa46B7\\""]',x'','[0.795118973877717, 0.9035005878221954]',x'737476526b',7694033135187929276),('[\\""hxpXF1wcqpseUEEsaKmHHeGp2c9KWDz8njOkkIMtML46xANdQO9eqvLL0sIyWbcs\\"", \\""8eX1Bli9sDUbx14mSB0grkn76HIDt7GZKureM72XIoXOl7D6Bm3fC8CNpKF5toSr\\"", \\""moCSChSTzLz3FZ84ZMFwL0Ix9bOWjgE5VOqDCBLixh6FvErQ8taIjyd0P5Z2ehAn\\"", \\""Ft0ZmxX3NMQbIBtZAcq3xEg6yBR5wnv6iRvtxGZw1dkoOziP7zMnB4s94cqYvhIJ\\""]','[\\""kKNsILLfPSoWavnl2LNE6fkKllYqaeqTj4zIS2lTG2d5O0F0e5jHes3Ua8GgzUNz\\"", \\""eyOsTY1hmZgfg4y2maHbTMh4AdfDcGj7VrecfbagS1xJklJGV2NtVCtR9DsN69yj\\"", \\""BZN4SWVR3MobJQsB5PjE6Pv9Q0zeReEoEIAq1ICYOR2ROGW6wsh3R41mHcsf0RZZ\\""]',x'7773266823475072','[0.15744289393856822, 0.23338231412982055, 0.22757162912768308, 0.38605872273044883]',x'5568235877757158795678',8021484697390902738),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',8260625723957049473),('[\\""ZE6eF2xv1xkFuctk2AigidCUGkWKyI4SK0XW4895CtiMMPs8B2QPXL2dfEDqsJcU\\""]','[\\""5HcaARgbOJCStbIpffLbH61EWe5GD1sBwQ5aQNf2Lm2Rb4cFaaSt6hzVmgxBGpin\\"", \\""7JN1NmQhMweq5633H3mVdYcXT6mGrV5vAaMABKugUhp46W7t0dgG7N6WUymTw8UN\\"", \\""wVM5L0inrOAOWX2vIv9ODUZsVIy3eZxM6K5UCijEGhGZFcVEDiD9SownRzmxFYFr\\""]',x'68','[0.049974563594808376, 0.09937511263049388, 0.9712576304190343, 0.49960134052404304, 0.2555888361215715]',NULL,8619475224050794690),('[\\""U2ZqVY5zkdMro0ebGJ7QA2xxBszk9ENU2hZSOjGxLc8mcIQfqjVOplLCekLQPiKr\\"", \\""n7g0hle2ZUEaxf34rkpR74QoD5nsNeX4AS93XEkGzB12k2S1UDo2go02vCokCAVn\\"", \\""6oQEVfnkNVPTcicvyNatJE7fvFR2ygCZgar5cbPOPESfjKhAoyn0PEmE2knI3I7T\\"", \\""Bj2icvUn3SXt9vsvuFNW49zc5ZpTG9TtnKzZAPnK8mhmc0AmqQrkppF5ujvW2uEW\\"", \\""P6Rsf13o3Iv6qqSQWOOJKkQeg6hpp2RnFMGafL09yaaKh0Df8P1kLufe0iNRr8FR\\""]','[\\""TgnEstp7XwchGz82F4j73GztPybNbScQRUOpaF9RefyHGKZiC8Fps2ycOnpmbqCs\\""]',x'55715079686b36','[0.03648504120105657, 0.7663623552993745, 0.5252041401881107]',x'5e6c3d4d457468294e587e',9053556814446246053),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',9223372036854775808),('[\\""GF9ZtEYXT12NeZKhbbK2CsgHrcEeqIce2ZDZfSzFJ5ERE983EiCsqYqCccrL6WiA\\""]','[\\""p4UlXvXUJEUc9iD9hZVl1xuo5ss3z2Q1YdIIVBBv7s7tk1Z6Eg9H49beduHQHGtS\\"", \\""uPViYZfMAsif3QuoVGga1ysLPKB7lFfhxUPK98rmilpEDklg4gIkQshf8HlO28rf\\"", \\""9c21KhAY1illzi9oDnmUtF1BPjjjKHugj4HVlIghVsqseMjZs1JZfTY1S4mJe3Dy\\"", \\""lqZHsZx8UprThGfVn8s38dRkCRd8fpDP4PwoTw00VaklJp9Plo5OOn60rTD26jvM\\"", \\""Ze5tUXSn5u2GrmPIvKdsh4QrXVGYrC0zpL17i8Dm1Jl9uwu9pDVv3yiM9b4v4td0\\""]',x'485a247e4158463364366967445e294f695079','[0.11064612299736534, 0.8731056303073097, 0.8765364700727529]',x'30796e4564377825',9223372036854775808),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',9748215865553509859),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vrAsS6t9Mdbo4McCAGz8I1u5QyXualbNsr2mRJlvPZXKDYsMapsR7aWSgkhPHfyh\\"", \\""16HnAybO4mviImAjbp690tSnOUUVWsny7wijuiQoP0rqK3zNtThJKR8j9003EC8D\\"", \\""lizUlpEZ8vbNT5BSN3ZmMUtHjc7Dt2F1ijaaks2cT2iJDhGrKGrX4ZCIFMTUWWBJ\\"", \\""ZtJQDrKfGJaN71oSazvibAU2aLhdNntPgnUaCx1SgGRpX9BQOwDQP4FDJGYiMZcq\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',9760468490470693505),('[\\""ibLT8YJCusPeM36bwZ8flDqvrwncGj4730bGqbzqHjFWLnXRKsXvE9CdZ8gifFYm\\"", \\""k7VNMj80mJ4HRbpDcQizKqe1beWAzrsAoOcCauR3YggcQ3Sf8oVFUrpEy6EkXfLQ\\""]','[\\""dAMuAgDBy6xFWt9JX4X1gxXD4iIG5QClasUY5Ln6sWIVIs2R6AwWomqwAzL9UcWp\\"", \\""T2bRrlBfj1q6qDJsHrgEqAf8T1qH3IbMrNN6q0D9ZrWaHhltnInsZ1sGcKqBN5LG\\"", \\""2oikAXBwn3TTQWCebiIGL7pD8o5I21ayl7TGwjgmSpTOJKXJmixEZlj3FZr8aLQR\\"", \\""Wk8GYscXO8g02nWhPsXl2xjSFtm1hCIOgsrNRc5snDkgmnsoRDKlAgFS9GJh1Ded\\""]',x'4243384c54','[0.20617946807749404, 0.501261699058806, 0.511250618115637]',x'347478653d4f765157576f4c',11461170185469616451),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""YJ1zBBialzQYBXIWLg6dkwARF9YOEON1y3gE4V2uRBxuoIFy2aGWfJEGC1bta8uk\\"", \\""fJtAQmWYSwU9vM3aPkN4nl4VXRnA5PIfDSpMAdHLPoBwh3LmcBJ1sPgPug8XAd12\\"", \\""V8q0j3QUgoZ531HSOoHNUqeYbXN0MhqyN13llN0pUHNesRBC5VdPPqqBIfW76FyX\\"", \\""SB0LId8OSyOnvnq0DSpXSenUF0B8oGeOunkYFEl7EW2RGPYzfI8HFHOJDqkMbnZf\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',12344579753288860388),('[\\""Mi1Z6BKd9WHOMyXHLJjFUhkBJqh1nlrmYFTWOM7EfFgK5tld77QoG4V3dLKXYrbU\\"", \\""0mG3OPJKeYRyNdKrmiYH71l2FH9aF0vv5IFOCMr3LACEmJeK313UwJ6qLKqkdu1J\\""]','[\\""pknA2GfUUorCx7OY9s8PzftK8wHUnJK3LwPEzCLbVevTk4XrIWW7jDGHeCe1JfJ6\\"", \\""LtLpHxI3jut0y6V9YDCSURHtiCmEVDZi78a5lHae0bywKo8lktwsXralsgliii4K\\"", \\""Xx1JHro7JUn7EHcXSY2GT5nTGqV5ku7sNPMXvKYr12mRoqnyD5Uc1zwfSS0e37Et\\""]',x'6c34416c456b6c','[0.5861391292409713, 0.35223440118823224]',x'6a2940535a4a234c232a4948346e6d582447',12471124777815378375),('[\\""pT0Z2py4vhU2Sn1fOO1YAuw7ENHquSAyPfK5eEr6oaTj5iwjLEbCnTKuhBeFkdxw\\"", \\""ns7Smz5MZonhHVveySZ6gtXSCxaPGAqcdtvlbf342Rkwxen1BdVHvfvsT1B0ORXr\\""]','[\\""IB6mxUgWNOb7AnryUcEHYZ0mz7VJoswK916zfgLJNhA9iC7MIoVoexpcVZ2NcXxM\\"", \\""umQrHSNLveU86SeVg9gK1ShIwK3goQ9GrPixVvlrs6uWk32XyA7BFwBKG8Ja6L1m\\"", \\""5j5VjJsH4zQMHzijTX9oKAfKZto0gIPoNS0mhx64qJVIUvm2s4yBLlAxfDqlbMsb\\""]',x'2d5f482467594569795726494d76787355','[0.16902177794671622, 0.5850705243340889, 0.3451077790787441]',x'313361',12566881072169831726),('[\\""ZlyKKYCLmTFbvbe6laelF7Vc4PXQkNqgZC0LSX0eyDwTVOIQdgakhu5XQ3vzTb1s\\"", \\""tBQu8qXDz8bSbJiwqy2MyYhW4nW1d5kaoLZro0N0AumU40cXe9GEMlz9fFXpPqjv\\"", \\""iqaJdEepzhI5p3jXebhL4Q6dMr6CwAOUVoYcMKei9qex9wQM9d2Kt4Uffb4YP2d0\\"", \\""XxqMpC8ErdxIeRz14Sl3vdxbcQ1ju90U8mjzoBHWAuVKp6bqOVpvMAIclJXrv094\\""]','[\\""vn61BMecMvuo2qRApVnfciSRJJewcBRsKpTIgd6FLNIUFiADlJkYxtvwsPk47D1Y\\"", \\""RRD3FI37yr4eBl6abi5vDWmdVoihIGtXkX7MQJ8lsp1Y1iBIkl6iM3Zj93lX29Bv\\""]',x'2a245e325a6336737261292a7745','[0.6055285446218888, 0.2335982396952773, 0.6773961776160401]',x'7753587843',12783910262735102564),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""QIJk1vdVwX4wZigmX1sUfMKk00frsqxze8kzHm13541YXH7frbxZDZ0qXzxVsoQe\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',12910485282430732282),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]',NULL,x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',14272632288117041381),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""vNJZV4fsg3Pex7J4a2ZjM9HYK8iVFM1hdjHuXSgFqXkBIsE0SKzlnU07SVE1P3bJ\\"", \\""eIoW9lRfjmaqcFwuwmgYw5od9I13JYyf6j7xPGfDkdgxcxNn8ZyylsrKMhIsZhqQ\\"", \\""wfAxKVMxlA2KU3jE5f95uFnn8Ak93bzOpf0BTtfzO7sZFAQMcIK9TC0ueMBaokQY\\"", \\""SY9bV2CSr4sri7fRfZqf3hOtHOt6sGrF8GwfCZVuxVOGSG2QNoQviPQuoiFXba0T\\"", \\""Ro2Yq9sXOyeh79HKqKa5U0eKDk2HQ2wk3t2nMrrlez8ppR4a1EuADl0NACkEx4aN\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',14679767536905291788),('[\\""60cLTQsk6aibsYBlEG7abm8NxcNYWpNf5RhEfFwOAAZy2HVi5rDiPlQ2CufCYM3W\\"", \\""ALjtWgUF3yS6lY6zTqyAo40IXoIhE5dDa6AkBQDKBkrXjijz0bSQs1GKusBpDJ0z\\"", \\""sjxgsbgRsvwrtB8U2TtK6sUn2XI2t4unckv6uO94iEAp0hrWORBzXCOiAknXZcMX\\"", \\""Hjcw5gAvD5SvV8TmHjEiBDWJNZjGanrVej1TuWwl3wQ7pzkT5IizgQZdESomeyln\\""]','[\\""Gppsen7flRuGCyZnTqOz24LWbKQXjG4Hrvcg6B0xoZOvYeqU7JudtreNYwO7NbJM\\""]',x'406532253549266348','[0.024834158705199283]',x'687857472932564d6225455079404936',15069582400360190992),('[\\""gA9hl4RmccU1Vv6U6WIshs2lknzZ3QUIS8U0To69oOMug4l0HqqbpcxfqHnQ4W7w\\"", \\""gKgDfyST3772fOsQzZQk8zvb3TuKPXBY5bUHjk27nSFVc1EZYnVuCOazB12ZO2O5\\"", \\""VOBHTWkKimH231z4bRXIIInNEhP5NwPdomtthOUZy7Wv44rKvOGAxQmXOsElhZ3Z\\""]','[\\""7QXliYgvP2aL7LikmZIN09BskWqa8uCdxn54YxXhv3ceHWKlLFiSxsxK1oyLU1kS\\"", \\""BaiigvPJFbCXVvqWV8HLk4fn5YfU01PmJ60jHVVrKqgy24oJaZ0QD5UhMW5Z4fxe\\""]',x'74286137','[0.45837587966902954, 0.8437973281029107, 0.9710229823136485, 0.552103424629388]',x'28336e4f6528502172',15286328840732975636),('[\\""OxeU4vGCNXJiZV1LYdkk9IdgHkIduJww0lW5fJ6PqeWLMyK4j3QGRG0r8jFIHUXm\\"", \\""5HlOGYjMWMKcpGh461GvP2eX2T5UNnbxGXu1lbysfEF6syzgx2KtNYUut3K0YLfn\\"", \\""ExBqAYaCfoi98ZdjKqytg9lqjlchIaIUacaPsuotSwFDzbORB0e1Chpvs3T7vHxa\\""]','[\\""YR7px824YytWCF0e8MDHrc8A5cnNm1EcZewhLzgtVojrZvNktQBCNg3VImgJtuRD\\"", \\""mRESMvC5YOYaz7ETUxeyX9PrY1mMjdOnSnBU3uHauqjIAs8xRB2weXLIpl566m4H\\"", \\""PbQJ1OZhiJEprQjbl7XFCMi5khDQSgg0aqJiAinve871YsYYDJKZ62ehIyuN24LZ\\"", \\""RwfQOSxLpZ8UdcRli1zEfMaZzN28mEtaxhRDgmF9HqSmrA2Cav2n5XGHNAWDm8as\\"", \\""i2pxIF4oAX1mP1C4carlTR0d09fXo8LtgbOUfXOmR4vI99zOnrezwJnhsz3IaV3v\\""]',x'79594b3d4f70565e52','[0.8123190937332948, 0.03258766664706177, 0.2473114399039884]',x'6f2131256f744c636e2b3159484e',17073126758539490309),('[\\""ivpvzbZ01Aqn2NdI2neW83CUmhlH1eWJ8eAHEUPEOwsqFNdmTAwnkDNwQ5kGHxji\\"", \\""4jpPcVr0q3TY36jO3gdr5wp7uC1HZ4AWRLm5I0FLWd6vdQo3edGhO1AHIyrlnOuv\\"", \\""dPbZ5rDle8Ko8laaLH4myRZesem8cHusJbvroETh46GIHsUaeyZn2FSLFimGkgFN\\"", \\""ngRMkumeYOE33W8PrWHObpf3XfDcOengd9v5o5rIvnCakw0qbkZdIzRooj073UNa\\""]','[\\""eJgs698yTVt6CDctE6thrHi2ZIItFRFGeOALaugRMrmS6hlHBDx86cNonUvd5vdQ\\""]',x'326a302a2d4145245a','[0.0659237005318939]',x'7a',17910151681881588230);  CREATE TABLE `te508771b` (   `col_87` varbinary(38) DEFAULT NULL,   `col_88` varchar(439) COLLATE utf8_general_ci NOT NULL,   `col_89` decimal(12,2) NOT NULL DEFAULT '15300',   `col_90` char(191) COLLATE utf8_general_ci DEFAULT NULL,   `col_91` varchar(43) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL DEFAULT 'jT(vJPvYy',   `col_92` double DEFAULT NULL,   `col_93` tinyint(3) unsigned NOT NULL,   `col_94` char(220) CHARACTER SET gbk COLLATE gbk_chinese_ci NOT NULL,   `col_95` varchar(365) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL,   `col_96` json DEFAULT NULL,   PRIMARY KEY (`col_93`) /*T![clustered_index] CLUSTERED */,   KEY `idx_41` (`col_90`,(cast(`col_96` as binary(64) array)),`col_87`),   KEY `idx_43` (`col_94`(4)) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_general_ci PARTITION BY RANGE (`col_93`) (PARTITION `p0` VALUES LESS THAN (6),  PARTITION `p1` VALUES LESS THAN (195),  PARTITION `p2` VALUES LESS THAN (MAXVALUE)); INSERT INTO `te508771b` VALUES(x'45397470352355457625','lN俌z荳柿',77.12,'','y#F8IoWhPlYw8nqGI',5717.836515431098,1,'譆5錼獄fy6禞鶏E臾%旨','3ovXFXxPFJ=&B22i%=Y','[\\""6s1YJEdbaleKr8LGVYkAVAK4OPh3v6mLDxkRaiSAP9S8uZ75x7rRG1JVdK68n8yn\\"", \\""pXO4J8gB6PSaJXwISvBPbSMf1VmnZcu2ml3Tgp0rMAV1WxA3ZhOk7JaHRy9FPRhz\\""]'),(x'45397470352355457625','_J鍭玼xHt&Zk免rw貾鉑#',45649.10,'','~ya$1t5TVk^1OHE2f',6838.942573493885,3,'詎~*%p6棥UsVP畡縼_盾','3ovXFXxPFJ=&B22i%=Y','[\\""XOSADpJLP5WAVoKW2Yaf0h3wFTEshh60NDm0cNpjuQeShaTzIKnthPJidOtkkf7z\\"", \\""dlMGj041ljldxN1wA735aqzPs4Ef9yKsFfDTBtTQADnwCKLLE0zp07fQQhI5leaQ\\""]'),(x'50266a3343362148492d676c773659','5v=+',87.70,'87w撨衺梸導0彪wO5螶o揷0g~p','~R',870.8013953946693,5,'0樥扊#挓bt溙泮A','',NULL),(x'45397470352355457625','k~J',1267.60,'UY=r28t嬞5&r','1XjUFVD_BgI',6838.942573493885,11,'rzUSS轐+蓨鷚)','3ovXFXxPFJ=&B22i%=Y','[\\""I4rOzvv3PGRXSni3hcB36T0dWXKhMMQE09ISrbsCtqXSPfGHl3te8ITxWpjMN9wx\\""]'),(x'45397470352355457625','Ml#4疚1(蓵)',29.35,'m樗鼷%5','euQDDgAD',6838.942573493885,16,'DLHh#t儞W+$非泣RY獓睉溨','3ovXFXxPFJ=&B22i%=Y','[\\""yLlgjrW3tdIENXJh0bfAuiLdWs73WsYLfW8TFdaGQPWxwMiWmB5ZyHaIpLtb8pLg\\"", \\""koELACEfWJiCEF5MgzoV6M6gqau0x0lJyprp4wAWQUXPA6xs4CGS1VIisKBciN1l\\"", \\""hFmIOaH1qtrQrdHBWMY2OKy4JpF2c80wqhcBgBvv7TpAnGHhnaHskiYR3cd5QyeC\\"", \\""9qEY7RtWlFlpkfu8VM1G0mfz0Lw7HaWwYsW7jifVMXUYZ5L48XOFdhCfjPzKLWan\\""]'),(x'45397470352355457625','m^畛9z淚緦C',2032.33,'s','lr+YImI9',9809.290737797808,18,'~0rq縊F1jW','3ovXFXxPFJ=&B22i%=Y','[\\""pYkVduzpnODiRiYAwvsTyShia6dBtJZeomFfJqUEhD7ScJBtPdAfNIP1e93PyqM3\\"", \\""AH3aqctdGEcNjXfvZNKboHZkpt6NJEiBIn5W20t18X5JxeY4BXsGaOsaQuzcme6f\\"", \\""UOqukg8gLvpqfyPYNguocevJO0V6h2OErrJIfAFtOoZCoDGZCcOi3WlhzGoaUcTO\\""]'),(x'45397470352355457625','Je互FVrT$',0.60,'虏讝梉*G旳NF鐱o櫴#槡C8','=',6838.942573493885,22,'Ai螺!胒G嗊wlH樽R&偱漗','3ovXFXxPFJ=&B22i%=Y','[\\""aqYAqToi9F4px8jVLCY0OVsdl5PxP82voRbk5v0yMoCGaWCMWM4VQi6hndxTPeVo\\""]'),(x'45397470352355457625','XKPV@NwE鬻D',1.50,'N8蔇Q畿q終俠_K^','v%C=7fdH',6838.942573493885,27,'p(祒W6f=w$估炳J)A睤YMl暩','3ovXFXxPFJ=&B22i%=Y','[\\""wK9DlJ9nBJJD5eZ2olIpqnPMOgomVjKLntJQ9V53zoRrSJTiC9CDgTH2kVwQP8V7\\"", \\""V7K21CvsBSQspkE4JWvuoh0M90nW8BLtw2LJnQtnd3MYKX7lG8DM84X7DPm99Uhf\\""]'),(x'45397470352355457625','V%*=g9咎',6368.90,'(产勜_j蠄#j虢o畦顼z嚄0','vQZ8MSE*fQhWEU-6P',2302.9774232879035,30,'oAK*W肚掓WX庲窿bQ枨t','3ovXFXxPFJ=&B22i%=Y','[\\""ONQrAX44Al0WXeWvif4Gb1VuHdiIPC0DQq75ZTkSzoPmenFhCbti81a3b44erkTo\\""]'),(x'45397470352355457625','e牄*',0.00,'鯣#p9-^萓n+','*',9809.290737797808,31,'U89鲡r煑','3ovXFXxPFJ=&B22i%=Y','[\\""oxHyyTajHLwjhdIrq5JHfmFkiQvg17p5dihzYofHQMZ2intpppaaNbRUGbjJV0Xg\\"", \\""7vZKrtVetyjimTUiQ92iFBklmCv0RE9iRtnojgaYbNQj3jRObnzM0he8bxM96lau\\"", \\""cmPigGpSrMoZfJiPL2Al4puBkm42RBe94WXeac3vT0I4w1oRSQGURtQmo3EDlsPi\\""]'),(x'45397470352355457625','鎖殪%岽譬鴝mfR7_P狅n',23126.42,'','ODd@s(1mBlMmb-',5045.973260921493,32,'sUn瓄S惚exh箭3*僢r3-k2褂','3ovXFXxPFJ=&B22i%=Y','[\\""eUk9sYI3mL94b6XhJ3w96deeoHGBUmOIXyAobOg1JlrJwhRv50vJVjEgP5HZmPXG\\"", \\""h5KU5FshWfu9JklCVvzXcxAFtmJUaUJKFUAAcC9rdA8bSgD9grOlYjkuGtRzQHpU\\"", \\""aYKEHqx7oAZl5F8DBRFouXbOSqIrpis6ecDU5i7dfIkiOFA6fRRhxzZAQfYjdFdr\\"", \\""cbNTXetH2TY4DT8OIekX0eTTL2pBxcbgTfJw2FRwpbc0345VX5T6Th4iSBvQUBnG\\""]'),(x'45397470352355457625','p-oBmWvG渰',9575.00,'o','SmOiIJ^vF',6838.942573493885,34,'噊曻b^pUU','3ovXFXxPFJ=&B22i%=Y','[\\""QFAVtLry4MTn9viyMSbxwVqVGsamkCBGCKN7K2Ud4uWYVK88GPjot9j30hR9fwNH\\""]'),(x'45397470352355457625','l1ArQP卋守',8.30,'(绮','Om*~6L!',6838.942573493885,38,'lp7业f34f诲C轲萾Pc櫍I3q','3ovXFXxPFJ=&B22i%=Y','[\\""5BcnE2sSTfCFWIRpQVMPIrMuxstKEys1acTdE9YSkKmS0eMcpjSh5oxXJAr72Rmh\\"", \\""G6tBe0NciJrunpMtAxPrjj9Sxd3YbdenJ6iCcdrrChg1z28pCyZx6cDm6mfHchEd\\"", \\""G9QwGrYXBWlrsCSYPqTp1ELRRTd6vBbN7Ua6aaRHF5G4r3YYtiT2IMxhtnDQLG9H\\"", \\""QMZRMr6jXViZJSdt89iZ8PC0XCLCc52kTS6j9NKu2vMbZq7oQgFo1QOlZA6ePxJ7\\"", \\""Z83CUjLj9ra0pAkGdEnLYtGAfPD05QYnyFwMdzGpaNGdBBDjt9m2vN4CIADAVKYv\\""]'),(x'45397470352355457625','Z媪罦逥*I',6475.50,'R29&JC飧笞峚1綹犭讪DF','',7738.4970491042195,47,'r','3ovXFXxPFJ=&B22i%=Y','[\\""s8SKVGD5euYIjh4Uvhrg2BnD1rSww421936zzQHaaevkc3yBduvr4rsYq6E5Onzw\\"", \\""DopryFpmWqEsycFSXsuYKVMxavNaedvc65sKPBhnbMxvfNqVl5bwobbm7l6dXDvV\\"", \\""8dVfcFuiEtZ3cOOYXV6ur6jGcvIaH0NOL8mjOkQETESXpdXq2I6UruDJcdyPfYpX\\"", \\""ZMdaWzqAyPuQM96xDt2B0eWJBDplGiSQwGAPUxi2dndVxfQExGOaE7E2pmnaDmCZ\\""]'),(x'45397470352355457625','S',68.69,'奃S','ppIFZ5ce~su',6838.942573493885,55,'','3ovXFXxPFJ=&B22i%=Y','[\\""gpnm2m8beo161NlYhG7jAg3WZKbUhH1KGAr2sLY9PHNkNLcyFyeacEVdOQjk7Xc1\\"", \\""rGXr81A0cuIlwxbePTaRvvUOUDwaLN5hPwUEs9GwIWx9AeTiDHtuK94dD8hrr9BE\\"", \\""xycB5jRja8I0Pv2vMb4iiC602lCrKgSlZcA6OHWXORtcRriCrzBUA5F5NgyF8iec\\""]'),(x'45397470352355457625','4$@D8xzkg炤L淖J*',39758.00,'吩哺觼$9PK淠昦馏VbW','IJV6^8jljN=Eq=',8675.40167320168,57,'','3ovXFXxPFJ=&B22i%=Y','[\\""8zOdxF4omN1gfpTepuQYoTvwwgI2VuqojbWyJPTT5XNGSuPXM1Zvci2YimF662PU\\"", \\""4XEC2Tao6fWyEgw8xouuqsHLdvj6TzqwRLah7UhLAMZ0j1mzRMm8dCaAKjpg0XH5\\"", \\""T55skVVwYsN6TQKrQS5yt3LOGZUZwNMJzWh7ubF7tKngDUyDVHUXsqQm3CUHizw1\\""]'),(x'45397470352355457625','$(2鉲%hl弮d姆浛g5LKD(',195881.00,'篋贇r7+','Z7CxU6qEG6o',6838.942573493885,59,'C','3ovXFXxPFJ=&B22i%=Y','[\\""uDGPxuVypMsI7NK1guNOGPh109PcjKOVDtJZ7hvpUpWWmW9w1otCujyy0LAzDe7a\\"", \\""4df69ooyhsGxzUnWE4ReGif98dGpcvAeZXFDVlpamkJGL9khuQ16TUxizfnkepea\\""]'),(x'45397470352355457625','l纴譄yj兾',0.90,'quk(脦aA摠掚S瓓獼蕼L3売瀑','^2X676KU*LSwGa',7466.250692887815,68,'DsY汴fb','3ovXFXxPFJ=&B22i%=Y','[\\""lhoFmYVPS4n33qEKZyWzog2veJWyh15MnZ1cQRmSdKkItZREEWow4ACydw9KYgsD\\""]'),(x'45397470352355457625','G-册壩s犑桠',916.10,'ygkk_颎埫1ME(稁&e','ypzm1ravj3',5541.517497688849,69,'J凣','3ovXFXxPFJ=&B22i%=Y','[\\""Rw5I0LaSBVTTgnhtwS82ZELNLscZUgz9dmzR5Mped9y4QeKiFumXu1DeghSMxMPa\\""]'),(x'45397470352355457625','覰軡~硂I3愿奏HaKm囔栴瘺',22508.60,'6蔏Ch磓hv蕡-AVg摯Wj羑_P','hA44_8Hcyo',8125.677813588512,72,'gB%椶','3ovXFXxPFJ=&B22i%=Y','[\\""G5DI9qoiAnYtZEGpRleIBk7pauzuABhL5YaLBD6GLEY5f8UTha5bKxjar0RQu7UT\\"", \\""MuXuKw4D8YwY3ijhP5wKx7VS4XNfSZuIP2XV45IZk1k2sHCR6Y9IwaB8qCPuII91\\"", \\""6XPRFAvAwOoDBQXWMHb31ShfExpHOIOI1ME6jcA4xIednmbZSwG5YI69CxWPc4Cq\\""]'),(x'45397470352355457625','9屉%95~栙蝲贱漋*3湩',7.98,'@和Q乒馼','hmtS!aTkxf%o',5661.849198851414,75,'4昫埧1陷hzdbb!','3ovXFXxPFJ=&B22i%=Y','[\\""h5tPSirQXdkZ3uD7Z0RlFEitSkr54nKiPQQxURDotNLywuBJtfiYYPC3LLsjoPIb\\"", \\""HFG39zLaggHTYTxx90RPrPTALqhZafOEyzG8vDiKP03C5nqSv98jTjWRa45ngpWf\\""]'),(x'45397470352355457625','&稕',2.00,'2Bf臱機','pfy)%J%3Umx_',6838.942573493885,77,'JxF嚥g4q9V蘷o','3ovXFXxPFJ=&B22i%=Y','[\\""oflb6LGlGpUDCEEQf96XtQAwnVRlmpdK3DgHyYM6lPeM43F608PE0gbAx1gq03ul\\"", \\""7tLOXzotmiv5pxvrKvpSuL5TcMmwJqmNgWQAnOOQOVEKheRW813mAr05T8Era73n\\""]'),(x'45397470352355457625','M抚c伦Gm姕ACa7閤輆璸姇',0.00,'説M褭f寂諗u','=dj3~wI#D&JNzbM',6838.942573493885,78,'重=b箠%B棏a5帔牀崌菈交Im*掆','3ovXFXxPFJ=&B22i%=Y','[\\""VTDOsVkHKPnMQzEsAgCisfaZH55oRY4ugyWAG61TlWyO6TC9huqiAvw6mWiRM2p0\\""]'),(x'45397470352355457625','Za7i撽4',885617.00,'CR僇)k5翰搬M~16嬦杠瀹','!Uh&6UC',6838.942573493885,79,'焬&V','3ovXFXxPFJ=&B22i%=Y','[\\""liVpa0ynYaHcmmjOXnorapnLZxJNX4aEDkFJBDNhs5eV6PYsdKY23MuSm94aFmPc\\""]'),(x'45397470352355457625','X0=Lt+nVF-dh媎鑨俴99j+',0.00,'j8V盐巊rp5檏','(Q-YV4K4pwj!FiMB',7620.968829130391,82,'!1j又~LecE笵)嘅','3ovXFXxPFJ=&B22i%=Y','[\\""sNgkZyJPhEGDfJBw05Ix7jeTGZpYFOLG3v8rNWwto5noztJ6hoKtSlOdcSjGDFUA\\"", \\""B7GavkpyUMUTONBwRrYbh6Ray4RmomBqxukwc9gQT3CrqKSZnhsBnSKbNuysNd4R\\"", \\""Rt0LrZ2vc9VwlrFMNNyLsBxofrZLVfFzRhVXawJDF61JW5cAjHIHsC9XKTwdazI5\\"", \\""Pd7u5Zuv1OhBBdE6jbKqQeZLWpfhNBgDgALNf81Sb8mOENYL7I6icw4iNmMlrheS\\"", \\""Peq9Unuf9OgvsBwUyYHHHPwDEGVKRhtWVLrV2FRI70fvIy3gGY9PghvKI9vqrdxc\\""]'),(x'45397470352355457625','^4g獫I驁hV謃Xc2J',4750.00,'W*z阐夈睏鄏','RK',6838.942573493885,84,'嵘','3ovXFXxPFJ=&B22i%=Y','[\\""nI63yJb1NLw5zDMkzQM1D9c6tZoQYGgz9r31mSeQ2pJbqPdmFuaPMeWrblVCFwm8\\"", \\""wSp99Sv3KvmiqSuRDC7xxt8EHnP2c7VsvaJybwlg1gZHZWbhbx0Cr0DPyLBGU3Mk\\"", \\""bFnvOvO8dZO6cACfAuEuU9WQC41XMK17NDLIE3Ao1bvMitKGuX7jFxCFKC1DvNi7\\"", \\""2X7kQx9mPWmXkwamj4rAOzjTkV4uLj8qPWs28D6090HufTrSrETUYM65uru0eHPm\\"", \\""RK8j15uQrxCqQMkuHIF5qfeUrBFHTPjJw6QPJLrICGZea4rkXC2xvyG3PxoRw1Cf\\""]'),(x'45397470352355457625','禑嵔X苞l跙l饓S=&ek0荲F糑t睔',95569.60,'躭畣b淅','*1_GjC+gN8=cDLD6',948.8021738248761,86,'幻~NWX餕0*','3ovXFXxPFJ=&B22i%=Y','[\\""hYSH3zO1dDgAwA7NVLrYQaF2vXNNz2SwTIXda3VWoqaZrDHfF0zGAufzqhSYSIBU\\"", \\""yGueFUDBHYW7Q5dk26ldF6l9XvEKVYuaCkfSmeYXcUNufblmXLUGZSEUVu3HoOGL\\"", \\""jtc9u247YkRUe5a35kB1Ic69Q6ceJKLb7QCHEUEaiicFSTKCl4bahNbBdbfJDuwI\\"", \\""efwGciiVbwwOZQpPDht8hGrrsDT7MaMQVpfn7jBsyZAuegYxXNN8zHGl0yMXzfQ3\\""]'),(x'45397470352355457625','玨vNv寘瞖yw50tB(3E蓻=',581091.40,'廂bA','EHVP2frT0)8Sbx0',0.009701410061036973,87,'嘿','3ovXFXxPFJ=&B22i%=Y','[\\""SshFt8Bu82vWv9apdOZiOngins2ATYutYs1C1SsnEqxmeXPfTDXMwNmJeiwR68fy\\"", \\""uH2IgtRa8nbH7GynNgGFsA8jhkAWQyNPRFOEXfgLM7dY20sd8FvhHlEbEwivlJU1\\"", \\""VzsdliQSoJshpEciBixXklDL70SAVHYJI2FQ5poAwt7stoMok7WQ1s5K3ABsdzlN\\"", \\""8akUJASlkmKsBhNoqldGORXBe7GKCOXM7L35uRtAssaocygcIE8pGl7XiDuulFoy\\"", \\""wQDUJrs8Kyn36K0Jrqe7xw3fNjCVXDMiVUIgTuFY7u8fj1A9vBOc3p9pqXoCHfIt\\""]'),(x'45397470352355457625','v韽cLH憼ZJ$鵼s~(jK鮴2p',0.00,'aL楢@篏a=mmHt1wBo玐2D%','bF)',5403.04428996166,89,'僰糙鬂uw-M(@C($p沐','3ovXFXxPFJ=&B22i%=Y','[\\""YmxQcPKZkZ9FYGN9iJ7wRUdckWGjvIPMvzAejg4eQYEg3u9dFcp86ulZDxMHHvRy\\"", \\""r4MU316lNorCc4TiW3DIHEQF58iBSmr9GQpw9QcGahVbv3aFCoEEEd35WlzhZgKT\\""]'),(x'45397470352355457625','8e錁吋5C羇u褈tm蹄#',0.00,'&*B','VzW*kLaU6q8zSc3ti%z',6838.942573493885,92,'#趺+n%悹s','3ovXFXxPFJ=&B22i%=Y','[\\""dfdkPjhG51Hh3Y5z1h5UbrKSuSMa3fEr8ilWJ9XQokIzmeAwrD8KRct8AcGTKmX5\\"", \\""uJdctUl0cXP9O1sgh9kln7DxpJLMU1tDHK1ldIL0ii8rEEgKjHDZHjhV1AroGTFs\\"", \\""3quuLmoCqZ0o2v2TeG7uaoPZrup8nznSvGsab74PNjhgt2mUBCL7rRVViRREXTHN\\"", \\""pBHmLJ6D6gu1S5do6lOd1RSdhHVXjesq4zYskwiX29x1SO33lKnQ0KPtGAIaWdPJ\\"", \\""dfEpYhqKtUJLT0qmPkqT5aZw0k4LMkDB06uKnej10x8KHYjZibL4ICcKoKJDeYmf\\""]'),(x'45397470352355457625','%T億Ld星啗',49352.00,'Qu槰Cw龿','z(A#Qjl&==XKZ^SeAX',6838.942573493885,93,'yAt諶L鬰谞襍v瞧$lC跟5広','3ovXFXxPFJ=&B22i%=Y','[\\""hSPG6hTLDPq2m6YR0hgzhYRwm6qKL1oC0NCR36cDZolDFLOMemwATpMV5XPA4Tp2\\"", \\""gttbhBy2hm3ypnxzuEMj73hCoo96B8Gyu24rDI6WMQ9TLWr4bvaIrTab8uh9DD6g\\"", \\""DEMZXaUmvXzQvDJO3k96DgpAjMkWrgD57RG4CkV7CKoCNUswdkXZlyBK8wDpyzej\\"", \\""nldnmjI5E73IYvhUNc8LAN9OL6nScaHbXTrnvj5LZHCWX24mCwqWuQ7ZyzWMkcDt\\"", \\""T7nqodq9l6wAD0aauJEvapOFLlQgZM1MjZOaQolDnBvYclPltxxnq4fSEt8qsCCN\\""]'),(x'45397470352355457625','嚸覊Lt軧f嗔',44.00,'s(s突袟椅仇quGaivm趯&Q旝f',')nE~3D4e~99-a',1761.3730283393754,95,'仡@m鼛x4Il蹌y馎Z譧','3ovXFXxPFJ=&B22i%=Y','[\\""T3QBZyRfbqXJBs5nC1M64bD3WM2OAlOzbI4E9lAVIzQpG7PV1oxGuz4bqUeRglNa\\"", \\""QZaQoZe7H1B86mzC3npee45X02CHC7hYsUfJE2UCA2FF75vSQ2mwgqX3IkBJF5GF\\"", \\""ccNCUPalnYMdARCmjFdRNQxKI6Ts4jhuVkF6CNqgqJx53Vi8ZvUDDUwitUF0uPl5\\"", \\""F56gkAuj093ZndETfbgru8JjILZ5UUMACW4KFBfA2AiiENBrUvcFGEd5s6nWXXX7\\"", \\""YQ37bYY58DL6UfCPWL29w6fzh3noO9S1yojjGruTUkGOhOG60SA9FVttI6o7Xa7W\\""]'),(x'45397470352355457625','鼚柵SP毃4e過p1儒沈2',18460.50,'儲剓)r糽v儔畿嘁趆Uo逼T','%IVZWNF)Z*c_p',9615.885171514492,97,'f疃甝嗞=鵸tYq$wr+3唳~趷龕','3ovXFXxPFJ=&B22i%=Y','[\\""Om8OMdifh3IcECDsDGTmpQ8kKb9vmXO8XNyGWqm0twY6K9qSmTSeE1y21mztW1SL\\"", \\""tkgoT7tBuicD34tFYtUu1gviMbfFa31l3jREtCmPX4DwI89QSu53DthERFHnrwhY\\"", \\""qD3k9oYGoF9xeJv8uC758BGjfWIev4W9Wivhole32gL6LjChdye2gj66JEjupfRE\\""]'),(x'45397470352355457625','歾堟F蚙',7795.95,'惵','&W',6838.942573493885,99,'fo潳AX橐敂襴6onU','3ovXFXxPFJ=&B22i%=Y','[\\""r88MDJfnYj1mD6yS6nLlq2xB9MfMGe638fOvgmFYiTJdugTGqXvWUuzUYS5ZBsPs\\"", \\""V25YhLtvFuzH2zGhhtD1yquPj5B3VBNBxYgM2vkWBb6z3qgEV3SVgAJXGqTm1dcC\\"", \\""z0Ljz12nyu1WKESW6oLo3uuVa9UThHN48023IdHkUZ0a8gxhlQCLCuVXYXLoJhbH\\"", \\""go5RwJT742IWQmICcdsDhI4jv0heK9MWJQErAiSwVDSB4DeI1We6jmygyw54JVl0\\"", \\""9Q5ckFjaA6x3XrbuRSLmqe3vAXQGOOUk8D9zPO7BBoxOh4oVygngpIC04CtSJWTh\\""]'),(x'45397470352355457625','9銞J罬u翨j礽斎q_D*^M^煟u',0.90,'%鐙v*崺rZ9_+v','gLOPaDI*',6838.942573493885,100,'徎1','3ovXFXxPFJ=&B22i%=Y','[\\""7Cs7UpMGSR44JkjomezSJj7PaVdZHY1qC4vdeDYa3lfIvvvFN32D8K4q4dyEM51h\\"", \\""t9lP9d4eMgYVETvLDoYlf0dWkirQBJMIKc4hvKJsO5rZ0iv5EnlhGJwREWWar81n\\""]'),(x'45397470352355457625','6軳&J-owKI',70.00,'AY*侮Xc譁^h','fx%1g',9809.290737797808,102,'w嬀E=笜4!=QgRk','3ovXFXxPFJ=&B22i%=Y','[\\""Vmoxm0SMaOU9E8aXQydhGR8zeHfnVgGzW5UT6pjxptLiXZXfgIhbIAVifTVcpeC7\\"", \\""7b7QALe71TMNwObuqE5eZh7RBlZYE7WwGtIkdtZcy06c55T3GLITrFekMTGyNS2j\\"", \\""rtIAEKgTDkfRFAqik5jTgC1YO7nZerSePKAq4fX3fdQqnxg3hWZUWzlmpI1PqIrv\\""]'),(x'45397470352355457625','@h0Nq钎_)mpl鈢糤嶨豝圗鶍',872335.54,'妖9pqQ6熞罺雝','!-$5w(ZlMOi1$5',4815.720673709614,106,'詎1+Z%DSQHFk@倜49d藰','3ovXFXxPFJ=&B22i%=Y','[\\""RzoykClHvyBrna4EXQZsxBhnfD6Vr1lC3rn1kHNkezlxx4mkZ8US3zeNq6ID8xFQ\\""]'),(x'45397470352355457625','2钩T蚞f',59.26,'64e鮄o^辜锐C','R*FDs',4137.561222646485,109,'軴5=','3ovXFXxPFJ=&B22i%=Y','[\\""VRYZ1QazJZEzLpZg4o8oMqLv97H9GPXpdaRL81sOYTx7Tyo7s3S0pDNn84AO9Y9d\\"", \\""FA06if72KOK0qhxWgEEbAII5ARcE1YSk6L62TqS9A8BSicrNd8L0Tak8SpTHKY9A\\""]'),(x'45397470352355457625','_JED$欁)A',27.00,'碎1+舂fPV','FnWwXlhX!yM!d4Z0tDk',6838.942573493885,115,'涇幭Oa淮scS猋穻J溱mr9','3ovXFXxPFJ=&B22i%=Y','[\\""lwhVbAnzjSc1XX8JOlbFXXzGc2RxMsVnsM73ucpEgIfLdZ5z4aTboWKOslcmV2NP\\"", \\""M9WApEa7s4UCEfSZgrq0vecXHlju89tPFwxTDgrDZNm4YawtVVXXo1wyl8qW85cc\\"", \\""Nw0xnwgWssvv6wU6Jv6iBeuddmnNjYlgfzpnvO5Msh1AwtcEOhUxHSKj80eUa2H5\\"", \\""Dx8NTHMFrifLPJGeh6Y9oc78Ii6AGH6zvOW3va0uyZUfzhJrBT5uGHg4Ne6DgCrq\\"", \\""v2OVV3GYTPvjL7MZ4k4N09D6gTvuhc1NgnnTIn4cYHVBCeMsiFuTEu57RCU1c6He\\""]'),(x'45397470352355457625','xx2裡~禞霯s&~t27杅1',27662.80,'楇9OaxReLU$ujQ靏Ut','A)f^Cdypn2bY',1306.7219291495294,116,'碿*O噪QqgK','3ovXFXxPFJ=&B22i%=Y','[\\""36Hi8SSqjMAxjy47YVZ4gnDGKl4P9iBEbPddneZlM6UfrRAtV3jyWwZO8mqFx6Og\\"", \\""tRrshiZHmOHw6BEsXaXktVAG4BOPrniuDUeZFmAPDZQ78gLXz64Zdyevxwm1NWYt\\"", \\""pFtIFmr0ZAK9KZArdxsQHPHhUYKp8oFWGLepuU5UOq7b1bkAx03NzdleNLRAuimd\\""]'),(x'45397470352355457625','M稬吅!F蔈@yQXb暥P',9.27,'+惟8Z卡gYg5pAQE脻栙P瑪','l++)VXMRdva48c&h#n',5475.8453400736225,118,'','3ovXFXxPFJ=&B22i%=Y','[\\""iBPe4Ja0Czq3sGruXXfSrBDtnKXZcXW52VIHlquHpzI8yT842EB71vLXMcx1W9I3\\"", \\""KrFHjuK1wgQKXqlLK1oFRKDv1eiC20L0xrJHUHdpAkyLQG8lX4SmVDwBVWnWrPrB\\"", \\""AX4jABN3JP2prZSma7pBeyia1dD13wyvR2mTHSzoKeJ6pbUhnOMfgxZlZNL0x38J\\""]'),(x'45397470352355457625','MHx阬蟇戠1蕀!借蘉劺)f$i',4.50,'*S*P嵖O坦AB','TG',6838.942573493885,119,'g鎙脂M稣拭戊弇nr','3ovXFXxPFJ=&B22i%=Y','[\\""YEpy0yAX4KXsYqzskW3pEjuD2jr0dVzwVaUcz9K4ivjBr5lM7PY1fo0bz6EFIru3\\"", \\""KqbfuODPgwghJXQpYKCqmV9U5Yj0So2IPwnb4eye9qhivTSxRvlTJT9Jl5CgjAdr\\"", \\""d1u9bxv0kyFDvtzY9FtEsw9e3bGMt53CY9NKDU3Sycw5O9m2vfIq3iuEREvc5NAh\\""]'),(x'45397470352355457625','(U豏囓',9.50,'唝d陕F糾J裪oPE9U釯Zc0M%','9PF-r++',1346.5630532459668,120,'Ib磘H襵_','3ovXFXxPFJ=&B22i%=Y','[\\""LorgT8e8vNXRB9BSTpftL79KBeV44xJjTsQIirbh4Zjuy2ZxMXLr1fFGvEyhUmis\\"", \\""MRSPbWsQi9ozThj1RjvPBiqYHU2hBikv5SopUn4ZCcJN40QYY3v7gtMYLVt94a5Z\\"", \\""kEejezYSHbq3mzMvOFGFqNVddNSGcUVZKs51MaffNDXwjOqHiQbQFrsbSVtq2t8z\\""]'),(x'45397470352355457625','!1X尊嶗6B5=',92753.60,'惊diiYl攼閁r3)','DEIn#2Ylwa',6710.289073423594,121,'l街輴','3ovXFXxPFJ=&B22i%=Y','[\\""Wcd13F5lEUXyHiegzXQ9PUx4FwMSvJdHfTrbg8jjmUEBIcN8XkobD5sZcMX0sB9q\\""]'),(x'45397470352355457625','dz#',198113.54,'zu%z','f&8%FU38!hE-Ew72',9809.290737797808,126,'','3ovXFXxPFJ=&B22i%=Y','[\\""42ClBgOmK2QKxPmoTOjwa7zEtNcbIiJUlz247Z4sHgdXGthbv7lAFb1Hupkf5l4R\\""]'),(x'45397470352355457625','yv笊蒄*亭(璻kG9@%所',950.72,'x(D碢Ve','FqJ@HMNwH2&8Y#(x',NULL,127,'c浐uXQI$槕','3ovXFXxPFJ=&B22i%=Y','[\\""rSM5rWdskYUeh5ZRoPbe2xeVCvWxnUQWGfOrwbS9gNdMcBxFyP15GxqVwTlVisks\\"", \\""Bwc2XtaPMITDfSznVVOVF5PfMSK88uNZZtLCU69NEY41pJZ9YMa0iPXczq5DO3iT\\""]'),(x'45397470352355457625','F蔼UpsTdz篤',4.40,'Ov嘛^xgug7h譆毆','8YG@jA0W(8-w',8637.31756655737,128,'Uq7讦QKW_捤t9嘼0缧mK3','3ovXFXxPFJ=&B22i%=Y','[\\""5ZJmQca4K44927MIN2xtFzOOJmcuMfTdUzXAOZ3FovRe4ISu7zcgEzkX8Y1dR9Va\\""]'),(x'45397470352355457625','g8fvg&',417353.00,'wK','JwPKyotoS0KkxL_H-d0',7421.157943854137,129,'g3満D=缔vf9','3ovXFXxPFJ=&B22i%=Y','[\\""xXcoCpIVvGmLiGrI5eUBHlk5J9x49OEKrWzsXJkTqyNVh4hGYCv7UQ0eKaCqWTCG\\"", \\""33N0Ld6a5OXc7rJwwmnFAICVaaAgemgZTBwfmzu8yYRF7OyzsHI2zfyHuXL7oPHE\\""]'),(x'45397470352355457625','^E6f缓(rY&N宱#',0.80,'ZM','~DU7KAnPqg',6838.942573493885,136,'鄫畕麛1!圑凞耔-','3ovXFXxPFJ=&B22i%=Y','[\\""W8gfd7ygfV5H0I1o1wGqnVHbtWUoxqd6RfZYA3nuwsVNJQTnHXHkNKHuAhA3n3jX\\"", \\""F6NvBLIWJy4qKZqs9TL0xCfM5mcYBiDhrMVoQvURI0JubkyAIwRxkGRRqqr0sLaK\\"", \\""yxINEzztqN0OuWZuYoweQELvq55FSPDSymbA9AIzL1dsuyhIREekd0AqLSePzG5l\\""]'),(x'45397470352355457625','齐Dm',66418.60,'嚽N9j%脳B甶fNc$B8廑%','ylLV#ij%boua!',6838.942573493885,138,'BuxrYI鉦z侰i','3ovXFXxPFJ=&B22i%=Y','[\\""k1aKSFRaUDrARSyygqrPXl97fB25LnwBjtQytS4ExP3TldngNOlVab8gwIUqo1y4\\"", \\""gFjhyQNbye1nr5ktLOaTKNcn5DiWgfBZRs0Q6W6FJuxpGOO3qsEMK01gsBeWSzLF\\""]'),(x'45397470352355457625','',9443.84,'U描琙龍衬蠰趤嗡t%l辞郁3I鱡9抗','^W^kdjaDNL3$SDw',6838.942573493885,140,'=+巚涯n5縅謓ZvY&膅歔-!激u','3ovXFXxPFJ=&B22i%=Y','[\\""QcVbfhJb7F1phewlPLAotu901thuAdou3obaMKv6IjUoodBlXQnG9Lcjp7dxWQP2\\"", \\""CzJvMP9E6E8c7dc4jIpeevm3a0T3aU2PQG2xY0Z1N2B8fuuW1GxWBqqvcAPTDnJi\\"", \\""6a5rv2qA7KrXZAyqjSTF6LaX2NTuiLNUZDW8MPVtHxjhuoCeM7VgOEdH6IZupHQf\\""]'),(x'45397470352355457625','咱穣H',855.50,'藏秪纆唪','4nlt*R71-8r86',5428.8271600513335,141,'','3ovXFXxPFJ=&B22i%=Y','[\\""pjORmixhdRqCNlK8UQhPMwJO9aBtAf0napqMdnE6DwFokD3MFr3UkI3LkgO6tycT\\""]'),(x'45397470352355457625','v#',72.00,'魿pS猯7簲燚瓇烇韂势','EgXq+kXJ_AkjQKGh3',6838.942573493885,142,'-謊','3ovXFXxPFJ=&B22i%=Y','[\\""C5MtAIUHsDokXhLsZDSOyM9XhH4UaTGCDE7tIo7j4VuOORSyDi6KubKxqZfEX0VP\\"", \\""7wzm69ZFx65c8yn06i14CGX8kywKOoNe0S6PPt0duW0CWMJpjqNbGFvzrgXe9UOn\\"", \\""8q60XSOXZZf8tQmC57z8BCubKPfxdOo3XXNQhSgTiKzjA8OD5yHKgVCm2K5TQg4W\\"", \\""Uz7b8cEFBYL4FcIPhnIpodeUap6aVloZd4hFkZICqQaRuMjhhIy4UZucOvWAmrqp\\""]'),(x'45397470352355457625','1I1騟+Si佡糜5',0.00,'衶鹯黺3d','sgS%s',6838.942573493885,144,'X+t篣#1IO恅^洘j','3ovXFXxPFJ=&B22i%=Y','[\\""VcjJgC0LEwI0qLbONgbLQzAtE3aFZplIuLUUoW5DY1tZthassV04kbxHvC9ybQ8V\\"", \\""yv78gPxX4c1MLL9N9h0tCnKss4UDFy505i7B2V75jLDUDhUwSvPzERWOAfCDxhws\\"", \\""Bqh50i5i9p4NTlRAj6lQ04RG736dR7rgcj5iDMpHyYF4TN5SljQZd3oaFkSyhNo3\\"", \\""u0Kf2vrOrUdFjVc6r4nUEtOkQvccPVItL7it5AoIQOrKoNVrqfqfZh5EC48oYERJ\\"", \\""RAyBEZBT3b4a2Hm5udINUgn58cKTrsoqcYkv6nw0J0JrubPF781QfpzfWIxAOmOs\\""]'),(x'45397470352355457625','箇鷷徔鱵Lz餍忉媦嫭8~=M媨jO',907347.02,'o溕4箬0悴eNip钚邤雖NM','@ZmDc-MV',12.364576604706953,145,'枾iwa','3ovXFXxPFJ=&B22i%=Y','[\\""BIlc8ne5At6HQSyHMJGqEb65QGgqX7wruOd0vxsMsIKqkLIoCk7Q3yXyz7WECe34\\"", \\""2sd8Z3tQpPJpeFScgqabLQpdh7RP7KFZqEVdnRIhREwKqKxuO48WArX3mQL2eG5J\\"", \\""hiEiaNVIuGjVlT1TzCnLAm47npGEM2DuuniBj2RNOnwW7htg0DxPnrBPBYcuDeKY\\"", \\""iihZh3Br7hXSz5aOlDMYAQKbIV0VqBvjSgSQAIiX5LfHGERidhAthbKFT9XPuNt3\\""]'),(x'45397470352355457625','5',642.00,'','WW%Vl',NULL,149,'做f歴U辯啀!#WI醚ZF瘼PK#g','3ovXFXxPFJ=&B22i%=Y','[\\""MDV9RX0WcKMckz1hrr8ecl3KurFsZcOAI8yZwjDz543ZIiwR0grJvsUg6pL8Npfw\\""]'),(x'45397470352355457625','钅ZW!%',0.00,'h掷K質Ub@緾b鱊TC=婦','r%~',2965.195503936985,150,'~','3ovXFXxPFJ=&B22i%=Y','[\\""lzZGrczoaDrDCYeedpgdqLrGUqyNCAbbkJnS80FIoNBqbDEQf9xYllnf6Y3rUsxx\\""]'),(x'45397470352355457625','藙k猷珳',191.50,'嚥F1kPh鴶枯藉哪$%RC','6#go5(',9809.290737797808,151,'棋','3ovXFXxPFJ=&B22i%=Y','[\\""IYrAxFewIwoNo4aCJoIrvYHEuFcfHWROuNtO0dKGBXfTf225LLTr2HmagzByQjYq\\""]'),(x'45397470352355457625','傤妓Y1Ia',0.00,'jxT2桇Vii','zxRpoVUXS',6838.942573493885,153,'L!(u','3ovXFXxPFJ=&B22i%=Y','[\\""VJsgiEuQF20bKVU4oYxcmsZkDQjBfW9qFpDBri3taPHNBF80FVD3HzWwweBSVVEp\\"", \\""XdVhiA03CadQsz1UfB1a5T57JCSE02u39TKNMZfdlGrcmc3cLVWDxFhC4XFGcHrO\\"", \\""Hnh2F9EPNkJins07AJKiccq9JhS1ej8eum7b9cm9DKjjoodLtlfMNZhLVNuKw07D\\"", \\""QLV4XIE0dya5lSmFXfjVuq6K3fpEWWYK078X52cgsXDlbEGqC79IbYDQlfSVDZb8\\""]'),(x'45397470352355457625','迂nH',45401.00,'','x!2742r',9809.290737797808,155,'潄T藺moS廑2+%+&yBV(','3ovXFXxPFJ=&B22i%=Y','[\\""kzkmZGQT2qXuki3ZniblDgT63Z6A91ZEgl7tbggYA4QpTiQ3pgEnLCo5TrhKVlVX\\"", \\""D08eKzwqYogyWqgQwcb9lIqYz8eM8yxI1mJdZNU2PjFyFUbacn1B9krG38Vlzh6b\\"", \\""78yG37Ftlyygnhju42e8g2u7gKCp4gciAfkJdfchSWUhENFzB1N35Aaswfx8R3Ao\\""]'),(x'45397470352355457625','凩怣z',7792.00,'idk致F蒰avZD燏f灧6瑮f恊','fhdxpT)$',9809.290737797808,156,'0_蜇襵诿苤覅#0k(躸','3ovXFXxPFJ=&B22i%=Y','[\\""2nXFwXMFuugSbm6Jm2FyC9HtUZlfu46u10dwH2LymeUeuKb7T25aqSjQuJmcMDLM\\""]'),(x'45397470352355457625','TY*綼JRG4垪撫FLqQ',144865.60,'笃B)pc閅輊(梵矫un','O3R9c7fjs%84qtDa',6838.942573493885,157,'fbug続T6秅M##*L溚d涂$膼','3ovXFXxPFJ=&B22i%=Y','[\\""nMKFVntjJu0WelRZkGfXKV4jWkpiqrh1rm8nab3OHZvMU8yG1cokqXmHiN1s0VJ1\\"", \\""aYKEUpFNfxxdA9djFXKsyfVnPrtPZkGHJZhUdQ0ASUXSzc0Sp5GGVL4U9yUpoz4C\\""]'),(x'45397470352355457625','嫾q)^kLU!i2g',9.20,'嵤vf菼5H悂K毝珢_I甂','zhqqp*e+)M',9809.290737797808,163,'梇苷#H8CiFR鹳7Z琞墈','3ovXFXxPFJ=&B22i%=Y','[\\""FZvNbcnQOLGEmRfafkP3ybA7WoVq9Q6PzJjk3AX8XkPyM0PN19pBOwm9L6HcSS8a\\"", \\""H4JQo4nFEUA1E5n1otQJ15JeEtF1I6Zg5sVy9O7SCzdv4YjXaSkOljQWTDIVCty3\\"", \\""Lruc8RZpT09H3k2eGbFxV97znhYYAFLraw7u1A6NcTayQfUXfWTximuMSHXX1zUd\\"", \\""4Z95GO98EvTMurR7fBy1XXT1iXZCjThThhE30GHomdMQyHiXB4MZOWfdITkRWqWW\\""]'),(x'45397470352355457625','M蜍篵p鱂怊+',3665.00,'Lk鏏Xs','MWT',6838.942573493885,164,'','3ovXFXxPFJ=&B22i%=Y','[\\""QWJUPgYAcPI07FrGDrazRpbdXDQjSxSisMieQEJmxAU7Pjwg93sy3GvYkGHd89Vs\\"", \\""W0chudnNM4QxKJ26HKHMGlGoLvgYAVYucBW8wBPiLn5WgdY7i6PzrzSgBSsZDYmW\\"", \\""IMk0ETBeDlITBC3K8fA24kMuvgP0osoZ9Md4p2Kzhtuh0ScbPEXfVrEjblZuW3Yy\\"", \\""bMPPeHXAuLy4catbYHXM09zlUKTtIDIYAxvGNbf1wsSuptJOO8RUSB3kPgnUsACV\\"", \\""Q2CbBXWjD1jYYhXmHyOaTMpDd95VBf5kQXAWdWy6E0Fyja8HQYg9MLq8AjR2HhKY\\""]'),(x'45397470352355457625','T抹勏(i闰鬘C窊',89.00,'G@仭1趍O鴭丘c颁i','vrDZ9',9064.889636879536,166,'~o総p&俟耈g萫鸷ZR蹅弝','3ovXFXxPFJ=&B22i%=Y','[\\""XFMM0lxqPkDprYrzOSjFPQDYeoSNc0eXx9FQDj04YLjoNzWdwByF14WCvGUWASP1\\""]'),(x'45397470352355457625','m簘+lb',451.91,'!沒踔_3swBuCA','ZHB$!cRC@2-~=t~n',9809.290737797808,168,'h47ba','3ovXFXxPFJ=&B22i%=Y','[\\""nf6bkkSHXJrFYqHKSmP2kb2A1p6VuzY0tTtwI4WVVyy2k4FrTsgAfLuVK9X9xND6\\""]'),(x'45397470352355457625','Mr暸T噷)Z9#^de',93.24,'蟦I嗴f衠萎','$6Q0^~%a',3845.5862302613177,171,'蚒t拸)D蚋橷fUB洖@','3ovXFXxPFJ=&B22i%=Y','[\\""BNJxKwI99Ev6xIgZOOKQ9wg31q01MjHZ2el11LDSqrKFAFVitW7TARA6SrP922Nn\\"", \\""c6cLEjDw0EIHr4BmQ2cMhQh2f26UZA4JQCBdg8tydnzySGlyHC39LjddDsdOEYzO\\"", \\""q5sFGNZ7sPOqc08AnEPDinnPb494qeH0vPjXA0bwQX8mLGQqgQuIL7OaxqvRKKVa\\"", \\""f9nHtDMPBCjYtLVznRINkNlV4exqpQvCGvU3MJQ57eKv9dZlZ6NdSh7oXz0pxQ80\\""]'),(x'45397470352355457625','6奐Ux2Q',0.00,'&$v@','',9784.697455914344,172,'!dD敆$o','3ovXFXxPFJ=&B22i%=Y','[\\""bA9RJAcSeSH15x4xa1PYhIoaBoRMcICEJfT0f1FAJYkQvgODz0nahGvhkEWSQz0P\\"", \\""20ZcfcoOh1pbIOdRGJslqe13BUO79hQDKPY81zpSpXv60oC4HWwbmQfczYeGThEo\\"", \\""LfTbhS0aYNLRW3PgZEFcJZVQ8qw4MGXiMdmNcgAMC4Sj4sKKAOGLBasOLU8d38O9\\"", \\""kacBdPQ5dfkGz7ubJbfnP1Pa1jclmGboQWvZkConN9UlrQhpeTWEYKCSFUBORvpO\\"", \\""wlAg0t0bVYhku7nl1oGxph90dCuC4YXOtWL8bs0nTrTS2zQDiaRiFRKA9oKPI2EF\\""]'),(x'45397470352355457625','A',181792.00,'辕肈I贸X抩$KF_B','i*I-+L6',359.4696623955375,180,'c蒥2D枂a摅冤(4fAAc俍','3ovXFXxPFJ=&B22i%=Y','[\\""sStDxKliSEwkSwFJGiiBXrxih6S4BrYBchuh6ZcvqtbT1jZGyIGYlVCPvjtfHQaJ\\"", \\""HbuIMBo6xWyAPxlrrMVh2SZXLz9FPX7vIRIJkE49tZH6ypLdbqMaPgUa9nK2YZA3\\"", \\""HDbFKuNOjICjQJj2cEcRcUxzigiRvFHHrwsR9F1S5MDr6FiMuUKcbJx87xefsRXo\\"", \\""uhvpmGFoOgrKvYcJb7oAWoqWk4OMzp7A7x9qBFoVe2k7ICyTQhwIWiEEYtlUy2YB\\"", \\""Ghn9GPbbbE9Xuju7lBQpbiiEnP0aB5Eqa2DyfIh94TwOVJKJyc7cspmwu7dVmxq4\\""]'),(x'45397470352355457625','',0.11,'9&+坓YK嫧K螪m','6U9Pgcio@o8v',9809.290737797808,183,'i~Kj*24jM$蛇','3ovXFXxPFJ=&B22i%=Y','[\\""UHz6jdVAg5upPWaYGx7lwJl1JBF1Y2xgxD35vDIPEKjJhvdvyn7ffWBvgQSwGp36\\"", \\""roAtpUyKvj4A2k0oBMfS5lKu9DNignpm9nlUylk0VCAykgKptfJ15agzJ6CMZRCZ\\"", \\""edGbzcysFuxGmumZkkeq8Dh5PhqAT9ECzYNObcJezcPHdyPAetMnftTAnTnm6D4y\\"", \\""NwWnDpELYoylH3qlcT3g6pvuJS4YOFgaffHcBP4YGxru4XdFgdHXzpUtZrDYis7A\\"", \\""pisAI8IfcorTCRDZcCoEjZhYg6yiLzZcpjvdjxe5XgzKyhqvdYfepIXsyE91UyY0\\""]'),(x'45397470352355457625','Ak',907.00,'O~燇綇f_=$0V狆','2!UM',6838.942573493885,186,'Z^==i睡g6','3ovXFXxPFJ=&B22i%=Y','[\\""XUjkWZlVS9s8xRHNO6UeqYM3roYQRyrtkaKu8tW1gJYxk5a60PFs63jFTgiMJvJv\\""]'),(x'45397470352355457625','+m4_+e',43.00,'彯-唁lAyX7绛m','!Esjn',9809.290737797808,187,'&嬒y武恖餋l旵豟$縜郗贑翜','3ovXFXxPFJ=&B22i%=Y','[\\""N5KBwMBKhwPPWBhbxrmVMdfi1YMapHzq53pRpZCDfFz3MTURkZcRygGT8C2ybeIC\\"", \\""vyDDiRyyyKUgphqiebJtdnDiIs00rnIDgeS8lyasjFCQXCZjnh6ZSfRD2ZflngiY\\"", \\""KBuQvFwH76XO21XYwYsAfm9tDWvvgDsD4oazG7PL4IWnM0RHEMlXyehf9PZ0Rswa\\"", \\""XJTa2Yx8GrOYvR51AqkeAdEksSiFI4Qt9RsXc0hvjHxiONbelZ37Eje8QFs32baU\\""]'),(x'45397470352355457625','5~',95170.70,'LZOWZ','1SX5PM#oH',6838.942573493885,189,'鮎^m1(甁O#瀊c8D','3ovXFXxPFJ=&B22i%=Y','[\\""QSVKKg3uJjimfaWkqdkUg48hva2xLNwf93WA3xotQJkkF69vh8zF5MThx03vtM8M\\"", \\""JzfPIjFVUzCXb3f7ObTohWJyYS52dOmq1jXiNvP23AEq7UMjBFq86Y1qYEgtk3Rv\\""]'),(x'45397470352355457625','萗劕oAy嚯攗4冫',8412.70,NULL,'FU_=6JO1LF6',6736.430201712505,195,'lpF謶=螇Q','3ovXFXxPFJ=&B22i%=Y','[\\""oWvgiG4gfgmMshiU06zuxr44ivuHDjpgsaxZatD5jJatv9PGRiIxh2msn3Mm1J8p\\""]'),(x'45397470352355457625','榏夏S^',35.80,'W!劂嵅~IL顋p胱2#+BUz^c','VXh8nw)DOp@xDfkk&',6838.942573493885,204,'_!呒y媭','3ovXFXxPFJ=&B22i%=Y','[\\""cLZRsyfhFSBJvXmnkL7Hi1GkGVxwai135774vtr4XBtQYF2D5zmcIBAaydpLS0mY\\"", \\""ecwmLi6TWgxd3kNqxY5u2d8YK011gVH7fC1aDXrGanWJwtAJ6mAZTdJcgPstVGkJ\\""]'),(x'45397470352355457625',')BOu淮Y1O',487165.90,'28Pgm^','BtJ)S0i%$S$(TzA!',9809.290737797808,206,'(Kuyw翘帯RzRW澈W厁','3ovXFXxPFJ=&B22i%=Y','[\\""Ar4ZjEZKS04qc8EHwAZE2TLPt0FD40JIgua0rSoLaABSh6C92OGWCzCR8USSvYfJ\\""]'),(x'45397470352355457625','铢J*aFri炑',7770.83,'鼧v藲fliT~fw','88mN-fjJ',4858.993433493079,207,'櫔禌敡','3ovXFXxPFJ=&B22i%=Y','[\\""qnN3jRQQwRXEL7mVWTojE8sTrbZeN178ObyF1kQg48zBSsI3SJ2JuVKKPg4yasac\\"", \\""fPyJpiaYnnbRIwRENMai03u9NdZ0x1WmIQAQrf1KAmToOzeFjh10hwB0f2ueOeCR\\"", \\""PdRXkbOHlBxovzJQklXTWyRxTanpOllk9vGw3acITHuihNTYwbaSJUvVekBYkYb8\\"", \\""mV6FRQ0KlKDg5dISsgeD8lhynQqsAIMBxHNfnDCB4czvRe9SPgYL22FiY2bTD7bU\\""]'),(x'45397470352355457625','B秷溳sf倚攛',86.24,'=v矯k断Sk靭耽bWYvk耬vUJX','68F',6838.942573493885,215,'躕=僎BREf(竁錱鱅^','3ovXFXxPFJ=&B22i%=Y','[\\""pZNewvCCcn7Ra51Q07EDxfCagqAI5br4umQwG7knLFEp3uoLCu6vDmbRoXWZiUor\\"", \\""WBVPeaIS50yyt3nfBNAt0oKRlcgdfTinztRvgXosWWKccBQ0ZOa2yZdKDvIc9Bqd\\"", \\""orXjQIsJi44eVgi9MbmBPa2Nt6qZi9snMpvSAVtBxeK2o5ARsJrhJYfixr1O1j7k\\""]'),(x'45397470352355457625','餭V1銼U2Z远l',86.00,'E=-_f愓BeI喆駲髼囓6%','UTfvyO!8sk',6838.942573493885,217,'禚16緒u','3ovXFXxPFJ=&B22i%=Y','[\\""WakIPBgVaSwQM96l6vwwLdKU2hAsYErIEooWV7Xg712IwPduJLxX9IsNubRhD2lk\\"", \\""X5HfkC6m4JpC08R5XQ5mLqzo01lP7mfQxhAD2BYFix6mq5n5UW9Vk0DSWubcCYAG\\"", \\""KfXtVXZCVR7Ppf45M68ya9AGvN5Urt70pT8iqdGRpouEcap8uB7YRkRF0MgpWyPe\\"", \\""4IhLJUssOYn3ln80rXL3dvkchGNYXXjDp950R4Ag4Z8k6iKPBxVh3MwCfNLJsJ9i\\""]'),(x'45397470352355457625','C緇诅旑K奴O潞%qv瀑搽63',50074.10,'@*kxR諉Zh','v%@nt6cKtftXz+-XL',6838.942573493885,218,')K詈豕2yh','3ovXFXxPFJ=&B22i%=Y','[\\""MAX9bInBN9uW2arzAlRjcrpxPTc09dvj52YqxuRSjHaoZN6bLZ7xzGQ0lDm7EYO5\\""]'),(x'45397470352355457625','nN晣濣u墬4$LzqiM',0.20,'*v3V','!wK4s)sCzOTn^h!C%tb',6838.942573493885,221,'罁~L怐zK涋avdqG溹缂_g#逗','3ovXFXxPFJ=&B22i%=Y','[\\""xDIGGlbXac3Cw4DjgGhPgxe1FpIFcKGki2AKhSsVGZXqnk9iwZrLkGmg1hnR3j9n\\""]'),(x'45397470352355457625','05蟧~抵etC',419175.00,'elw寓','j$8mWAUY9p#%Y0Ezfh#',8770.489412485314,223,'謜KiP妞庂照%d','3ovXFXxPFJ=&B22i%=Y','[\\""wQy1yDrsVOYPvHRPJbFX8LqGrSWjXgy0pz5ltDfFCnh2elhKhZAjrBcwwtfR9JGk\\""]'),(x'45397470352355457625','$蜛墣6k!Z湾4',98132.90,'鋵廪ELa呴歶oR^蜼','Q*8',6838.942573493885,224,'腬納','3ovXFXxPFJ=&B22i%=Y','[\\""hMKNHdquogBwYCGCtWQOGNGoYAHGXavPJv110UM5KwK9V9MdgphyKuZG2671l403\\"", \\""N7fJnJgLMrcCgr1a1n40DreGi3rfDIYbWJmMKfDghrWD3gL9pvGafUb32voZ3evU\\"", \\""5M9hdqJZOVYn54JQYrJHmRjDlA0m0wZ6GbLgeo7dKDBkHxsFYh2CTqquFyFyjMYp\\"", \\""JTiGh3OiNQWbhRPU1SMRg97flZwXnPFQ8gpvIatSHMx3FdjyWgNzTqTriVTHIK2d\\"", \\""k5fEIWHr7SZpNxQgwjSug4nzuyyZZxOpoBtn7sqwdP2K6Xf1UnzkaoiBtRxBfRB4\\""]'),(x'45397470352355457625','2处襤g',37.00,'騡(嵚4ii飹','983',6838.942573493885,226,'','3ovXFXxPFJ=&B22i%=Y','[\\""VcjJgC0LEwI0qLbONgbLQzAtE3aFZplIuLUUoW5DY1tZthassV04kbxHvC9ybQ8V\\"", \\""yv78gPxX4c1MLL9N9h0tCnKss4UDFy505i7B2V75jLDUDhUwSvPzERWOAfCDxhws\\"", \\""Bqh50i5i9p4NTlRAj6lQ04RG736dR7rgcj5iDMpHyYF4TN5SljQZd3oaFkSyhNo3\\"", \\""u0Kf2vrOrUdFjVc6r4nUEtOkQvccPVItL7it5AoIQOrKoNVrqfqfZh5EC48oYERJ\\"", \\""RAyBEZBT3b4a2Hm5udINUgn58cKTrsoqcYkv6nw0J0JrubPF781QfpzfWIxAOmOs\\""]'),(x'45397470352355457625','P',6048.00,'D鍴w7uD拧smV朖vXX','V1wuo-f&xl',6838.942573493885,231,'#ZOBYUa=瀒%=','3ovXFXxPFJ=&B22i%=Y','[\\""OuHiJD73Kmh8Vz039cc5LMl7Q6JACvpYX4f5B8Py4hrqHwiJpWICsa9CJRGapMwr\\"", \\""TJKC51JSAAvV329r2cdEiiu3jczu1Wx5tZqxtnZmCEpPwmIpDlk9RMIQW17nYTmA\\"", \\""tb9C6Q6ewLXDRrgvKU4P6FKtMD9HOXxNXYbKlNYxwzzXPDcaqIgTT7KvCxsW9tS5\\""]'),(x'45397470352355457625','V粭p韪UATd粓!轅',8.46,'曹RiTp%PRqdJ饺7&=钀','WxOa~pf',6838.942573493885,232,'w锢)SWu穘','3ovXFXxPFJ=&B22i%=Y','[\\""2yafOGkKlEyuBif1C5GjBA4nzJF0JN5zYvKPwIyhXY7DudPffcZvh9n3DTJIqxdw\\"", \\""poRgkwstQUlY2B2CQXHSinL6X2s0A4zsCDoED3NIE6C5uaWNJGb9rZtphzsmZKfq\\"", \\""jHfFHMGOuVpDJYmX0Cw5JavLXgEZpfQa2JuRZRz0fhByEntaioMKsD7HqwCdWNP9\\"", \\""4Ern6OSm4KYEmpIDTalF10fVChbjBsS1BT0D1RXhmBsvPighbYkjOzYRSW6j8ANz\\"", \\""Qf80HsNf8qHR8fqVaWYoLz7J1Dd1S8k2JOoEACpcetutsyXSqsB8WfjPr3T9vISz\\""]'),(x'45397470352355457625','$%Jv焒偶厩)疖K',147.00,'L甧娜jK闥1窴%v軥h','R@R=cp!X*',6838.942573493885,233,'hE輍','3ovXFXxPFJ=&B22i%=Y','[\\""3KPrmXh5oOPrqyQldE6s6OWdJj2XPoQ7qzVQAsYYyQgvd2ZXbYlInL2nalyZAuAW\\"", \\""ckCWMD2kd0WyiHWLj9JFHQeJbY0JojWDBcm0WJohbPFZe7nfUqgyPVJi3UTbRjCh\\""]'),(x'45397470352355457625','jg橿B@jNczK偃#',92570.00,'0L墊ib86堟J鈥p瓶8mT怚M9','ms',318.6727891836038,234,'es@讀彝e-K','3ovXFXxPFJ=&B22i%=Y','[\\""xFSxkXtRGZ7X8UlSce9bM3hujyBkfHtliF7h8YGWgqRayh7rJLy0bSVUFDPCbiQU\\""]'),(x'45397470352355457625','+QPF愦K',9.95,'澊C銴T0eTWC*W趜ps','cJRveZI$oQ#hNt4l',6838.942573493885,239,'nv-yI4vi跧h弅)禐C','3ovXFXxPFJ=&B22i%=Y','[\\""smMM9iqkNfyCdOWZSer0hXFH1Rd019z4F06bDI13YzCEk8kTeBz61w6uG3J46XFz\\"", \\""dl2cMgoj2VjjtEgwT8lHLSAgE6DAjRwdEzmlgK6ttPDbkrQ6Th932wcYB3Wj5QCd\\"", \\""BGr5BbBl7LHn25b8Bmpe276S9JiMqm9guMn6e0GzHYc5EVJ8B55ckraYnDtfGcfm\\"", \\""wMJAtJmg21BJH8xtf2vsiTITxn058VWPQYuMYH3W3QkvPLBCAQr0xpOEYLBQ5vOK\\"", \\""phXYFwYjfTykC028WHuhm5xqkZHdPXirHZ1R4FjNEGcRilOkA9O76VTzTkC1lkI9\\""]'),(x'45397470352355457625','n9=gaoY鈜Oo夗b蝩魚4_i)1',22067.90,'kd橝q鄪趰','NGBL3)',6838.942573493885,244,'緉#V仨)f慬nKWF喱','3ovXFXxPFJ=&B22i%=Y','[\\""eFj5Jhcquu6MZLdHSnqrppi4koOJ5aDra9vj08YpZJecBqzDBgP5FLftKdKlJWPK\\"", \\""iuxIMCtIBmYgwwzdYjOFYdZEeJ23pVElZuN0iNfyxu1OwYR26J39rUARoXOj8LpA\\""]'),(x'45397470352355457625','h8教E緆霧5*z*',0.17,'c觷p齡R橑eu','$jFShFfiM+d@fuK',6838.942573493885,246,'RqF媇odL蝁K-y麑','3ovXFXxPFJ=&B22i%=Y','[\\""VPd5N5nR0EIee1rzDYwTBa7BXPmGOV5160bm787tQgto0wPZuECNlNlE5xulp8Qa\\"", \\""ql5xaBmQb7oRQZjn29pF4G2kLga4BhjJmrdjnMSqblz8gdIqNB2k3VkDM7DXXSl7\\"", \\""rz85BQ7k1cw27C1b5IbgrWXe0xOuEwCkWTVRcWBuSYsRt065Ob6NOAWt9JndUq8C\\""]'),(x'45397470352355457625','my@扻棒z嚦1嬵QPv',8.00,'乱','N%fL!g5Zi=RLG=QD',9809.290737797808,254,'r3躝葯鷋h$轲眮b絋jZ喬KprN','3ovXFXxPFJ=&B22i%=Y','[\\""EuyhAB8DsueD06eQekP39pTyUFJabFTUWvsl1mkwrsrGG6uBAEAQPa5DAwvs6wKw\\"", \\""yQlyzNNyX2wtDBC4WKEZaJG11QQyO1InBNFjSOq5BgSprqvMx4FKSrebXX5W0cIv\\""]'),(x'45397470352355457625','p擡sKPP嬑鮒牸鶸餒u*&',99.00,'范','',1621.465891863473,255,'','3ovXFXxPFJ=&B22i%=Y','[\\""XhsyefxOM8zwmKctCleKwJZ1BVyE5glV0sShcxK64dYvffLzbj2NfbgzDHtdFp1X\\"", \\""IRqgPqs4aD0iVlHbgAfqqLSsyCLx69hOCegAklL7aQkUO8e5Qfl6xNJpd9kMiIZx\\"", \\""Q95kv0jnZBDXNQRceWEJX0AfYEQgNi6tgYYaiFmQZQySRCmj5Fr0xJysDSd1PZpV\\""]');  SELECT col_93 FROM `tde5cf9ab` JOIN `te508771b` ON `tde5cf9ab`.`col_35`=`te508771b`.`col_87` WHERE `te508771b`.`col_93`=55;  ```  ### 2. What did you expect to see? (Required)  One row in result  ### 3. What did you see instead (Required) ``` sql mysql> SELECT col_93 FROM `tde5cf9ab` JOIN `te508771b` ON `tde5cf9ab`.`col_35`=`te508771b`.`col_87` WHERE `te508771b`.`col_93`=55; Empty set (0.00 sec) ``` ### 4. What is your TiDB version? (Required) e4e08f0e80037e4c9c3e295aea359efbe5083cd6 <!-- Paste the output of SELECT tidb_version() -->  ",2024-03-28T08:26:06+00:00,2024-03-28T13:18:49+00:00,6,https://github.com/pingcap/tidb/issues/52198,54686.0,2024-07-22T08:22:34+00:00,https://github.com/pingcap/tidb/pull/54686,0,1,2,3,40,1,0,13,2783.9411111111112,type/bug;type/regression;sig/sql-infra;severity/critical;fuzz/randomtest;affects-8.0,False,True,normal,configuration,"[{""filename"": ""pkg/planner/core/find_best_task.go"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integrationtest/r/executor/partition/issues.result"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/executor/partition/issues.test"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
apache/rocketmq,8963,[Bug] CONSUMER_SEND_MSG_BACK request may be sent to ns,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  linux  ### RocketMQ version  all  ### JDK Version  jdk1.8  ### Describe the Bug  In some scenarios, for example, - In master-slave mode, after the master fails, the slave node will consume data. If consumption fails, it will be sent back to the master broker. At this time, there is no master in the routing information, and the result is null. The request will be sent to ns. - In master-slave mode, when the master fails and recovers, the producer gets the routing information before the consumer. At this time, the producer sends data to the master, and the consumer consumes data on the slave node. If consumption fails, it will be sent back to the master broker. At this time, there is no master in the routing information, and the result is null. The request will be sent to ns.  <img width=""1244"" alt=""image"" src=""https://github.com/user-attachments/assets/4a142c5a-3c05-4761-907a-23b117659378""> <img width=""881"" alt=""image"" src=""https://github.com/user-attachments/assets/2a800df7-8eed-4d34-b9b2-af4bc74bf2e1"">   ### Steps to Reproduce  In master-slave mode, simulated consumption fails and returns ConsumeConcurrentlyStatus.RECONSUME_LATER At this time, if you stop the master node, you will find that there is a request to send network ns.  ### What Did You Expect to See?  CONSUMER_SEND_MSG_BACK requests are only sent to brokers.  ### What Did You See Instead?  CONSUMER_SEND_MSG_BACK request is sent to ns.  ### Additional Context  _No response_",2024-11-20T12:48:20+00:00,2024-11-27T06:43:46+00:00,0,https://github.com/apache/rocketmq/issues/8963,8964.0,2024-11-27T06:43:45+00:00,https://github.com/apache/rocketmq/pull/8964,0,2,0,2,7,0,0,7,161.92361111111111,,False,True,normal,networking,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultMQPullConsumerImpl.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultMQPushConsumerImpl.java"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8935,[Bug] TimerMessageStore's getEnqueueBehindMillis use wrong unit,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  all  ### RocketMQ version  5.0+  ### JDK Version  _No response_  ### Describe the Bug  use wrong unit, named millis, but use second unit.  ### Steps to Reproduce  code review  ### What Did You Expect to See?  use millis  ### What Did You See Instead?  use second  ### Additional Context  _No response_",2024-11-15T08:43:45+00:00,2024-11-19T09:35:11+00:00,0,https://github.com/apache/rocketmq/issues/8935,8936.0,2024-11-19T09:35:10+00:00,https://github.com/apache/rocketmq/pull/8936,0,1,0,1,1,1,0,2,96.85694444444444,,False,True,normal,functional,"[{""filename"": ""store/src/main/java/org/apache/rocketmq/store/timer/TimerMessageStore.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7565,"[Bug] When executing tasks in DefaultLitePullConsumerImpl, it is not possible to cancel the discarded tasks, resulting in memory leaks.","### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  windows server  ### RocketMQ version  RocketMQ verison ： 4.9.3 and all  ### JDK Version  jdk： 1.8  ### Describe the Bug  When executing the pull task in DefaultLitePullConsumerImpl, it verifies whether the current task is paused. This verification is done before checking if the task should be discarded. As a result, the discarded task will never be cleaned up, causing the scheduled task to remain unreleased and leading to memory leaks.                  // After rebalancing, the assignedMessageQueue is deleted. This condition always returns true, causing an empty scheduled task to keep running indefinitely.                 if (assignedMessageQueue.isPaused(messageQueue)) {                     scheduledThreadPoolExecutor.schedule(this, PULL_TIME_DELAY_MILLS_WHEN_PAUSE, TimeUnit.MILLISECONDS);                     log.debug(""Message Queue: {} has been paused!"", messageQueue);                     return;                 }                  ProcessQueue processQueue = assignedMessageQueue.getProcessQueue(messageQueue);                  if (null == processQueue || processQueue.isDropped()) {                     log.info(""The message queue not be able to poll, because it's dropped. group={}, messageQueue={}"",                     defaultLitePullConsumer.getConsumerGroup(), this.messageQueue);                     return;                 }                                 // If the assignedMessageQueueState does not have any queued messages, the condition always evaluates to true.                 public boolean isPaused(MessageQueue messageQueue) {                     MessageQueueState messageQueueState = assignedMessageQueueState.get(messageQueue);                     if (messageQueueState != null) {                         return messageQueueState.isPaused();                     }                     return true;                 }        // After rebalancing, any message queues that are not assigned to oneself will be removed from the assignedMessageQueueState.       public void updateAssignedMessageQueue(String topic, Collection<MessageQueue> assigned) {           synchronized (this.assignedMessageQueueState) {               Iterator<Map.Entry<MessageQueue, MessageQueueState>> it = this.assignedMessageQueueState.entrySet().iterator();               while (it.hasNext()) {                   Map.Entry<MessageQueue, MessageQueueState> next = it.next();                   if (next.getKey().getTopic().equals(topic)) {                       if (!assigned.contains(next.getKey())) {                           next.getValue().getProcessQueue().setDropped(true);                           it.remove();                       }                   }               }               addAssignedMessageQueue(assigned);           }       }  ### Steps to Reproduce  1、In a running environment, there is only one consumer that concurrently consumes four queues. 2、When adding a new consumer, it triggers a rebalancing strategy, resulting in an average of two queues assigned to each consumer. 3、The two tasks allocated to the first consumer cannot clean up the scheduled tasks and are not canceled.  ### What Did You Expect to See?  If this is not intended behavior, there is indeed a bug that needs to be fixed.  ### What Did You See Instead?  Bug Report  ### Additional Context  _No response_",2023-11-16T06:35:32+00:00,2024-11-19T00:10:31+00:00,2,https://github.com/apache/rocketmq/issues/7565,7566.0,,https://github.com/apache/rocketmq/pull/7566,0,1,0,1,7,5,0,12,8849.583055555555,stale,False,True,normal,performance,"[{""filename"": ""client/src/main/java/org/apache/rocketmq/client/impl/consumer/DefaultLitePullConsumerImpl.java"", ""lines_added"": 7, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8925,[Bug] RocketMQ 5.3.x return success while update user password or status but nothing got changed.,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Ubuntu20.04  ### RocketMQ version  5.3.0/5.3.1  ### JDK Version  OpenJDK 1.8.0_342  ### Describe the Bug  RocketMQ 5.3.0/5.3.1 update user password or update user type or disable user all return success, but none get changed. Still the same as before. ![rocketmq 5 3 0 update user password bug](https://github.com/user-attachments/assets/95ac2ac9-68c3-44c2-9678-538f9680816e)   ### Steps to Reproduce  [[ broker.conf ]]  brokerClusterName = DefaultCluster brokerName = broker-a brokerId = 0 deleteWhen = 04 fileReservedTime = 48 brokerRole = ASYNC_MASTER flushDiskType = ASYNC_FLUSH namesrvAddr= 192.168.57.10:9876 listenPort = 10911 authenticationEnabled = true authenticationProvider = org.apache.rocketmq.auth.authentication.provider.DefaultAuthenticationProvider initAuthenticationUser = {""username"":""rocketmq"",""password"":""12345678""} innerClientAuthenticationCredentials = {""accessKey"":""rocketmq"",""secretKey"":""12345678""} authenticationMetadataProvider = org.apache.rocketmq.auth.authentication.provider.LocalAuthenticationMetadataProvider authorizationEnabled = true authorizationProvider = org.apache.rocketmq.auth.authorization.provider.DefaultAuthorizationProvider authorizationMetadataProvider = org.apache.rocketmq.auth.authorization.provider.LocalAuthorizationMetadataProvider  reproduce everytime when update user password and user status。  ### What Did You Expect to See?  Update user password and status successfully。  ### What Did You See Instead?  return success, but got nothing changed.  ### Additional Context  _No response_",2024-11-14T11:02:56+00:00,2024-11-15T02:35:38+00:00,3,https://github.com/apache/rocketmq/issues/8925,8926.0,2024-11-15T02:35:37+00:00,https://github.com/apache/rocketmq/pull/8926,0,1,0,1,1,1,0,2,15.544722222222225,,False,True,normal,database,"[{""filename"": ""broker/src/main/java/org/apache/rocketmq/broker/processor/AdminBrokerProcessor.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,56011,statistics: fix wrong behavior for primary key' non-lite init stats (#53298),"This is an automated cherry-pick of #53298  <!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: ref #37548  Problem Summary:  ### What changed and how does it work?   primary key's is_index is false. so it cannot load the topn. it's stats cannon be set as ```AllFull```.  ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [x] Manual test (add detailed scripts or steps below)  according to https://github.com/pingcap/tidb/issues/37548, it's result is right.  - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2024-09-11T04:14:36+00:00,2024-11-06T07:20:10+00:00,3,https://github.com/pingcap/tidb/pull/56011,56011.0,,https://github.com/pingcap/tidb/pull/56011,0,2,3,5,13,2,0,10,1347.092777777778,sig/planner;size/S;do-not-merge/cherry-pick-not-approved;release-note-none;type/cherry-pick-for-release-6.1,False,True,normal,database,"[{""filename"": ""statistics/handle/handle_test.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""statistics/histogram.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/graceshutdown/go.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""tests/graceshutdown/go.sum"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/readonlytest/go.sum"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
apache/rocketmq,7539,[Bug] IndexFile size will be created indefinitely in some condition,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  windows11，idea 2023.2.4  ### RocketMQ version  commit id：46962c262c37554ff09afe9e02c7baf66a5ecc73  ### JDK Version  jdk8，java-se-8u43-ri  ### Describe the Bug  when i running broker, and send message to broker,  the message have beed sent to broker successfully, but after a while i occasionally found my disk has been fill up (the index file occupy my 200G disk). i try to find what happen. then i've found follow exception in class : IndexService.getAndCreateLastIndexFile  when we send message to broker, broker will create index file, the steps for index file creation as follows: - try to find last index file  throught `indexFileList` - if `indexFileList` is empty then create a `IndexFile` - `IndexFile` constructor contain a `DefaultMappedFile` - `DefaultMappedFile` will create a `RandomAccessFile`   - follow statement are the root problem `this.fileChannel = new RandomAccessFile(this.file, ""rw"").getChannel();  this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize);` after RandomAccessFile has been created, then will channel map to allocate disk space and mmap to memory,  but the problem is allocate disk space and mmap is not atomiclly, so if disk space has been allocated successfully, but mmap failed, then a exception will be throw, this exception will cause a useless 400M IndexFile created in disk and can't be used, and there's something worse, every index log exception will cause a useless 400M IndexFile.  may be we can delete that created IndexFile after a exception if the IndexFile has been allocate disk space   ### Steps to Reproduce  create a IndexFile and allocate disk space successfully but mmap failed, like OutOfMemory ![image](https://github.com/apache/rocketmq/assets/77013030/465d23d5-8a6f-48d3-b201-d94903d8a73c) ![image](https://github.com/apache/rocketmq/assets/77013030/077d2f04-afc2-48e7-8778-6a872ec94594) ![image](https://github.com/apache/rocketmq/assets/77013030/844a60a3-1aa6-4bcd-8b5f-4d15f1add06a)       ### What Did You Expect to See?  delete the useless IndexFile if the file has been allocate disk space after a exception occur  ### What Did You See Instead?  many useless IndexFile occupied my 200G disk space  ### Additional Context  _No response_",2023-11-04T14:49:19+00:00,2024-11-10T00:10:46+00:00,2,https://github.com/apache/rocketmq/issues/7539,4759.0,2022-08-02T00:46:41+00:00,https://github.com/apache/rocketmq/pull/4759,0,1,0,1,1,0,0,1,-11030.04388888889,module/store;Improvement Required;stale,False,True,normal,security,"[{""filename"": ""controller/src/main/java/org/apache/rocketmq/controller/processor/ControllerRequestProcessor.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,7540,[Bug] Earlier messages in compaction topic cannot be consumed,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  centos / macos  ### RocketMQ version  5.1  ### JDK Version  jdk8  ### Describe the Bug  When using compaction topic to store some kv data, the earlier (but alive) messages in compaction topic may cannot be consumed by clients, resulting in data loss.  And the cause is that clients get wrong minOffset of queue when calling seekToBegin. <img width=""758"" alt=""image"" src=""https://github.com/apache/rocketmq/assets/103550934/b763f0ac-0bc5-4e16-ad47-abcdc552b8d6""> <img width=""980"" alt=""image"" src=""https://github.com/apache/rocketmq/assets/103550934/82da84c3-7edc-4d90-b677-1f3ddd5cd00d"">  For compaction topics, broker should query offsets from compactionStore. Because normal consume queue files could be deleted when the commitlog minimal physical offset moves forward.   ### Steps to Reproduce  1. create a compaction topic 2. send enough messages to this topic and trigger commitlog cleanup 3. start a pullconsumer to try to fetch all kv data ([ref](https://github.com/apache/rocketmq/blob/develop/docs/en/Example_Compaction_Topic.md#consume-message))  ### What Did You Expect to See?  All alive kv data in compaction topic can be consumed by client.   ### What Did You See Instead?  Some earlier data get lost.  ### Additional Context  _No response_",2023-11-06T11:30:41+00:00,2024-11-10T00:10:45+00:00,2,https://github.com/apache/rocketmq/issues/7540,4678.0,2022-08-17T02:25:42+00:00,https://github.com/apache/rocketmq/pull/4678,0,2,0,2,56,42,0,98,-10713.083055555557,stale,False,True,normal,database,"[{""filename"": ""store/src/main/java/org/apache/rocketmq/store/MappedFileQueue.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""store/src/main/java/org/apache/rocketmq/store/logfile/DefaultMappedFile.java"", ""lines_added"": 55, ""lines_deleted"": 41, ""file_type"": ""app_code""}]",store;logfile,True
apache/rocketmq,8808,[Bug] jdk21 Unrecognized VM option 'UseBiasedLocking' ,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  CentOS 7  ### RocketMQ version  5.3.1   ### JDK Version  OpenJDK 21  ### Describe the Bug  runbroker.sh  set   `JAVA_OPT=""${JAVA_OPT} -XX:-UseLargePages -XX:-UseBiasedLocking"" ` , but JDK 21 no support it, and start broker failed. after remove `-XX:-UseBiasedLocking`, start broker OK. or add `-XX:+IgnoreUnrecognizedVMOptions`, start broker OK.   ### Steps to Reproduce  1. set JDK 21 JAVA_HOME  2.  ./bin/mqbroker    3.  start failed:  Unrecognized VM option 'UseBiasedLocking'  4.  remove `-XX:-UseBiasedLocking` or add `-XX:+IgnoreUnrecognizedVMOptions` in bin/runmqbroker.sh or bin/runbroker.cmd 5.  ./bin/mqbroker     6.  start  OK  ### What Did You Expect to See?  start broker OK with JDK 21   ### What Did You See Instead?  Unrecognized VM option 'UseBiasedLocking'   ### Additional Context  https://stackoverflow.com/questions/74027869/unrecognized-vm-option-usebiasedlocking-error-could-not-create-the-java-virtu",2024-10-10T13:45:22+00:00,2024-11-06T11:30:19+00:00,0,https://github.com/apache/rocketmq/issues/8808,8809.0,2024-11-06T11:30:18+00:00,https://github.com/apache/rocketmq/pull/8809,0,0,2,2,3,3,0,0,645.7488888888889,,False,True,normal,functional,"[{""filename"": ""distribution/bin/runbroker.cmd"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""distribution/bin/runbroker.sh"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
apache/rocketmq,8868,[Bug] Request Code overflow,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Linux Fedora  ### RocketMQ version  Git commit id: ecb45bb90dba46bb35b51520b01890c9c47ba55c  ### JDK Version  _No response_  ### Describe the Bug   **Description:**   The following message request codes exceed the maximum value for a short integer (32767), resulting in an overflow. The original values are as follows:  ```java public static final int POP_MESSAGE = 200050; public static final int ACK_MESSAGE = 200051; public static final int BATCH_ACK_MESSAGE = 200151; public static final int PEEK_MESSAGE = 200052; public static final int CHANGE_MESSAGE_INVISIBLETIME = 200053; public static final int NOTIFICATION = 200054; public static final int POLLING_INFO = 200055; ```  **Actual Values After Overflow:**   Due to the overflow, the actual values of these constants are:  ```java public static final int POP_MESSAGE = 3442; public static final int ACK_MESSAGE = 3443; public static final int BATCH_ACK_MESSAGE = 3543; public static final int PEEK_MESSAGE = 3444; public static final int CHANGE_MESSAGE_INVISIBLETIME = 3445; public static final int NOTIFICATION = 3446; public static final int POLLING_INFO = 3447; ```  **Proposed Solution:**   To prevent overflow, we need to ensure that all message codes remain within the valid range of short integers. Consider using values less than 32767 or refactoring the code to use a larger data type (e.g., `int` or `long`) if necessary.   ### Steps to Reproduce  rocketMQProtocolDecode code: https://github.com/apache/rocketmq/blob/ecb45bb90dba46bb35b51520b01890c9c47ba55c/remoting/src/main/java/org/apache/rocketmq/remoting/protocol/RocketMQSerializable.java#L205C1-L209  ### What Did You Expect to See?  a short int  ### What Did You See Instead?    <img width=""373"" alt=""image"" src=""https://github.com/user-attachments/assets/010961c1-928d-46e2-b746-505165439d09"">   ### Additional Context  _No response_",2024-10-28T14:03:00+00:00,2024-10-29T11:44:21+00:00,0,https://github.com/apache/rocketmq/issues/8868,8871.0,2024-10-29T11:44:20+00:00,https://github.com/apache/rocketmq/pull/8871,0,2,0,2,44,7,0,51,21.68888888888889,,False,True,normal,database,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/protocol/RequestCode.java"", ""lines_added"": 7, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""remoting/src/test/java/org/apache/rocketmq/remoting/protocol/RocketMQSerializableTest.java"", ""lines_added"": 37, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8850,[Bug] NettyBridgeLogger method mismatch with underlaying methd,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Not runtime.  ### RocketMQ version  branch: develop  ### JDK Version  No response  ### Describe the Bug  The `NettyBridgeLogger#trace` method should call `Logger#trace`, but it call `Logger#info` method.  ### Steps to Reproduce  In `org.apache.rocketmq.remoting.netty.NettyLogger.NettyBridgeLogger` source file.  ### What Did You Expect to See?  `NettyBridgeLogger#trace` should use `Logger#trace` method.  ### What Did You See Instead?  `NettyBridgeLogger#trace` should use `Logger#info` method.  ### Additional Context  _No response_",2024-10-22T11:28:13+00:00,2024-10-23T01:57:36+00:00,0,https://github.com/apache/rocketmq/issues/8850,8851.0,2024-10-23T01:57:35+00:00,https://github.com/apache/rocketmq/pull/8851,0,1,0,1,12,12,0,24,14.489444444444445,,False,True,normal,functional,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/netty/NettyLogger.java"", ""lines_added"": 12, ""lines_deleted"": 12, ""file_type"": ""app_code""}]",,False
apache/pulsar,23713,[fix][client] Fix wrong start message id when it's a chunked message id,"### Motivation  When a reader's start message id is a chunked message id, this chunked message will be read even if `startMessageIdInclusive()` is not configured.  ### Modifications  Only use `getFirstChunkMessageId` as the start message id when `startMessageIdInclusive()` is configured. Improve the `testSeekChunkMessages` to verify it.  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: <!-- ENTER URL HERE -->  <!-- After opening this PR, the build in apache/pulsar will fail and instructions will be provided for opening a PR in the PR author's forked repository.  apache/pulsar pull requests should be first tested in your own fork since the  apache/pulsar CI based on GitHub Actions has constrained resources and quota. GitHub Actions provides separate quota for pull requests that are executed in  a forked repository.  The tests will be run in the forked repository until all PR review comments have been handled, the tests pass and the PR is approved by a reviewer. --> ",2024-12-11T11:13:59+00:00,2024-12-13T06:38:04+00:00,1,https://github.com/apache/pulsar/pull/23713,23713.0,2024-12-13T06:38:04+00:00,https://github.com/apache/pulsar/pull/23713,0,2,0,2,23,10,0,33,43.40138888888889,type/bug;area/client;doc-not-needed;ready-to-test;cherry-picked/branch-3.0;cherry-picked/branch-4.0;release/3.0.9;release/4.0.2,False,True,normal,configuration,"[{""filename"": ""pulsar-broker/src/test/java/org/apache/pulsar/client/impl/MessageChunkingTest.java"", ""lines_added"": 16, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client/src/main/java/org/apache/pulsar/client/impl/ConsumerImpl.java"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59852,objstore: retry on GCS EOF error  | tidb-test=release-8.1.2,"This is an automated cherry-pick of #59851  <!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59754  Problem Summary:  ### What changed and how does it work? see https://github.com/googleapis/google-cloud-go/issues/7090 and https://github.com/golang/go/issues/53472, it seems a bug of golang net/http. we workaround it ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-03-01T06:30:11+00:00,2025-03-01T07:18:36+00:00,4,https://github.com/pingcap/tidb/pull/59852,59852.0,2025-03-01T07:18:36+00:00,https://github.com/pingcap/tidb/pull/59852,0,2,0,2,16,0,0,16,0.8069444444444445,size/S;release-note-none;approved;lgtm,False,True,normal,database,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/gcs_test.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8820,[Bug] Variables mismatch annotation,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  Not runtime.  ### RocketMQ version  branch: develop  ### JDK Version  _No response_  ### Describe the Bug  The class `ResetOffsetRequestHeader` set wrong `@RocketMQResource` value in fields `topic`, `group`.  ``` java public class ResetOffsetRequestHeader extends TopicQueueRequestHeader {      @CFNotNull     @RocketMQResource(ResourceType.GROUP) // annotation value mismatch topic     private String topic;      @CFNotNull     @RocketMQResource(ResourceType.TOPIC) // annotation value mismatch group     private String group;      ... } ```  ### Steps to Reproduce  In `org.apache.rocketmq.remoting.protocol.header.ResetOffsetRequestHeader` source file.    ### What Did You Expect to See?  Use correct annotion value in variables.  ### What Did You See Instead?  Use correct annotion value in variables.  ### Additional Context  _No response_",2024-10-14T11:06:59+00:00,2024-10-18T01:18:42+00:00,0,https://github.com/apache/rocketmq/issues/8820,8821.0,2024-10-18T01:18:41+00:00,https://github.com/apache/rocketmq/pull/8821,0,1,0,1,2,2,0,4,86.195,,False,True,normal,functional,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/protocol/header/ResetOffsetRequestHeader.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59851,objstore: retry on GCS EOF error,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59754  Problem Summary:  ### What changed and how does it work? see https://github.com/googleapis/google-cloud-go/issues/7090 and https://github.com/golang/go/issues/53472, it seems a bug of golang net/http. we workaround it ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-03-01T04:49:44+00:00,2025-03-01T06:29:29+00:00,9,https://github.com/pingcap/tidb/pull/59851,59851.0,2025-03-01T06:29:29+00:00,https://github.com/pingcap/tidb/pull/59851,0,2,0,2,16,0,0,16,1.6625,size/S;release-note-none;approved;lgtm,False,True,normal,database,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/gcs_test.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59754,"should retry on  GCS error `Get \\""https://storage.googleapis.com/storage"": EOF`","## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> when generating subtasks using global sort on GCS, we meet a `Get \\""https://storage.googleapis.com/storage/v1/b/global-sort-datasets/o/internal-sort%2F30001%2F755%2Fdata%2F6d55efb2-0766-4699-a8f7-757c1578bf2b_stat%2F0?alt=json&prettyPrint=false&projection=full\\"": EOF`, in most cases EOF shouldn't be retried, but this case we need to  ### 2. What did you expect to see? (Required) retry ### 3. What did you see instead (Required) task fail ### 4. What is your TiDB version? (Required) master <!-- Paste the output of SELECT tidb_version() -->  ",2025-02-25T09:46:30+00:00,2025-03-01T06:29:30+00:00,0,https://github.com/pingcap/tidb/issues/59754,59852.0,2025-03-01T07:18:36+00:00,https://github.com/pingcap/tidb/pull/59852,0,2,0,2,16,0,0,16,93.535,type/bug;severity/major;component/ddl;affects-7.5;affects-8.1;affects-8.5,False,True,normal,configuration,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/gcs_test.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/rocketmq,8824,[Bug] IllegalStateException caused by logical errors,"### Before Creating the Bug Report  - [X] I found a bug, not just asking a question, which should be created in [GitHub Discussions](https://github.com/apache/rocketmq/discussions).  - [X] I have searched the [GitHub Issues](https://github.com/apache/rocketmq/issues) and [GitHub Discussions](https://github.com/apache/rocketmq/discussions)  of this repository and believe that this is not a duplicate.  - [X] I have confirmed that this bug belongs to the current repository, not other repositories of RocketMQ.   ### Runtime platform environment  mac  ### RocketMQ version  develop  ### JDK Version  jdk11  ### Describe the Bug  IllegalStateException caused by logical errors <img width=""1575"" alt=""image"" src=""https://github.com/user-attachments/assets/7316376d-ce97-468e-adc8-2ef40d361f8b""> <img width=""1053"" alt=""image"" src=""https://github.com/user-attachments/assets/8161e7e6-3956-4707-bdfc-3b6c552fe193"">   ### Steps to Reproduce  When executing the method org.apache.rocketmq.remoting.rpc.RpcClientImpl#handlePullMessage, the operationSucceed() inside does not return the correct result.  ### What Did You Expect to See?  Correct return without error.  ### What Did You See Instead?  Run error.  ### Additional Context  _No response_",2024-10-15T14:22:16+00:00,2024-10-16T03:00:52+00:00,0,https://github.com/apache/rocketmq/issues/8824,8825.0,2024-10-16T03:00:51+00:00,https://github.com/apache/rocketmq/pull/8825,0,1,0,1,1,0,0,1,12.643055555555556,,False,True,normal,functional,"[{""filename"": ""remoting/src/main/java/org/apache/rocketmq/remoting/rpc/RpcClientImpl.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59524,release the internal session which may meet error like #54022 ,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  like the analysis in #54022, the internal session may not be released after it meets some error.  the pull that intended to fix that issue only removed the error found in the issue. but the unrealised behavior is not changed.  <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  ",2025-02-13T14:10:02+00:00,2025-02-18T13:48:27+00:00,0,https://github.com/pingcap/tidb/issues/59524,59522.0,,https://github.com/pingcap/tidb/pull/59522,0,6,1,7,58,0,0,54,119.64027777777778,type/bug;sig/planner;severity/major;may-affects-5.4;may-affects-6.1;affects-7.5;affects-8.1;impact/leak;affects-8.5,False,True,normal,ui,"[{""filename"": ""pkg/domain/infosync/info.go"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/server/server.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/util/BUILD.bazel"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/statistics/handle/util/util.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/statistics/handle/util/util_test.go"", ""lines_added"": 17, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/testkit/mocksessionmanager.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/util/processinfo.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
apache/pulsar,9092,Function worker service is not done initializing,"**Describe the bug**  I have a pulsar cluster deployed in EKS. I was trying to run `functions` or `sinks` sub commands, but I got annoying errors.  The commands and errors are:  `functions list` ```shell export TOKEN=""xxxxx"" # token generate from Pulsar Admin UI bin/pulsar-admin --admin-url http://pulsar-broker.pulsar:8080 --auth-params ""token:${ADMIN_JWT_TOKEN}"" --auth-plugin ""org.apache.pulsar.client.impl.auth.AuthenticationToken"" functions list ```  `sinks list` ```shell export TOKEN=""xxxxx"" # token generate from Pulsar Admin UI bin/pulsar-admin --admin-url http://pulsar-broker.pulsar:8080 --auth-params ""token:${ADMIN_JWT_TOKEN}"" --auth-plugin ""org.apache.pulsar.client.impl.auth.AuthenticationToken"" sinks list ``` error: ``` Function worker service is not done initializing. Please try again in a little while.  Reason: HTTP 503 Service Unavailable ```   ",2020-12-30T08:00:36+00:00,2021-01-30T16:15:11+00:00,7,https://github.com/apache/pulsar/issues/9092,22511.0,2024-04-16T00:04:11+00:00,https://github.com/apache/pulsar/pull/22511,0,2,0,2,116,1,0,117,28864.05972222222,type/bug;area/function,False,True,normal,security,"[{""filename"": ""pulsar-io/kafka/src/main/java/org/apache/pulsar/io/kafka/KafkaAbstractSource.java"", ""lines_added"": 27, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pulsar-io/kafka/src/test/java/org/apache/pulsar/io/kafka/source/KafkaAbstractSourceTest.java"", ""lines_added"": 89, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59848,external storage: retry on a new http2 error (#59593),"This is an automated cherry-pick of #59593  <!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: ref #59849  Problem Summary:  ### What changed and how does it work?  ### Check List  Tests <!-- At least one of them must be included. -->  - [ ] Unit test - [ ] Integration test - [x] Manual test (add detailed scripts or steps below)  it's verified on GCS global sort test  - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-03-01T03:48:53+00:00,2025-03-01T04:31:15+00:00,4,https://github.com/pingcap/tidb/pull/59848,59848.0,2025-03-01T04:31:15+00:00,https://github.com/pingcap/tidb/pull/59848,0,1,0,1,1,0,0,1,0.7061111111111111,size/XS;release-note-none;approved;lgtm;type/cherry-pick-for-master,False,True,normal,database,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,4029,[WASI-NN] ggml: fix the warmup regression,,2025-02-21T15:54:33+00:00,2025-02-23T05:01:09+00:00,1,https://github.com/WasmEdge/WasmEdge/pull/4029,4029.0,2025-02-23T05:01:09+00:00,https://github.com/WasmEdge/WasmEdge/pull/4029,0,1,1,2,2,1,0,1,37.11,c-Plugin;c-Test;WASI-NN,False,True,normal,functional,"[{""filename"": ""plugins/wasi_nn/wasinn_ggml.cpp"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/plugins/wasi_nn/CMakeLists.txt"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
pingcap/tidb,59593,external storage: retry on a new http2 error | tidb-test=release-8.1.2,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #xxx  Problem Summary:  ### What changed and how does it work?  ### Check List  Tests <!-- At least one of them must be included. -->  - [ ] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-02-18T02:07:32+00:00,2025-02-18T09:19:40+00:00,14,https://github.com/pingcap/tidb/pull/59593,59593.0,2025-02-18T09:19:40+00:00,https://github.com/pingcap/tidb/pull/59593,0,1,0,1,1,0,0,1,7.202222222222222,size/XS;release-note-none;approved;lgtm,False,True,normal,database,"[{""filename"": ""br/pkg/storage/gcs.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,23688,[fix][cli] Fix set topic retention policy failed,"<!-- ### Contribution Checklist      - PR title format should be *[type][component] summary*. For details, see *[Guideline - Pulsar PR Naming Convention](https://pulsar.apache.org/contribute/develop-semantic-title/)*.     - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.      - Each pull request should address only one issue, not mix up code from multiple issues.      - Each commit in the pull request has a meaningful commit message    - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below. -->  <!-- Either this PR fixes an issue, -->  Fixes #xyz  <!-- or this PR is one task of an issue -->  Main Issue: #xyz  <!-- If the PR belongs to a PIP, please add the PIP link here -->  PIP: #xyz   <!-- Details of when a PIP is required and how the PIP process work, please see: https://github.com/apache/pulsar/blob/master/pip/README.md -->  ### Motivation When using this command `bin/pulsar-admin topics set-retention -s 20g -t 200d persistent://public/default/test_v1` to set a retention policy for a topic, it failed with following exception ``` PicocliException: Could not set value for field private java.lang.Integer org.apache.pulsar.admin.cli.CmdTopics$SetRetention.retentionTimeInSec to 17280000 while processing argument at or before arg[5] '200d' in [topics, set-retention, -s, 20g, -t, 200d, persistent://public/default/test_v1]: picocli.CommandLine$PicocliException: Could not set value for field private java.lang.Integer org.apache.pulsar.admin.cli.CmdTopics$SetRetention.retentionTimeInSec to 17280000 Usage: pulsar-admin topics set-retention [-hv] -s=<sizeLimit>        -t=<retentionTimeInSec> <topicName> Set the retention policy for a topic       <topicName>          persistent://tenant/namespace/topic   -h, --help               Show this help message and exit.   -s, --size=<sizeLimit>   Retention size limit with optional size unit suffix.                              For example, 4096, 10M, 16G, 3T.  The size unit                              suffix character can be k/K, m/M, g/G, or t/T.  If                              the size unit suffix is not specified, the default                              unit is bytes. 0 or less than 1MB means no                              retention and -1 means infinite size retention   -t, --time=<retentionTimeInSec>                            Retention time with optional time unit suffix. For                              example, 100m, 3h, 2d, 5w. If the time unit is not                              specified, the default unit is seconds. For                              example, -t 120 will set retention to 2 minutes. 0                              means no retention and -1 means infinite time                              retention.   -v, --version            Print version information and exit. ```  It is caused by cast Long to Integer failed https://github.com/apache/pulsar/blob/28e47fa99dcb080385cfa567b844e74ab0cd85b1/pulsar-client-tools/src/main/java/org/apache/pulsar/admin/cli/CmdTopics.java#L1854-L1855  https://github.com/apache/pulsar/blob/28e47fa99dcb080385cfa567b844e74ab0cd85b1/pulsar-cli-utils/src/main/java/org/apache/pulsar/cli/converters/picocli/TimeUnitToSecondsConverter.java#L28-L35  ### Modifications Change the type of `retentionTimeInSec` from `Integer` to `Long`  ### Verifying this change  - [ ] Make sure that the change passes the CI checks.  *(Please pick either of the following options)*  This change is a trivial rework / code cleanup without any test coverage.  *(or)*  This change is already covered by existing tests, such as *(please describe tests)*.  *(or)*  This change added tests and can be verified as follows:  *(example:)*   - *Added integration tests for end-to-end deployment with large payloads (10MB)*   - *Extended integration test for recovery after broker failure*  ### Does this pull request potentially affect one of the following parts:  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  *If the box was checked, please highlight the changes*  - [ ] Dependencies (add or upgrade a dependency) - [ ] The public API - [ ] The schema - [ ] The default values of configurations - [ ] The threading model - [ ] The binary protocol - [ ] The REST endpoints - [ ] The admin CLI options - [ ] The metrics - [ ] Anything that affects deployment  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: <!-- ENTER URL HERE -->  <!-- After opening this PR, the build in apache/pulsar will fail and instructions will be provided for opening a PR in the PR author's forked repository.  apache/pulsar pull requests should be first tested in your own fork since the  apache/pulsar CI based on GitHub Actions has constrained resources and quota. GitHub Actions provides separate quota for pull requests that are executed in  a forked repository.  The tests will be run in the forked repository until all PR review comments have been handled, the tests pass and the PR is approved by a reviewer. --> ",2024-12-06T18:15:48+00:00,2024-12-06T22:27:00+00:00,5,https://github.com/apache/pulsar/pull/23688,23688.0,2024-12-06T22:27:00+00:00,https://github.com/apache/pulsar/pull/23688,0,1,0,1,1,1,0,2,4.1866666666666665,type/bug;area/client;doc-not-needed;ready-to-test;cherry-picked/branch-3.3;cherry-picked/branch-4.0;release/3.3.4;release/4.0.2,False,True,normal,configuration,"[{""filename"": ""pulsar-client-tools/src/main/java/org/apache/pulsar/admin/cli/CmdTopics.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3999,bug: Failed to connect to github.com port 443 after 134439 ms: Couldn't connect to server,"### Summary   I can't connect to wasmEdge, it says something like this Failed to connect to github.com port 443 after 134439 ms: Couldn't connect to server  ### Current State  I can't connect to wasmEdge github to download wasmEdge  ![Image](https://github.com/user-attachments/assets/afeb97bc-e1b1-4731-acbc-b3b96db4f3a8)  ### Expected State  _No response_  ### Reproduction steps  1. Build with options '...' 2. Execute with flags '....' 3. Execute with inputs '....' 4. Get error   ### Screenshots  ![DESCRIPTION](LINK.png)  ![Image](https://github.com/user-attachments/assets/0e7422c8-d0cf-45ae-831a-db4eb310fe1e)  ### Any logs you want to share for showing the specific issue  curl: (28) Failed to connect to objects.githubusercontent.com port 443 after 300462 ms: Timeout was reached Warning: Problem : timeout. Will retry in 1 seconds. 3 retries left.   ### Components  CLI  ### WasmEdge Version or Commit you used  WasmEdge-0.14.1  ### Operating system information  ubuntu 24.4  ### Hardware Architecture  amd64  ### Compiler flags and options  _No response_",2025-01-27T06:49:12+00:00,2025-01-28T05:43:25+00:00,2,https://github.com/WasmEdge/WasmEdge/issues/3999,2089.0,2022-11-16T05:00:03+00:00,https://github.com/WasmEdge/WasmEdge/pull/2089,0,1,1,2,11,2,0,7,-19273.819166666668,awaiting-response,False,True,normal,networking,"[{""filename"": ""lib/validator/validator.cpp"", ""lines_added"": 5, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""test/spec/hostfunc.h"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
pingcap/tidb,53047,log backup: checkpoint cannot advance after pause->stop->start,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required) 1. start a log backup 2. pause it 3. stop it 4. start a new one <!-- a step by step guide for reproducing the bug. -->  ### 2. What did you expect to see? (Required) normal ### 3. What did you see instead (Required) checkpoint cannot advance, but status shows normal ```               name: longrun-pitr-test             status: ● NORMAL              start: 2024-05-06 02:20:40.574 +0000                end: 2090-11-18 14:07:45.624 +0000            storage: s3://qe-testing/kernel-testing/longrun/log-0506        speed(est.): 0.00 ops/s checkpoint[global]: 2024-05-06 02:20:40.574 +0000; gap=5h25m28s ```  ### 4. What is your TiDB version? (Required)  v8.1  ",2024-05-07T03:03:59+00:00,2024-05-08T15:35:39+00:00,2,https://github.com/pingcap/tidb/issues/53047,53091.0,2024-05-08T15:35:38+00:00,https://github.com/pingcap/tidb/pull/53091,0,3,1,4,49,3,0,50,36.5275,type/bug;severity/critical;component/br;affects-6.5;affects-7.1;affects-7.5;affects-8.1;report/customer,False,True,normal,ui,"[{""filename"": ""br/pkg/streamhelper/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""br/pkg/streamhelper/advancer.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/streamhelper/advancer_test.go"", ""lines_added"": 29, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/streamhelper/basic_lib_for_test.go"", ""lines_added"": 16, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,22150,[fix][cli] Fix the bug when set-retention specified size with -T," Fixes #22138   ### Motivation  When specified with -s with `-xT`, it will overflow when convert long to int. ![image](https://github.com/apache/pulsar/assets/6297296/806392eb-bb99-4433-82a9-aa02fff4d513) ![image](https://github.com/apache/pulsar/assets/6297296/f99eb7af-5d30-4abe-9322-f753dc2c79ce)    ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->   ",2024-02-28T10:57:38+00:00,2024-02-28T14:30:09+00:00,2,https://github.com/apache/pulsar/pull/22150,22150.0,2024-02-28T14:30:09+00:00,https://github.com/apache/pulsar/pull/22150,0,2,0,2,56,4,0,60,3.5419444444444443,doc-not-needed;ready-to-test;cherry-picked/branch-3.2;release/3.2.1,False,True,normal,ui,"[{""filename"": ""pulsar-client-tools/src/main/java/org/apache/pulsar/admin/cli/CmdNamespaces.java"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pulsar-client-tools/src/test/java/org/apache/pulsar/admin/cli/TestCmdNamespaces.java"", ""lines_added"": 52, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3996,bug: wasi_nn-ggml-cuda fails to download with HTTP Error 404: Not Found on Linux,"### Summary  This command used to work  for me on Gentoo Linux, but now I get the following error:  ```bash $ curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml Using Python: /usr/bin/python3  <stdin>:250: SyntaxWarning: invalid escape sequence '\\d' <stdin>:1129: SyntaxWarning: invalid escape sequence '\\+' INFO    - CUDA cannot be detected via nvcc ERROR   - Exception on process - rc= 1 output= b'' command= ['cat /etc/lsb-release 2>/dev/null | grep RELEASE'] ERROR   - Exception on process - rc= 1 output= b'' command= ['cat /etc/lsb_release 2>/dev/null | grep DESCRIPTION'] WARNING - Experimental Option Selected: plugins WARNING - plugins option may change later INFO    - Compatible with current configuration INFO    - Running Uninstaller which: no wasmedge in (/home/mamadou/.cargo/bin:/opt/android/ant/bin:/home/mamadou/android-studio/jre/bin:/opt/android/sdk/ndk/25.1.8937393:/opt/android/sdk/extras/android/support:/opt/android/sdk/build-tools:/opt/android/sdk/platform-tools:/opt/android/sdk/tools:/home/mamadou/.local/lib64/perl5/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/opt/bin:/usr/lib/llvm/19/bin:/usr/lib/llvm/18/bin:/home/mamadou/dev/go/bin) WARNING - SHELL variable not found. Using zsh as SHELL INFO    - shell configuration updated INFO    - Downloading WasmEdge |============================================================|100.00 %INFO    - Downloaded <stdin>:148: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior. INFO    - Installing WasmEdge INFO    - WasmEdge Successfully installed INFO    - Downloading Plugin: wasi_nn-ggml-cuda ERROR   - Download error from urllib: HTTP Error 404: Not Found ERROR   - URL: https://github.com/WasmEdge/WasmEdge/releases/download/0.14.1/WasmEdge-plugin-wasi_nn-ggml-cuda-0.14.1-manylinux2014_x86_64.tar.gz ```  It does not matter which version I install. Furthermore, checking the GitHub assets from the release page confirms that there's no such file for Linux. It only exists for Ubuntu, unlike other plugins.  Also, this is a blocker since it gives me the error:  ``` $ env LD_LIBRARY_PATH=/home/mamadou/.wasmedge/lib:$LD_LIBRARY_PATH /home/mamadou/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf llama-api-server.wasm -p deepseek-r1                                                                                                          unknown option: nn-preload ```  ### Current State  WasmEdge installation and usage is broken on Gentoo Linux.  ### Expected State  It should work as it used to.  ### Reproduction steps  1. Install with  curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml 2. Execute with flags ' env LD_LIBRARY_PATH=/home/mamadou/.wasmedge/lib:$LD_LIBRARY_PATH /home/mamadou/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf llama-api-server.wasm -p deepseek-r1' 3. Get error   ### Screenshots  _No response_  ### Any logs you want to share for showing the specific issue  Using Python: /usr/bin/python3  <stdin>:250: SyntaxWarning: invalid escape sequence '\\d' <stdin>:1129: SyntaxWarning: invalid escape sequence '\\+' INFO    - CUDA cannot be detected via nvcc ERROR   - Exception on process - rc= 1 output= b'' command= ['cat /etc/lsb-release 2>/dev/null | grep RELEASE'] ERROR   - Exception on process - rc= 1 output= b'' command= ['cat /etc/lsb_release 2>/dev/null | grep DESCRIPTION'] WARNING - Experimental Option Selected: plugins WARNING - plugins option may change later INFO    - Compatible with current configuration INFO    - Running Uninstaller which: no wasmedge in (/home/mamadou/.cargo/bin:/opt/android/ant/bin:/home/mamadou/android-studio/jre/bin:/opt/android/sdk/ndk/25.1.8937393:/opt/android/sdk/extras/android/support:/opt/android/sdk/build-tools:/opt/android/sdk/platform-tools:/opt/android/sdk/tools:/home/mamadou/.local/lib64/perl5/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/opt/bin:/usr/lib/llvm/19/bin:/usr/lib/llvm/18/bin:/home/mamadou/dev/go/bin) WARNING - SHELL variable not found. Using zsh as SHELL INFO    - shell configuration updated INFO    - Downloading WasmEdge |============================================================|100.00 %INFO    - Downloaded <stdin>:148: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior. INFO    - Installing WasmEdge INFO    - WasmEdge Successfully installed INFO    - Downloading Plugin: wasi_nn-ggml-cuda ERROR   - Download error from urllib: HTTP Error 404: Not Found ERROR   - URL: https://github.com/WasmEdge/WasmEdge/releases/download/0.14.1/WasmEdge-plugin-wasi_nn-ggml-cuda-0.14.1-manylinux2014_x86_64.tar.gz   ### Components  CLI  ### WasmEdge Version or Commit you used  v0.14.1  ### Operating system information  Gentoo Linux  ### Hardware Architecture  x86_64  ### Compiler flags and options  _No response_",2025-01-26T15:33:22+00:00,2025-01-26T16:46:01+00:00,4,https://github.com/WasmEdge/WasmEdge/issues/3996,2041.0,2022-12-07T12:14:28+00:00,https://github.com/WasmEdge/WasmEdge/pull/2041,0,4,3,7,784,0,0,742,-18747.315,bug;platform-linux;c-Installer,False,True,critical,configuration,"[{""filename"": ""include/loader/serialize.h"", ""lines_added"": 37, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lib/loader/CMakeLists.txt"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lib/loader/serialization/description.cpp"", ""lines_added"": 127, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/loader/serialization/serialize.cpp"", ""lines_added"": 244, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/loader/serialization/type.cpp"", ""lines_added"": 115, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/loader/CMakeLists.txt"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""test/loader/serializeSectionTest.cpp"", ""lines_added"": 256, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3481,bug: fails to raise an exception on a invalid test case,"### Summary  The test case is invalid.  The output of the command `wasm-validate --enable-all <test_case>` is  ``` <test_case>:00000b3: error: alignment must be equal to natural alignment (4) ```   ### Current State  No exception  ### Expected State  Raise an exception.  ### Reproduction steps   1. Build with command: cmake  -DCMAKE_INSTALL_PREFIX=""$install_dir"" -DCMAKE_PREFIX_PATH=""/usr/lib/llvm-12""  -DCMAKE_BUILD_TYPE=Release -DWASMEDGE_BUILD_AOT_RUNTIME=OFF     .. 2. Execute with command: wasmedge run --reactor <test_case> ""to_test"" [i32.eqz_7BBBBPBBPBBPBBB_615_17181389666367977.zip](https://github.com/user-attachments/files/15859272/i32.eqz_7BBBBPBBPBBPBBB_615_17181389666367977.zip)   ### Screenshots  ![DESCRIPTION](LINK.png)   ### Any logs you want to share for showing the specific issue  _No response_  ### Components  CLI  ### WasmEdge Version or Commit you used  813c200119dad7ebd5ea2ce462bc61549ecab115  ### Operating system information  Ubuntu 20.04  ### Hardware Architecture  x86_64  ### Compiler flags and options  _No response_",2024-06-16T17:53:57+00:00,2024-10-07T02:44:19+00:00,6,https://github.com/WasmEdge/WasmEdge/issues/3481,3987.0,2025-01-21T09:45:13+00:00,https://github.com/WasmEdge/WasmEdge/pull/3987,0,1,0,1,1,4,0,5,5247.854444444444,bug;help wanted;fuzz-different-behavior,False,True,normal,ui,"[{""filename"": ""lib/validator/formchecker.cpp"", ""lines_added"": 1, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59796,planner: fix the wrong transform in the join reorder (#59773),"This is an automated cherry-pick of #59773  <!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59643  Problem Summary:  ### What changed and how does it work?  after https://github.com/pingcap/tidb/pull/58521, when to convert the equal condition， We only consider the equality condition. but nulleq is considered too. but in the join reorder, We still believed that the equal condition could only be EQ, so we forcibly changed all functions to EQ, which led to the occurrence of errors. ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note planner: fix the wrong transform in the join reorder  修复 join reorder 时错误的表达式转换 ``` ",2025-02-27T01:33:03+00:00,2025-02-28T08:25:19+00:00,10,https://github.com/pingcap/tidb/pull/59796,59796.0,2025-02-28T08:25:19+00:00,https://github.com/pingcap/tidb/pull/59796,0,2,1,3,59,4,0,61,30.871111111111112,sig/planner;release-note;size/M;cherry-pick-approved;approved;lgtm;type/cherry-pick-for-release-8.5,False,True,normal,database,"[{""filename"": ""pkg/planner/core/issuetest/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/planner/core/issuetest/planner_issue_test.go"", ""lines_added"": 54, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/rule_join_reorder.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",rule_join_reorder.go,False
WasmEdge/WasmEdge,3939,bug: Incorrect Handling of throw Exception Constructs with JIT,"### Summary  WasmEdge crashes with a segmentation fault when attempting to execute a WebAssembly module containing a `throw` instruction.  ### Current State  Run the following WebAssembly module:   ```wasm (module   (tag $one_exception)   (func $throw_func     (throw $one_exception)   )    (export ""main"" (func $throw_func)) ) ``` [program.wasm.txt](https://github.com/user-attachments/files/18262028/program.wasm.txt)  Observed output: ```bash $ ./wasmedge run --enable-all --enable-jit ./program.wasm main [2024-12-19 18:14:32.755] [warning] GC proposal is enabled, this is experimental. [2024-12-19 18:14:32.755] [warning] component model is enabled, this is experimental. [2024-12-19 18:14:32.755] [info] compile start Segmentation fault (core dumped) ```   ### Expected State  The WebAssembly module should execute without causing a segmentation fault. The `throw` instruction should trigger the exception associated with the `$one_exception` tag, and the runtime should handle or propagate the exception gracefully.  ### Reproduction steps  1. Download [wasmedge-0.14.1-release](https://github.com/WasmEdge/WasmEdge/releases/download/0.14.1/WasmEdge-0.14.1-ubuntu20.04_x86_64.tar.gz). 2. Run the command: ```bash ./wasmedge run --enable-all --enable-jit  ./program.wasm main ```   ### Screenshots  ![DESCRIPTION](LINK.png)   ### Any logs you want to share for showing the specific issue  _No response_  ### Components  CLI  ### WasmEdge Version or Commit you used  0.14.1  ### Operating system information  Ubuntu 20.04  ### Hardware Architecture  amd64  ### Compiler flags and options  _No response_",2024-12-27T16:22:08+00:00,2025-01-21T06:16:37+00:00,1,https://github.com/WasmEdge/WasmEdge/issues/3939,3981.0,2025-01-21T06:16:36+00:00,https://github.com/WasmEdge/WasmEdge/pull/3981,0,8,2,10,67,2,0,65,589.9077777777778,bug;v-0.15.0,False,True,critical,ui,"[{""filename"": ""include/common/enum.inc"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""include/llvm/compiler.h"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lib/api/wasmedge.cpp"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""lib/driver/compilerTool.cpp"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/driver/fuzzTool.cpp"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/llvm/compiler.cpp"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/vm/vm.cpp"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/llvm/LLVMcoreTest.cpp"", ""lines_added"": 25, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/mixcall/mixcallTest.cpp"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/thread/ThreadTest.cpp"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",lib,False
pingcap/tidb,49839,Show create table should display pre_split_regions if possible,"## Enhancement  ``` mysql> CREATE TABLE `t` (   `a` bigint(20) NOT NULL /*T![auto_rand] AUTO_RANDOM(2) */ ,   `b` int(11) DEFAULT NULL,   PRIMARY KEY (`a`) /*T![clustered_index] CLUSTERED */ ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin /*T! PRE_SPLIT_REGIONS=2 */; Query OK, 0 rows affected, 1 warning (0.13 sec)  mysql> show create table t; +-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table                                                                                                                                                                                                                   | +-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | t     | CREATE TABLE `t` (   `a` bigint(20) NOT NULL /*T![auto_rand] AUTO_RANDOM(2) */,   `b` int(11) DEFAULT NULL,   PRIMARY KEY (`a`) /*T![clustered_index] CLUSTERED */ ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin | +-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) ```  `PRE_SPLIT_REGIONS` disappeared in result.",2023-12-27T08:08:17+00:00,2023-12-27T12:07:59+00:00,0,https://github.com/pingcap/tidb/issues/49839,59843.0,2025-02-28T08:20:53+00:00,https://github.com/pingcap/tidb/pull/59843,0,2,1,3,18,1,0,17,10296.21,type/bug;type/enhancement;severity/moderate;affects-7.5;affects-8.1,False,True,normal,database,"[{""filename"": ""pkg/executor/show.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/executor/test/seqtest/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/executor/test/seqtest/seq_executor_test.go"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3928,bug: Incorrect handling try/catch exception constructs with JIT,"### Summary  WasmEdge crashes with a segmentation fault when attempting to execute a WebAssembly module containing a try block with exception handling enabled.  ### Current State  Run the following WebAssembly module:   ```wasm (module   (tag $one_exception)      (func $try_func (result i32)     (try (result i32)        (do         i32.const 42       )       (catch 0         i32.const -1       )     )   )    (export ""main"" (func $try_func)) )  ``` [program.wasm.txt](https://github.com/user-attachments/files/18194940/program.wasm.txt)  Observed output: ```bash $ ./wasmedge run --enable-all --enable-jit  ./program.wasm main [2024-12-19 16:38:38.304] [warning] GC proposal is enabled, this is experimental. [2024-12-19 16:38:38.304] [warning] component model is enabled, this is experimental. [2024-12-19 16:38:38.306] [info] compile start Segmentation fault (core dumped) ```  ### Expected State  The `try` block should execute without causing a segmentation fault. The expected behavior is to return the `i32` constant value `42` from the `do` block of the `try`. If the `catch` block is triggered, it should handle the exception and return `-1`.   ### Reproduction steps  1. Download [wasmedge-0.14.1-release](https://github.com/WasmEdge/WasmEdge/releases/download/0.14.1/WasmEdge-0.14.1-ubuntu20.04_x86_64.tar.gz). 2. Run the command: ```bash ./wasmedge run --enable-all --enable-jit  ./program.wasm main ```   ### Screenshots  ![DESCRIPTION](LINK.png)   ### Any logs you want to share for showing the specific issue  _No response_  ### Components  CLI  ### WasmEdge Version or Commit you used  0.14.1  ### Operating system information  Ubuntu 20.04  ### Hardware Architecture  amd64  ### Compiler flags and options  _No response_",2024-12-19T08:46:38+00:00,2025-01-21T06:16:37+00:00,1,https://github.com/WasmEdge/WasmEdge/issues/3928,3981.0,2025-01-21T06:16:36+00:00,https://github.com/WasmEdge/WasmEdge/pull/3981,0,8,2,10,67,2,0,65,789.4994444444444,bug;v-0.15.0,False,True,critical,ui,"[{""filename"": ""include/common/enum.inc"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""include/llvm/compiler.h"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""lib/api/wasmedge.cpp"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""lib/driver/compilerTool.cpp"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/driver/fuzzTool.cpp"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/llvm/compiler.cpp"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lib/vm/vm.cpp"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/llvm/LLVMcoreTest.cpp"", ""lines_added"": 25, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/mixcall/mixcallTest.cpp"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/thread/ThreadTest.cpp"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",lib,False
pingcap/tidb,59534,exchange partition incorrectly report `Found a row that does not match the partition`,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. -->  ```sql CREATE TABLE di_tidb_wzp_test (a1 int(11) not null,a2 int(11) not null,a3 date default null, primary key (`a1`,`a2`)) partition by range columns(`a1`,`a2`)(partition `p10` values less than (10,10),partition `p20` values less than (20,20),partition `pmax` values less than (maxvalue,maxvalue)); insert into di_tidb_wzp_test values(5,10,null); insert into di_tidb_wzp_test values(10,4,null); CREATE TABLE di_tidb_wzp_test_np (a1 int(11) not null,a2 int(11) not null,a3 date default null, primary key (`a1`,`a2`)); insert into di_tidb_wzp_test_np values(10,4,null); insert into di_tidb_wzp_test_np values(4,10,null); alter table di_tidb_wzp_test exchange partition p10 with table di_tidb_wzp_test_np; ```  ### 2. What did you expect to see? (Required)  alter table succeeds.  ### 3. What did you see instead (Required)  ``` mysql> alter table di_tidb_wzp_test exchange partition p10 with table di_tidb_wzp_test_np; ERROR 1737 (HY000): Found a row that does not match the partition ```  ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  v8.5.0",2025-02-14T04:28:40+00:00,2025-02-28T04:27:39+00:00,2,https://github.com/pingcap/tidb/issues/59534,59837.0,2025-02-28T05:12:07+00:00,https://github.com/pingcap/tidb/pull/59837,0,3,1,4,241,11,0,251,336.7241666666667,type/bug;sig/sql-infra;severity/major;component/tablepartition;affects-6.5;affects-7.1;affects-7.5;affects-8.1;report/customer;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/ddl/partition.go"", ""lines_added"": 86, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/partition_test.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/tests/partition/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/ddl/tests/partition/exchange_partition_test.go"", ""lines_added"": 143, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59701,data inconsistency in table after kill pd leader during adding index,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required) 1、enable tidb_enable_dist_task and global sort 2、run sysbench 3、add index for one table 4、kill pd leader  ### 2. What did you expect to see? (Required) admin check success  ### 3. What did you see instead (Required) admin check failed data inconsistency in table: sbtest2, index: index_test_1740259129016, handle: 37480194, index-values:"""" != record-values:""handle: 37480194, values: [KindString 12789685643-64 577169320-18715872406-73821408681-48829052116-26062083313-11277804280-59971535651-34848965584-44255881774]"")  operatorLogs: [2025-02-23 05:18:49] ###### start adding index ALTER TABLE `sbtest2` ADD INDEX `index_test_1740259129016`(`c`) [2025-02-23 05:18:49] ###### wait for ddl job finish [2025-02-23 05:24:19] ###### ddl job finished select job_id, db_name, table_name, job_type, create_time, start_time, end_time, state, query from information_schema.ddl_jobs where query = 'ALTER TABLE `sbtest2` ADD INDEX `index_test_1740259129016`(`c`)' jobId: 425, job type: add index, state: synced, comments: ingest, DXF, cloud, max_node_count=3 add index done, it takes: 5m30.58627409s [2025-02-23 05:24:19] ###### start admin check admin check index sysbench_64_7000w.sbtest2 index_test_1740259129016  ### 4. What is your TiDB version? (Required) ./tidb-server -V  Release Version: v9.0.0-alpha-307-g6c0159e Edition: Community Git Commit Hash: 6c0159e2cf033e9862fb2b19b8556ae07bb59f4a Git Branch: HEAD UTC Build Time: 2025-02-21 10:53:12 GoVersion: go1.23.6 Race Enabled: false Check Table Before Drop: false Store: unistore 2025-02-23T05:11:27.172+0800  ",2025-02-24T02:45:57+00:00,2025-02-27T16:58:23+00:00,1,https://github.com/pingcap/tidb/issues/59701,59818.0,2025-02-27T23:30:28+00:00,https://github.com/pingcap/tidb/pull/59818,0,5,1,6,184,45,0,226,92.74194444444444,type/bug;severity/critical;component/ddl;affects-8.1;impact/inconsistency;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/ddl/backfilling.go"", ""lines_added"": 76, ""lines_deleted"": 43, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/backfilling_operators.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/backfilling_test.go"", ""lines_added"": 89, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/ddl/ingest/BUILD.bazel"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/ddl/ingest/integration_test.go"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/util/dbterror/ddl_terror.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59749,external: improve S3 error message (#59327),"This is an automated cherry-pick of #59327  <!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59326  Problem Summary:  ### What changed and how does it work?  simply use `%s` instead of `%v` when printf  ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note None ``` ",2025-02-25T08:22:23+00:00,2025-02-27T10:38:08+00:00,6,https://github.com/pingcap/tidb/pull/59749,59749.0,2025-02-27T10:38:08+00:00,https://github.com/pingcap/tidb/pull/59749,0,3,0,3,42,4,0,46,50.2625,size/M;cherry-pick-approved;release-note-none;approved;lgtm;type/cherry-pick-for-release-7.5,False,True,normal,database,"[{""filename"": ""br/pkg/storage/ks3.go"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/s3.go"", ""lines_added"": 7, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/s3_test.go"", ""lines_added"": 27, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,15228,"infoschema, planner: fix wrong `table_names` in statement summary","<!-- Thank you for contributing to TiDB! Please read TiDB's [CONTRIBUTING](https://github.com/pingcap/community/blob/master/CONTRIBUTING.md) document **BEFORE** filing this PR. -->  ### What problem does this PR solve? <!--add issue link with summary if exists--> 1. In `create database test` statement, `table_names` in `events_statements_summary_by_digest` is ""test."". 2. In point get statement, schema name in `table_names` is current db, not the schema where table exists.  ### What is changed and how it works? 1. If table name is empty, do not output this table name. 2. Replace the schema name with the right one in point get statement.  `StmtCtx.Tables` is only used in statement summary and test cases, so it's safe.  ### Check List <!--REMOVE the items that are not applicable-->  Tests <!-- At least one of them must be included. -->   - Unit test  Code changes  N/A  Side effects  N/A  Related changes   - Need to cherry-pick to the release branch  Release note   - Fix the bug that `table_names` in statement summary tables may be wrong in some statements. ",2020-03-09T08:47:18+00:00,2020-03-09T09:54:17+00:00,3,https://github.com/pingcap/tidb/pull/15228,15228.0,2020-03-09T09:54:17+00:00,https://github.com/pingcap/tidb/pull/15228,0,3,0,3,21,3,0,24,1.1163888888888889,status/can-merge;component/infoschema,False,True,normal,database,"[{""filename"": ""infoschema/perfschema/tables_test.go"", ""lines_added"": 16, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""planner/core/point_get_plan.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""util/stmtsummary/statement_summary.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3389,"On Darwin, `fd_pwrite` should respect offset even with append flag","### Summary  I reported this issue with WasmEdge on Linux in #3062. Because of [the pwrite/append bug on Linux](https://linux.die.net/man/2/pwrite), other runtimes behave similar to WasmEdge (with the exception of Wasmtime). However, WasmEdge behavior diverges on Darwin.  Specifically, Darwin does not have the Linux pwrite/append bug, so pwrite should write to the specified offset even with append flag.  ### Current State  _No response_  ### Expected State  _No response_  ### Reproduction steps  This repro is taken from #3062. Compare the produced file between WasmEdge and some other runtime, e.g. Wasmtime.  ```c #include <fcntl.h> #include <unistd.h> #include <stdio.h> #include <stdlib.h>  int main() {     int f = open(""tmp/a"", O_CREAT | O_WRONLY);     if (f == -1) {         perror(""open""); 	return 1;     }      char buf[67];      int written = write(f, buf, 67);     printf(""written %d\\n"", written);      int ret = fcntl(f, F_SETFL, O_APPEND);     if (ret == -1) { 	perror(""fcntl""); 	return 1;     }       char buf2[102];      int written2 = pwrite(f, buf2, 102, 0);     printf(""written %d\\n"", written2); } ```  ### Screenshots  ![DESCRIPTION](LINK.png)   ### Any logs you want to share for showing the specific issue  _No response_  ### Components  CLI  ### WasmEdge Version or Commit you used  0f11476c19a5570a027035e4a5d58a203c17c7a5  ### Operating system information  Mac OS Darwin Kernel Version 23.4.0  ### Hardware Architecture  amd64  ### Compiler flags and options  _No response_",2024-05-07T21:34:08+00:00,2024-11-29T06:32:33+00:00,1,https://github.com/WasmEdge/WasmEdge/issues/3389,3390.0,,https://github.com/WasmEdge/WasmEdge/pull/3390,0,2,0,2,86,11,0,97,4928.973611111112,bug,False,True,normal,ui,"[{""filename"": ""lib/host/wasi/inode-macos.cpp"", ""lines_added"": 14, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""test/host/wasi/wasi.cpp"", ""lines_added"": 72, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3317,bug: error trying to connect: dns error: Success (os error 85) when running on cluster,"### Summary  When trying to call an (AKS cluster) internal service or an external service like  ```     let url =         url::Url::parse(""http://eu.httpbin.org/ip"").map_err(|e| ServiceError::ParseError(e))?;     tracing::info!(""test url {}"", url);      let response = reqwest::Client::new()         .get(url)         .send()         .await         .map_err(|e| ServiceError::ReqwestError(e))?         .text()         .await         .map_err(|e| ServiceError::ReqwestError(e))?; ```  I get this error when running on a cluster - with local `wasmedge run` the call is succeeding.  ``` error sending request for url (http://eu.httpbin.org/ip): error trying to connect: dns error: Success (os error 85) ```  ### Current State  I install [#wasmedge](https://cloud-native.slack.com/archives/C0215BBK248) 0.3.0 with [#kwasm](https://cloud-native.slack.com/archives/C05P2KNK7RV) node installer [ghcr.io/kwasm/kwasm-node-installer:v0.3.1](http://ghcr.io/kwasm/kwasm-node-installer:v0.3.1) on AKS,  add a runtime class,  ``` apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata:   name: wasmedge-v1 handler: wasmedge scheduling:   nodeSelector:     agentpool: wasm ```  deploy a service with [#knative-serving](https://cloud-native.slack.com/archives/C04LMU0AX60),  ``` apiVersion: serving.knative.dev/v1 kind: Service metadata:   name: distributor   namespace: default spec:   template:     spec:       runtimeClassName: wasmedge-v1       timeoutSeconds: 1       containers:         - name: http-server           image: { will be replaced by deployment script }           ports:             - containerPort: 8080               protocol: TCP           livenessProbe:             tcpSocket:               port: 8080   traffic:     - latestRevision: true       percent: 100 ```   ### Expected State  I expect DNS resolution of cluster is applied and call to internal and external services is succeeding.  ### Reproduction steps  1. Build with options : `wasmedge compile target/wasm32-wasi/release/warpwasi_dapr_rs.wasm target/warpwasi_dapr_rs.wasm` 2. Execute with flags : see [call to WasmEdge SDK in runwasi containerd shim](https://github.com/containerd/runwasi/tree/main/crates/containerd-shim-wasmedge) 3. Execute with inputs : n.a. 4. Get error  ----  infrastructure: <https://github.com/ZEISS/enterprise-wasm/tree/knative/infra/aks-kn-dapr> module: <https://github.com/ZEISS/enterprise-wasm/tree/knative/samples/warpwasi-dapr-rs>  ### Screenshots  _No response_  ### Any logs you want to share for showing the specific issue  _No response_  ### Components  Rust SDK  ### WasmEdge Version or Commit you used  0.13.5  ### Operating system information  CBL-Mariner/Linux  ### Hardware Architecture  arm64  ### Compiler flags and options  _No response_",2024-04-02T11:35:25+00:00,2024-11-26T05:31:38+00:00,3,https://github.com/WasmEdge/WasmEdge/issues/3317,350.0,2021-08-12T04:16:02+00:00,https://github.com/WasmEdge/WasmEdge/pull/350,0,0,1,1,2,0,0,0,-23143.32305555556,bug,False,True,normal,networking,"[{""filename"": ""include/common/int128.h"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
pingcap/tidb,16456,"executor, infoschema, util: add sum_errors and sum_warnings to statement summary tables","<!-- Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve?  Problem Summary: Add `sum_errors` and `sum_warnings` to `statements_summary`. These 2 fields indicate the total errors and warnings encountered for each kind of SQL.  ### What is changed and how it works?  What's Changed: ONLY summarize runtime errors now. It's a big move to also consider compiler errors, so it's not suitable for 4.0. MySQL: Statements that has parser errors are not summarized in statement summary tables. TiDB: Statements that has parser errors or compiler errors are not summarized.  How it Works: Error: check whether it succeeded. Warning: get `StmtCtx.WarningCount()`.  ### Related changes  - PR to update `pingcap/docs`/`pingcap/docs-cn`: - Need to cherry-pick to the release branch  ### Check List <!--REMOVE the items that are not applicable-->  Tests <!-- At least one of them must be included. -->  - Unit test  Side effects  N/A  ### Release note <!-- bugfixes or new feature need a release note -->  - Add two fields `sum_errors` and `sum_warnings` to statement summary tables.",2020-04-16T04:59:22+00:00,2020-04-17T02:31:36+00:00,8,https://github.com/pingcap/tidb/pull/16456,16456.0,2020-04-17T02:31:36+00:00,https://github.com/pingcap/tidb/pull/16456,0,6,0,6,52,9,0,61,21.537222222222223,sig/execution;status/can-merge;component/infoschema;sig/sql-infra,False,True,normal,database,"[{""filename"": ""executor/adapter.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""infoschema/tables.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""infoschema/tables_test.go"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""session/tidb.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""util/stmtsummary/statement_summary.go"", ""lines_added"": 14, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""util/stmtsummary/statement_summary_test.go"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,17108,variables: return error when setting statement summary variables to invalid values,"<!-- Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve?  Problem Summary: System variables are transformed to a valid value according to min/max values. But this valid value is stored into TiKV, while the current server still uses the invalid value. For example,  ```sql set global tidb_stmt_summary_refresh_interval = -1; ``` `-1` is invalid and the value is transformed to `1` when validating it, and `1` is stored to `mysql.global_variables` table. However, 1 is not returned in the current `SET` statement, and the `SET` statement passes `-1` to statement summary.  ### What is changed and how it works?  What's Changed: Throw an error when it's set to an invalid value. This is simple and reasonable.  ### Related changes  - Need to cherry-pick to the release branch  ### Check List <!--REMOVE the items that are not applicable-->  Tests <!-- At least one of them must be included. -->  - Unit test - Manual test (add detailed scripts or steps below) ``` mysql> set global tidb_stmt_summary_refresh_interval=1<<31; ERROR 1231 (42000): Variable 'tidb_stmt_summary_refresh_interval' can't be set to the value of '2147483648' mysql> set session tidb_stmt_summary_refresh_interval=0; ERROR 1231 (42000): Variable 'tidb_stmt_summary_refresh_interval' can't be set to the value of '0' ```  Side effects  - Breaking backward compatibility  ### Release note <!-- bugfixes or new feature need a release note -->  - return error when setting statement summary system variables to invalid values.",2020-05-12T03:16:21+00:00,2020-05-12T09:01:07+00:00,5,https://github.com/pingcap/tidb/pull/17108,17108.0,2020-05-12T09:01:06+00:00,https://github.com/pingcap/tidb/pull/17108,0,2,0,2,25,19,0,44,5.745833333333334,type/bugfix;status/can-merge;compatibility-breaker,False,True,normal,database,"[{""filename"": ""sessionctx/variable/varsutil.go"", ""lines_added"": 15, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""sessionctx/variable/varsutil_test.go"", ""lines_added"": 10, ""lines_deleted"": 15, ""file_type"": ""app_code""}]",,False
pingcap/tidb,20275,ddl: investigate failure of tics-test [DNM],"<!-- Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve?  Issue Number: close #xxx <!-- REMOVE this line if no issue to close -->  Problem Summary: Do not merge, just to investigate https://internal.pingcap.net/idc-jenkins/blue/organizations/jenkins/tidb_ghpr_tics_test/detail/tidb_ghpr_tics_test/576/pipeline/  ### What is changed and how it works?  Proposal: [xxx](url) <!-- REMOVE this line if not applicable -->  What's Changed:  How it Works:  ### Related changes  - PR to update `pingcap/docs`/`pingcap/docs-cn`: - Need to cherry-pick to the release branch  ### Check List <!--REMOVE the items that are not applicable-->  Tests <!-- At least one of them must be included. -->  - Unit test - Integration test - Manual test (add detailed scripts or steps below) - No code  Side effects  - Performance regression     - Consumes more CPU     - Consumes more MEM - Breaking backward compatibility  ### Release note <!-- bugfixes or new feature need a release note -->  - <!-- Please write a release note here to describe the change you made when it is released to the users of TiDB. If your PR doesn't involve any change to TiDB(like test enhancements, RFC proposals...), you can write `No release note`. --> ",2020-09-28T08:52:55+00:00,2020-09-29T02:29:20+00:00,5,https://github.com/pingcap/tidb/pull/20275,20275.0,,https://github.com/pingcap/tidb/pull/20275,0,4,0,4,31,55,0,86,17.606944444444444,sig/sql-infra,False,True,normal,performance,"[{""filename"": ""ddl/ddl_api.go"", ""lines_added"": 10, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""ddl/partition.go"", ""lines_added"": 9, ""lines_deleted"": 13, ""file_type"": ""app_code""}, {""filename"": ""ddl/placement_rule_test.go"", ""lines_added"": 8, ""lines_deleted"": 29, ""file_type"": ""app_code""}, {""filename"": ""domain/infosync/info.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",ddl,False
cloudwego/kitex,1509,feat(retry): support Mixed Retry which integrating Failure Retry and Backup Request ,"#### What type of PR is this? <!-- Add one of the following kinds:  build: Changes that affect the build system or external dependencies (example scopes: gulp, broccoli, npm) ci: Changes to our CI configuration files and scripts (example scopes: Travis, Circle, BrowserStack, SauceLabs) docs: Documentation only changes feat: A new feature optimize: A new optimization fix: A bug fix perf: A code change that improves performance refactor: A code change that neither fixes a bug nor adds a feature style: Changes that do not affect the meaning of the code (white space, formatting, missing semi-colons, etc) test: Adding missing tests or correcting existing tests chore: Changes to the build process or auxiliary tools and libraries such as documentation generation -->  #### Check the PR title. <!-- The description of the title will be attached in Release Notes,  so please describe it from user-oriented, what this PR does / why we need it. Please check your PR title with the below requirements: --> - [ ] This PR title match the format: \\<type\\>(optional scope): \\<description\\> - [ ] The description of this PR title is user-oriented and clear enough for others to understand. - [ ] Attach the PR updating the user documentation if the current PR requires user awareness at the usage level. [User docs repo](https://github.com/cloudwego/cloudwego.github.io)   #### (Optional) Translate the PR title into Chinese. zh: feat(retry): 支持 Mixed Retry，融合了 Failure Retry 和 Backup Request 的功能  #### (Optional) More detailed description for this PR(en: English/zh: Chinese). <!-- Provide more detailed info for review(e.g., it's recommended to provide perf data if this is a perf type PR). --> en:  zh(optional):   #### (Optional) Which issue(s) this PR fixes: <!-- Automatically closes linked issue when PR is merged. Eg: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. -->  #### (optional) The PR that updates user documentation: <!-- If the current PR requires user awareness at the usage level, please submit a PR to update user docs. [User docs repo](https://github.com/cloudwego/cloudwego.github.io) -->",2024-08-26T09:16:42+00:00,2024-08-28T12:56:15+00:00,0,https://github.com/cloudwego/kitex/pull/1509,1509.0,2024-08-28T12:56:15+00:00,https://github.com/cloudwego/kitex/pull/1509,0,18,0,18,2267,1065,0,3332,51.65916666666666,,False,True,normal,configuration,"[{""filename"": ""client/callopt/options.go"", ""lines_added"": 15, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""client/callopt/options_test.go"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/option.go"", ""lines_added"": 23, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""client/option_test.go"", ""lines_added"": 82, ""lines_deleted"": 39, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/backup.go"", ""lines_added"": 37, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/backup_retryer.go"", ""lines_added"": 17, ""lines_deleted"": 27, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/backup_test.go"", ""lines_added"": 68, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/failure.go"", ""lines_added"": 103, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/failure_retryer.go"", ""lines_added"": 107, ""lines_deleted"": 93, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/failure_test.go"", ""lines_added"": 600, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/mixed.go"", ""lines_added"": 82, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/mixed_retryer.go"", ""lines_added"": 270, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/mixed_test.go"", ""lines_added"": 625, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/policy.go"", ""lines_added"": 34, ""lines_deleted"": 129, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/policy_test.go"", ""lines_added"": 75, ""lines_deleted"": 47, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/retryer.go"", ""lines_added"": 10, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/retryer_test.go"", ""lines_added"": 106, ""lines_deleted"": 699, ""file_type"": ""app_code""}, {""filename"": ""pkg/retry/util.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
WasmEdge/WasmEdge,3811,bug: `INode::fdWrite` sometimes returns 0 incorrectly on Windows,"### Summary  While trying to replicate a read-slowdown bug observed in a Rust project, I found that sometimes `File::write_all` was reporting `WRITE_ALL_EOF` failure despite writing all its data.  On examination, it seems that `File::write` was sometimes returning `Ok(0)` despite data having been written, and this appears to be bubbling up from   ### Current State  Given this repro case in Rust: ```rust // https://github.com/WasmEdge/WasmEdge/issues/3811 use std::fs::File; use std::io::{Error, Write};  fn inspect_failure(e: &Error, attempts: usize, buffer_size: usize) {     assert_eq!(e.kind(), std::io::ErrorKind::WriteZero);     let expected_size: u64 = (attempts * buffer_size).try_into().unwrap();     let filesize = std::fs::metadata(""tmp.txt"").unwrap().len();     println!(""Unexpected failure seen; after {attempts} loops of writing a {buffer_size} string ({expected_size} expected bytes), filesize was {filesize}."");     assert_eq!(expected_size, filesize); }  fn main() -> std::io::Result<()> {     const BUF: &[u8] =         ""This textline will be repeated multiple times as part of a test\\n"".as_bytes();     assert_eq!(BUF.len(), 64);     let mut output = File::create(""tmp.txt"")?;      for attempts in 1..=1024 {         output             .write_all(BUF)             .inspect_err(|err| inspect_failure(err, attempts, BUF.len()))?;     }     Ok(()) } ```  the program fails more often than it passes.  Note that `File::write_all`'s implementation is to repeatedly call `File::write` with the given buffer until it is empty, and it throws an error if `File::write` reports a 0-byte success. (If the buffer is already empty, `File::write` is not called.)  Per Rust's `Write::Write` trait documentation: ```     /// If this method consumed `n > 0` bytes of `buf` it must return [`Ok(n)`].     /// If the return value is `Ok(n)` then `n` must satisfy `n <= buf.len()`.     /// A return value of `Ok(0)` typically means that the underlying object is     /// no longer able to accept bytes and will likely not be able to in the     /// future as well, or that the buffer provided is empty. ```  I did have a simpler repro which open-coded `File::write_all` and confirmed that the failure was always coming through on the first loop, so there was no OS interrupts or partial writes being seen, which makes sense for such a small buffer.  I believe this to be a bug on the WasmEdge side because, per my repro, the data _was_ written by the call that returned `Ok(0)`, so I suspect the bug is that in `INode::fdWrite`; specifically, there's a code-path in [this block](https://github.com/WasmEdge/WasmEdge/blob/0.14.1/lib/host/wasi/inode-win.cpp#L1210-L1227) where `NWritten += NumberOfBytesWrite` is not called, even though bytes were written, and no error was returned to the caller. I can't immediately see the faulty code path, since it seems like the only way to not call that addition is to set Result. And in this case, there's only one IOV.  I _suspect_ the fix is to _aways_ call `NWritten += NumberOfBytesWrite` after `GetOverlappedResult`, irrespective of the value of the result, so that we do not accidentally signal EOF due to a (valid) partial write.  Looking at the code, the failure-path loop that calls `GetOverlappedResult` on the remaining cancelled operations may wipe out the number of bytes written, so I think we should be adding `NWritten` immediately after `GetOverlappedResult` in all cases, before looking at the status code. (This probably applies to all 8 calls to `GetOverlappedResult` in this file, I think).  ### Expected State  All 65kb of data should be written and no errors raised.  ### Reproduction steps  ```pwsh cargo new wasmedge-3811 cd wasmedge-3811 # Populate src/main.rs with the above repro case however you see fit $Env:CARGO_TARGET_WASM32_WASIP1_RUNNER=""wasmedge --dir .:."" cargo run --target wasm32-wasip1 ```    ### Screenshots  _No response_  ### Any logs you want to share for showing the specific issue  ```console > cargo run --target wasm32-wasip1     Compiling wasmedge-3811 v0.1.0 (C:\\Users\\paulh\\Rust\\wasmedge-3811)     Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.61s      Running `wasmedge --dir .:. target\\wasm32-wasip1\\debug\\wasmedge-3811.wasm` Unexpected failure seen; after 1 loops of writing a 64 string (64 expected bytes), filesize was 64. Error: Error { kind: WriteZero, message: ""failed to write whole buffer"" } error: process didn't exit successfully: `wasmedge --dir .:. target\\wasm32-wasip1\\debug\\wasmedge-3811.wasm` (exit code: 1) ```  Note that other failure counts are possible, e.g., ```console Unexpected failure seen; after 25 loops of writing a 64 string (1600 expected bytes), filesize was 1600. ```  ### Components  CLI, Rust SDK  ### WasmEdge Version or Commit you used  0.14.1  ### Operating system information  Windows 11 (Microsoft Windows [Version 10.0.22631.4169])  ### Hardware Architecture  x86_64  ### Compiler flags and options  _No response_",2024-10-01T14:28:41+00:00,2024-11-12T16:35:57+00:00,5,https://github.com/WasmEdge/WasmEdge/issues/3811,3870.0,2024-11-12T16:35:56+00:00,https://github.com/WasmEdge/WasmEdge/pull/3870,0,1,1,2,20,27,0,19,1010.1208333333332,bug;priority:low;platform-windows;WASI,False,True,normal,database,"[{""filename"": ""include/system/winapi.h"", ""lines_added"": 12, ""lines_deleted"": 16, ""file_type"": ""other""}, {""filename"": ""lib/host/wasi/inode-win.cpp"", ""lines_added"": 8, ""lines_deleted"": 11, ""file_type"": ""app_code""}]",system,False
pingcap/tidb,31322,privileges: show grants return incorrect results when granted with 2 or more privileges,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and  linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #30855  Problem Summary: When there is no db privilege granted to the user, but more than 1 table privilege granted to him, `show grants` will show only one table privilege due to a bug.  ### What is changed and how it works?  Simplify the code in `showGrants` because it's buggy: it checks `dbPrivTable[record.DB]` before updating `tablePrivTable`. I think the author copied the wrong code.  ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No code  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- bugfix or new feature needs a release note -->  ```release-note - Fix the bug that `show grants` returns incorrect results when granted with 2 or more privileges ``` ",2022-01-05T07:21:56+00:00,2022-01-06T03:40:37+00:00,5,https://github.com/pingcap/tidb/pull/31322,31322.0,2022-01-06T03:40:37+00:00,https://github.com/pingcap/tidb/pull/31322,0,2,0,2,42,29,0,71,20.31138888888889,status/LGT2;release-note;status/can-merge;sig/sql-infra;size/M,False,True,normal,database,"[{""filename"": ""privilege/privileges/cache.go"", ""lines_added"": 4, ""lines_deleted"": 20, ""file_type"": ""app_code""}, {""filename"": ""privilege/privileges/privileges_test.go"", ""lines_added"": 38, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59403,Memory leak of LoadPrivilegeLoop?,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. -->  Like https://github.com/pingcap/tidb/issues/59400  > I create 2M users, and the keep 10% of them active, then run >  > ""set password for test = 'xxx'"" for all the 2M users >  > then run >  > ""alter user test%d failed_login_attempts 10"" for all the 2M users >  > ...   This time analyze the memory usage after the workload is gone.  I suspect there is a leak, after the workload gone, the memory usage is still there:  <img width=""1918"" alt=""Image"" src=""https://github.com/user-attachments/assets/5201211e-0d22-4f20-a024-8a9e032ead70"" />  As you can see the CPU is 0%, but at time 15:45, the memory usage is high. And after a privilege full reload (the spike), the memory usage finally decreased.  <img width=""1847"" alt=""Image"" src=""https://github.com/user-attachments/assets/948d2055-1119-49af-8302-bf066483e0e2"" />  Here is the mem profile before and after the full reload:  [after.pb.gz](https://github.com/user-attachments/files/18746934/after.pb.gz) [before.pb.gz](https://github.com/user-attachments/files/18746935/before.pb.gz)    ### 2. What did you expect to see? (Required)  The memory usage should reduce after the workload is gone like after.pb.gz  <img width=""1916"" alt=""Image"" src=""https://github.com/user-attachments/assets/9de7ef92-a83f-4701-8977-1c40b35a40c1"" />  ### 3. What did you see instead (Required)   See before.pb.gz <img width=""1920"" alt=""Image"" src=""https://github.com/user-attachments/assets/0725ed21-4f64-46e9-ab18-41e746c72bea"" />   ### 4. What is your TiDB version? (Required)  <!-- Paste the output of SELECT tidb_version() -->  master? ``` commit 27365b47b27820152faf79395f6cf1b96eb3031f (HEAD -> master, origin/master, origin/HEAD) Author: xzhangxian1008 <xzhangxian@foxmail.com> Date:   Tue Feb 11 15:35:39 2025 +0800      executor: fix the incorrect return when hash join encounters error (#59381)          close pingcap/tidb#59377 ``` ",2025-02-11T08:19:22+00:00,2025-02-27T06:54:08+00:00,1,https://github.com/pingcap/tidb/issues/59403,59812.0,,https://github.com/pingcap/tidb/pull/59812,0,4,0,4,705,80,0,785,382.5794444444445,type/bug;sig/sql-infra;severity/minor;feature/developing,False,True,normal,security,"[{""filename"": ""pkg/domain/domain.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/privilege/privileges/cache.go"", ""lines_added"": 625, ""lines_deleted"": 56, ""file_type"": ""app_code""}, {""filename"": ""pkg/privilege/privileges/cache_test.go"", ""lines_added"": 75, ""lines_deleted"": 23, ""file_type"": ""app_code""}, {""filename"": ""pkg/privilege/privileges/privileges.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
pingcap/tidb,50451,"import into failed when inject global sort dir network delay 50ms with error “read xxx bytes from external storage, exceed max limit 1073741824”","## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required) 1、tidb_enable_dist_task='on' and  2、enable global sort and tidb_cloud_storage_uri is set to minio dir 3、run import into 4、inject minio network delay 50ms  ### 2. What did you expect to see? (Required) import into can success  ### 3. What did you see instead (Required) import failed 						 the status of import job is not finished or running (now: 2024-01-16 02:48:14, jobId: 150003, step: importing, status: failed) operatorLogs: [2024-01-16 02:28:44] ###### start import into [2024-01-16 02:28:44] ###### wait for import job to finish [2024-01-16 02:48:14] ###### wait for import job to finish failed select id, step, status from mysql.tidb_import_jobs where start_time >= '2024-01-16 02:28:44' jobId: 150003, step: importing, status: failed  tidb logs： [2024/01/16 02:48:06.486 +08:00] [ERROR] [reader.go:49] [""read all data failed""] [takeTime=2m27.255433123s] [error=""failed to read file 150003/90036/data/b0dba35c-6041-47e1-af29-cc98ec29d765/40: read 316276121654947278 bytes from external storage, exceed max limit 1073741824""]  [tidb-0.tar.gz](https://github.com/pingcap/tidb/files/13945076/tidb-0.tar.gz) [tidb-1.tar.gz](https://github.com/pingcap/tidb/files/13945078/tidb-1.tar.gz)   ### 4. What is your TiDB version? (Required) ./tidb-server -V  Release Version: v7.6.0 Edition: Community Git Commit Hash: 42960c3352d165810c79852b85ada448a68efa4c Git Branch: heads/refs/tags/v7.6.0 UTC Build Time: 2024-01-15 11:33:46 GoVersion: go1.21.5 Race Enabled: false Check Table Before Drop: false Store: unistore 2024-01-16T02:28:41.835+0800  ",2024-01-16T03:32:03+00:00,2025-02-24T02:17:49+00:00,3,https://github.com/pingcap/tidb/issues/50451,59694.0,2025-02-24T02:17:48+00:00,https://github.com/pingcap/tidb/pull/59694,0,3,0,3,49,0,0,49,9718.7625,type/bug;severity/moderate;component/ddl;affects-8.1;affects-8.5,False,True,normal,networking,"[{""filename"": ""br/pkg/storage/ks3.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/s3.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""br/pkg/storage/s3_test.go"", ""lines_added"": 47, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59773,planner: fix the wrong transform in the join reorder,"<!--  Thank you for contributing to TiDB!  PR Title Format: 1. pkg [, pkg2, pkg3]: what's changed 2. *: what's changed  -->  ### What problem does this PR solve? <!--  Please create an issue first to describe the problem.  There MUST be one line starting with ""Issue Number:  "" and linking the relevant issues via the ""close"" or ""ref"".  For more info, check https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/contribute-code.html#referring-to-an-issue.  -->  Issue Number: close #59643  Problem Summary:  ### What changed and how does it work?  after https://github.com/pingcap/tidb/pull/58521, when to convert the equal condition， We only consider the equality condition. but nulleq is considered too. but in the join reorder, We still believed that the equal condition could only be EQ, so we forcibly changed all functions to EQ, which led to the occurrence of errors. ### Check List  Tests <!-- At least one of them must be included. -->  - [x] Unit test - [ ] Integration test - [ ] Manual test (add detailed scripts or steps below) - [ ] No need to test   > - [ ] I checked and no code files have been changed.   > <!-- Or your custom  ""No need to test"" reasons -->  Side effects  - [ ] Performance regression: Consumes more CPU - [ ] Performance regression: Consumes more Memory - [ ] Breaking backward compatibility  Documentation  - [ ] Affects user behaviors - [ ] Contains syntax changes - [ ] Contains variable changes - [ ] Contains experimental features - [ ] Changes MySQL compatibility  ### Release note  <!-- compatibility change, improvement, bugfix, and new feature need a release note -->  Please refer to [Release Notes Language Style Guide](https://pingcap.github.io/tidb-dev-guide/contribute-to-tidb/release-notes-style-guide.html) to write a quality release note.  ```release-note planner: fix the wrong transform in the join reorder  修复 join reorder 时错误的表达式转换 ``` ",2025-02-26T06:24:17+00:00,2025-02-26T13:46:59+00:00,9,https://github.com/pingcap/tidb/pull/59773,59773.0,2025-02-26T13:46:58+00:00,https://github.com/pingcap/tidb/pull/59773,0,2,1,3,59,4,0,61,7.378055555555555,sig/planner;release-note;size/M;approved;lgtm,False,True,normal,database,"[{""filename"": ""pkg/planner/core/issuetest/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/planner/core/issuetest/planner_issue_test.go"", ""lines_added"": 54, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/rule_join_reorder.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",rule_join_reorder.go,False
cloudwego/kitex,1311,kitex将下层的错误加上了remote or network error[remote]: biz error: 前缀，我该如何在上层获取原始返回的err呢,"我在kitex服务端返回的错误是dal.ErrUserNotExist 然后在controller层判断错误errors.Is(err,dal.ErrUserNotExist)，可是实际的err多了remote or network error[remote]: biz error: 前缀，可以直接获取原始err吗",2024-03-23T08:50:37+00:00,2024-04-10T06:19:13+00:00,4,https://github.com/cloudwego/kitex/issues/1311,1736.0,2025-03-18T11:22:31+00:00,https://github.com/cloudwego/kitex/pull/1736,0,5,0,5,82,3,0,85,8642.531666666666,,False,True,normal,networking,"[{""filename"": ""client/callopt/options.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/callopt/streamcall/call_options.go"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/callopt/streamcall/call_options_test.go"", ""lines_added"": 35, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""client/client.go"", ""lines_added"": 8, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""client/client_test.go"", ""lines_added"": 25, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,20935,[improve] [ml] Persist mark deleted ops to ZK if create cursor ledger was failed,"### Motivation  The progress Persist mark deleted position is like this: - persist to BK - if failed to persist to BK, try to persist to ZK  But in the current implementation: if the cursor ledger was created failed, Pulsar will not try to persist to ZK. It makes if the cursor ledger created fails, a lot of ack records can not be persisted, and we will get a lot of repeat consumption after the BK recover.  ### Modifications  Try to persist the mark deleted position to ZK if the cursor ledger was created failed  ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->  ### Matching PR in forked repository  PR in forked repository: x ",2023-08-06T17:41:26+00:00,2023-08-31T18:37:01+00:00,5,https://github.com/apache/pulsar/pull/20935,20935.0,2023-08-31T18:37:01+00:00,https://github.com/apache/pulsar/pull/20935,0,4,0,4,241,30,0,271,600.9263888888889,type/bug;doc-not-needed;cherry-picked/branch-3.0;release/3.0.2;release/3.1.1;cherry-picked/branch-3.1;category/functionality,False,True,normal,ui,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedCursorImpl.java"", ""lines_added"": 47, ""lines_deleted"": 30, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java"", ""lines_added"": 28, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/impl/ManagedCursorTest.java"", ""lines_added"": 102, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/test/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerTest.java"", ""lines_added"": 64, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,58613,importinto from select: negative value cast to upper bound instead of 0 on non-strict sql mode,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> ```sql mysql> create table t(id int unsigned); Query OK, 0 rows affected (0.07 sec)  mysql> create table s(id int); Query OK, 0 rows affected (0.05 sec)  mysql> insert into s values(-1); Query OK, 1 row affected (0.01 sec)  mysql> insert into t select * from s; ERROR 1690 (22003): constant -1 overflows int mysql> import into t from select * from s; ERROR 1690 (22003): constant -1 overflows int   mysql> set sql_mode=''; Query OK, 0 rows affected (0.00 sec)  mysql> insert into t select * from s; Query OK, 1 row affected, 1 warning (0.03 sec) Records: 1  Duplicates: 0  Warnings: 1  mysql> show warnings; +---------+------+---------------------------+ | Level   | Code | Message                   | +---------+------+---------------------------+ | Warning | 1690 | constant -1 overflows int | +---------+------+---------------------------+ 1 row in set (0.01 sec)  mysql> select * from t; +------+ | id   | +------+ |    0 | +------+ 1 row in set (0.00 sec)  mysql> truncate table t; Query OK, 0 rows affected (0.26 sec)  mysql> import into t from select * from s; select * from t; Query OK, 1 row affected (2.62 sec) Records: 1, ID: 26a2f775-b3b4-4148-880c-523929a77573  mysql> select * from t; +------------+ | id         | +------------+ | 4294967295 | +------------+ 1 row in set (0.00 sec) ``` ### 2. What did you expect to see? (Required)  ### 3. What did you see instead (Required)  ### 4. What is your TiDB version? (Required) 8.5 <!-- Paste the output of SELECT tidb_version() -->  ",2024-12-30T08:23:05+00:00,2025-01-06T06:28:18+00:00,1,https://github.com/pingcap/tidb/issues/58613,59729.0,,https://github.com/pingcap/tidb/pull/59729,0,4,1,5,757,0,0,756,166.08694444444444,type/bug;severity/major;component/ddl;affects-8.1;affects-8.5,False,True,normal,database,"[{""filename"": ""br/pkg/lightning/backend/kv/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/lightning/backend/kv/context.go"", ""lines_added"": 252, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/lightning/backend/kv/context_test.go"", ""lines_added"": 316, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/util/misc.go"", ""lines_added"": 22, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/realtikvtest/importintotest2/from_select_test.go"", ""lines_added"": 166, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
apache/pulsar,23524,[fix][broker] Fix reading entries failed due to max in-flight reading,"### Motivation  If the `estimatedReadSize` larger than the `managedLedgerMaxReadsInFlightSizeInMB`, reading entries will fail.  ``` 2024-10-10T14:31:46,939+0000 [BookKeeperClientWorker-OrderedExecutor-2-0] ERROR org.apache.bookkeeper.mledger.impl.cache.RangeEntryCacheImpl - Time-out elapsed while acquiring enough permits on the memory limiter to read from ledger 114226, public/default/persistent/test-partition-0, estimated read size 120875300 bytes for 100 entries (check managedLedgerMaxReadsInFlightSizeInMB) ```   ### Documentation  <!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->  - [ ] `doc` <!-- Your PR contains doc changes. --> - [ ] `doc-required` <!-- Your PR changes impact docs and you will update later --> - [x] `doc-not-needed` <!-- Your PR changes do not impact docs --> - [ ] `doc-complete` <!-- Docs have been already added -->   ",2024-10-28T13:39:38+00:00,2024-11-08T01:54:30+00:00,3,https://github.com/apache/pulsar/pull/23524,23524.0,,https://github.com/apache/pulsar/pull/23524,0,2,0,2,13,2,0,15,252.24777777777777,doc-not-needed;ready-to-test;release/3.3.3;release/3.0.8;release/4.0.1,False,True,normal,ui,"[{""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/cache/InflightReadsLimiter.java"", ""lines_added"": 5, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/cache/RangeEntryCacheImpl.java"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59643,Wrong result returned using DISTINCT and <=> operator in JOIN,"## Bug Report  TiDB v8.5.1 returned the wrong result using DISTINCT and <=> operator in JOIN. ```sql with base (c1, c2) as (   select distinct 1, 'Alice'   union   select NULL, 'Bob'  ), base2 (c1, c3) as (   select 1, 100   union   select NULL, NULL ) select * from base inner join base2 	on base.c1 <=> base2.c1; ```  ### 1. Minimal reproduce step (Required)  ```sql tidb:4000 > SELECT VERSION(); +--------------------+ | VERSION()          | +--------------------+ | 8.0.11-TiDB-v8.5.1 | +--------------------+ 1 row in set (0.01 sec)  tidb:4000 > with base (c1, c2) as (     ->   select distinct 1, 'Alice'     ->   union     ->   select NULL, 'Bob'     -> ),     -> base2 (c1, c3) as (     ->   select 1, 100     ->   union     ->   select NULL, NULL     -> )     -> select * from base     -> inner join base2     -> on base.c1 <=> base2.c1; +------+-------+------+------+ | c1   | c2    | c1   | c3   | +------+-------+------+------+ |    1 | Alice |    1 |  100 | +------+-------+------+------+ 1 row in set (0.00 sec) ```  ### 2. What did you expect to see? (Required)  v8.1.2 returns expected result. ```sql tidb:49590 > SELECT VERSION(); +--------------------+ | VERSION()          | +--------------------+ | 8.0.11-TiDB-v8.1.2 | +--------------------+ 1 row in set (0.00 sec)  tidb:49590 > with base (c1, c2) as (     ->   select distinct 1, 'Alice'     ->   union     ->   select NULL, 'Bob'     -> ),     -> base2 (c1, c3) as (     ->   select 1, 100     ->   union     ->   select NULL, NULL     -> )     -> select * from base     -> inner join base2     -> on base.c1 <=> base2.c1; +------+-------+------+------+ | c1   | c2    | c1   | c3   | +------+-------+------+------+ |    1 | Alice |    1 |  100 | | NULL | Bob   | NULL | NULL | +------+-------+------+------+ 2 rows in set (0.01 sec) ```  MySQLv8 also returns as expected. ```sql mysql> SELECT VERSION(); +-----------+ | VERSION() | +-----------+ | 8.0.33    | +-----------+ 1 row in set (0.01 sec)  mysql> with base (c1, c2) as (     ->   select distinct 1, 'Alice'     ->   union     ->   select NULL, 'Bob'     -> ),     -> base2 (c1, c3) as (     ->   select 1, 100     ->   union     ->   select NULL, NULL     -> )     -> select * from base     -> inner join base2     -> on base.c1 <=> base2.c1; +------+-------+------+------+ | c1   | c2    | c1   | c3   | +------+-------+------+------+ |    1 | Alice |    1 |  100 | | NULL | Bob   | NULL | NULL | +------+-------+------+------+ 2 rows in set (0.01 sec) ```  If you don't use `DISTINCT`, you can get the expected result in v8.5.1. ```sql tidb:4000 > with base (c1, c2) as (     ->   select /* distinct */ 1, 'Alice'     ->   union     ->   select NULL, 'Bob'     -> ),     -> base2 (c1, c3) as (     ->   select 1, 100     ->   union     ->   select NULL, NULL     -> )     -> select * from base     -> inner join base2     -> on base.c1 <=> base2.c1; +------+-------+------+------+ | c1   | c2    | c1   | c3   | +------+-------+------+------+ |    1 | Alice |    1 |  100 | | NULL | Bob   | NULL | NULL | +------+-------+------+------+ 2 rows in set (0.00 sec) ```  ### 3. What did you see instead (Required)  v8.5.1 doesn't return the record joined by NULL `| NULL | Bob   | NULL | NULL |`.  ### 4. What is your TiDB version? (Required)  ```sql tidb:4000 > SELECT VERSION(); +--------------------+ | VERSION()          | +--------------------+ | 8.0.11-TiDB-v8.5.1 | +--------------------+ 1 row in set (0.00 sec) ```  ",2025-02-19T08:13:48+00:00,2025-02-26T13:47:00+00:00,3,https://github.com/pingcap/tidb/issues/59643,59796.0,2025-02-28T08:25:19+00:00,https://github.com/pingcap/tidb/pull/59796,0,2,1,3,59,4,0,61,216.19194444444443,type/bug;sig/planner;type/regression;severity/major;severity/moderate;impact/wrong-result;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/planner/core/issuetest/BUILD.bazel"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""pkg/planner/core/issuetest/planner_issue_test.go"", ""lines_added"": 54, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/planner/core/rule_join_reorder.go"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",rule_join_reorder.go,False
cloudwego/kitex,1309,"Compilation Error: ""expected pseudo-register; found R13""","> This issue is created for searching  # 错误模式 | Error Pattern Compilation error like:  > /go/pkg/mod/github.com/choleraehyq/pid@v0.0.13/pid_go1.5_amd64.s:28: expected pseudo-register; found R13 > /go/pkg/mod/github.com/choleraehyq/pid@v0.0.13/pid_go1.5_amd64.s:29: expected pseudo-register; found R14 > asm: assembly of /go/pkg/mod/github.com/choleraehyg/pid@v0.0.13/pid_go1.5_amd64.s failed   # 解决方案 | Solution  Update `github.com/choleraehyq/pid` to the latest version by  > go get github.com/choleraehyq/pid@latest  # 说明 | Explanation  - This pid library provides a way to get the ID of a Processor or Goroutine (refer to [GMP model](https://www.google.com/search?q=golang+gmp+model) for more details)   - It is implemented by exporting Go runtime's internal symbols, making it bound to certain Go versions and CPU Arch and needs to be updated each time a new version of Go is released. - Kitex relies on this library to provide certain functionalities.",2024-03-20T09:53:45+00:00,2024-03-20T09:53:52+00:00,0,https://github.com/cloudwego/kitex/issues/1309,1729.0,2025-03-12T02:51:35+00:00,https://github.com/cloudwego/kitex/pull/1729,0,3,0,3,241,80,0,321,8560.96388888889,,False,True,normal,performance,"[{""filename"": ""tool/internal_pkg/pluginmode/protoc/plugin.go"", ""lines_added"": 21, ""lines_deleted"": 80, ""file_type"": ""app_code""}, {""filename"": ""tool/internal_pkg/tpl/pbtpl/pbtpl.go"", ""lines_added"": 129, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tool/internal_pkg/tpl/pbtpl/pbtpl_test.go"", ""lines_added"": 91, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
pingcap/tidb,59677,current user can alter its own password option without 'CREATE USER' privilege,"## Bug Report  Please answer these questions before submitting your issue. Thanks!  ### 1. Minimal reproduce step (Required)  <!-- a step by step guide for reproducing the bug. --> on mysql ```sql mysql> create user test identified by '123456'; Query OK, 0 rows affected (0.07 sec) ``` then login with `test` ``` mysql> alter user test password expire never; ERROR 1227 (42000): Access denied; you need (at least one of) the CREATE USER privilege(s) for this operation ``` on tidb ```sql mysql> create user test identified by '123456'; Query OK, 0 rows affected (0.04 sec) ``` then login with `test` ```sql mysql> alter user test password expire never; Query OK, 0 rows affected (0.04 sec) ```  quote from mysql doc https://dev.mysql.com/doc/refman/8.4/en/alter-user.html > In most cases, [ALTER USER](https://dev.mysql.com/doc/refman/8.4/en/alter-user.html) requires the global [CREATE USER](https://dev.mysql.com/doc/refman/8.4/en/privileges-provided.html#priv_create-user) privilege, or the [UPDATE](https://dev.mysql.com/doc/refman/8.4/en/privileges-provided.html#priv_update) privilege for the mysql system schema. The exceptions are:  but in our impl, the current user can alway alter params related to itself https://github.com/pingcap/tidb/blob/811be5aaa57390f32dfe6be6ee2ac0f8cd97ae71/pkg/executor/simple.go#L1758-L1786  ### 2. What did you expect to see? (Required) err ### 3. What did you see instead (Required) ok ### 4. What is your TiDB version? (Required) master <!-- Paste the output of SELECT tidb_version() -->  ",2025-02-20T09:09:58+00:00,2025-02-26T08:30:54+00:00,0,https://github.com/pingcap/tidb/issues/59677,59756.0,2025-02-26T08:30:53+00:00,https://github.com/pingcap/tidb/pull/59756,0,1,2,3,46,2,0,9,143.3486111111111,type/bug;sig/sql-infra;severity/major;affects-6.1;affects-6.5;affects-7.1;affects-7.5;affects-8.1;affects-8.5,False,True,normal,database,"[{""filename"": ""pkg/executor/simple.go"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integrationtest/r/privilege/privileges.result"", ""lines_added"": 16, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""tests/integrationtest/t/privilege/privileges.test"", ""lines_added"": 22, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
cloudwego/hertz,1226,chore: add debug log for HERTZ_DISABLE_REQUEST_CONTEXT_POOL env,"#### What type of PR is this? chore  #### Check the PR title. <!-- The description of the title will be attached in Release Notes,  so please describe it from user-oriented, what this PR does / why we need it. Please check your PR title with the below requirements: --> - [ ] This PR title match the format: \\<type\\>(optional scope): \\<description\\> - [ ] The description of this PR title is user-oriented and clear enough for others to understand. - [ ] Attach the PR updating the user documentation if the current PR requires user awareness at the usage level. [User docs repo](https://github.com/cloudwego/cloudwego.github.io)   #### (Optional) Translate the PR title into Chinese. 给 HERTZ_DISABLE_REQUEST_CONTEXT_POOL env 添加一个剩下日志，便于用户问题排查  #### (Optional) More detailed description for this PR(en: English/zh: Chinese). <!-- Provide more detailed info for review(e.g., it's recommended to provide perf data if this is a perf type PR). --> en: zh(optional):  #### (Optional) Which issue(s) this PR fixes: <!-- Automatically closes linked issue when PR is merged. Eg: `Fixes #<issue number>`, or `Fixes (paste link of issue)`. -->  #### (Optional) The PR that updates user documentation: <!-- If the current PR requires user awareness at the usage level, please submit a PR to update user docs. [User docs repo](https://github.com/cloudwego/cloudwego.github.io) -->",2024-11-11T08:00:48+00:00,2025-02-27T04:58:02+00:00,0,https://github.com/cloudwego/hertz/pull/1226,1226.0,,https://github.com/cloudwego/hertz/pull/1226,0,2,0,2,8,6,0,14,2588.9538888888887,,False,True,normal,database,"[{""filename"": ""pkg/protocol/http1/server.go"", ""lines_added"": 6, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/protocol/http1/server_test.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",http1,False
nats-io/nats.go,1823,Fetch tries to access metadata after parsing error,"### Observed behavior  Looking at the code here: https://github.com/nats-io/nats.go/blob/0f6e9ef1c9c2d1ae2b107a39e1bbb72e23c693d4/jetstream/pull.go#L810-L814 you are setting res.err if there is an error parsing metadata, but then you go ahead and try and use meta.Sequence.Stream anyway. I don't know if this ever happens in practice as I didn't see it happen, but I was looking at the code and it seems to me this could cause a panic.  ### Expected behavior  The code does not try to use the metadata after it fails to parse.  ### Server and client version  nats server 2.10.26 nats client v0.1.6  ### Host environment  _No response_  ### Steps to reproduce  _No response_",2025-03-18T07:06:27+00:00,2025-03-19T15:52:50+00:00,1,https://github.com/nats-io/nats.go/issues/1823,1828.0,2025-03-19T15:52:49+00:00,https://github.com/nats-io/nats.go/pull/1828,0,1,0,1,3,0,0,3,32.772777777777776,defect,False,True,normal,database,"[{""filename"": ""jetstream/pull.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
cloudwego/hertz,1261,cmd tool hz init files with unexpected executable permissions,"**Describe the bug**  hz 初始化项目后，除 build.sh 与 script/bootstrap.sh 外的文件也有可执行权限 After initializing a project, files other than ""build.sh"" and ""script/bootstrap.sh"" also have executable permissions.  **To Reproduce**  Steps to reproduce the behavior: 1. [ in shell ] hz new -module xxx/yyy 2. [ in shell ] ls -lah 3. See error  **Expected behavior**  只有 build.sh 与 script/bootstrap.sh 文件具有可执行权限 Only the files ""build.sh"" and ""script/bootstrap.sh"" have executable permissions.  **Hertz version:** hz version v0.9.1 github.com/cloudwego/hertz v0.9.5  ",2025-01-16T19:19:21+00:00,2025-01-22T08:34:44+00:00,1,https://github.com/cloudwego/hertz/issues/1261,1262.0,2025-01-22T08:34:43+00:00,https://github.com/cloudwego/hertz/pull/1262,0,1,1,2,12,9,0,12,133.25611111111112,,False,True,normal,security,"[{""filename"": ""cmd/hz/generator/template.go"", ""lines_added"": 5, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""cmd/hz/test_hz_unix.sh"", ""lines_added"": 7, ""lines_deleted"": 2, ""file_type"": ""other""}]",,False
micronaut-projects/micronaut-core,11598,Introspected interface referencing Lombok builder class fails with java.lang.IllegalAccessError,"### Expected Behavior  The interface should be successfully introspected at compile time, and an application context using the interface should start successfully.  ### Actual Behaviour  With Micronaut 4.6.3, everything works fine. With Micronaut 4.7.6 it seems to fail either at compile time or at context initialization time, with an error.  An example stack trace:  ``` Failed to load a service: java.lang.IllegalAccessError: class example.$Bar$Introspection tried to access protected method 'void java.lang.Record.<init>()' (example.$Bar$Introspection is in unnamed module of loader 'app'; java.lang.Record is in module java.base of loader 'bootstrap') io.micronaut.core.io.service.SoftServiceLoader$ServiceLoadingException: Failed to load a service: java.lang.IllegalAccessError: class example.$Bar$Introspection tried to access protected method 'void java.lang.Record.<init>()' (example.$Bar$Introspection is in unnamed module of loader 'app'; java.lang.Record is in module java.base of loader 'bootstrap') 	at io.micronaut.core.io.service.ServiceScanner$ServiceInstanceLoader.collect(ServiceScanner.java:244) 	at io.micronaut.core.io.service.ServiceScanner$DefaultServiceCollector.collect(ServiceScanner.java:157) 	at io.micronaut.core.io.service.SoftServiceLoader.collectDynamicServices(SoftServiceLoader.java:199) 	at io.micronaut.core.io.service.SoftServiceLoader.collectAll(SoftServiceLoader.java:174) 	at io.micronaut.core.beans.DefaultBeanIntrospector.getIntrospections(DefaultBeanIntrospector.java:117) 	at io.micronaut.core.beans.DefaultBeanIntrospector.findIntrospection(DefaultBeanIntrospector.java:88) 	at io.micronaut.core.reflect.InstantiationUtils.tryInstantiate(InstantiationUtils.java:165) 	at io.micronaut.context.conditions.MatchesCustomCondition.matches(MatchesCustomCondition.java:58) 	at io.micronaut.context.AbstractInitializableBeanDefinitionAndReference.matches(AbstractInitializableBeanDefinitionAndReference.java:111) 	at io.micronaut.context.AbstractInitializableBeanDefinitionAndReference.isEnabled(AbstractInitializableBeanDefinitionAndReference.java:94) 	at io.micronaut.context.DefaultBeanContext$BeanDefinitionProducer.isDefinitionEnabled(DefaultBeanContext.java:4294) 	at io.micronaut.context.DefaultBeanContext$BeanDefinitionProducer.isDefinitionEnabled(DefaultBeanContext.java:4274) 	at io.micronaut.context.DefaultBeanContext.collectBeanCandidates(DefaultBeanContext.java:2167) 	at io.micronaut.context.DefaultBeanContext.findBeanCandidates(DefaultBeanContext.java:2135) 	at io.micronaut.context.DefaultBeanContext.findBeanCandidatesInternal(DefaultBeanContext.java:3429) 	at io.micronaut.context.DefaultBeanContext.getBeanRegistrations(DefaultBeanContext.java:3485) 	at io.micronaut.context.DefaultBeanContext.getBeansOfType(DefaultBeanContext.java:1446) 	at io.micronaut.context.DefaultBeanContext.getBeansOfType(DefaultBeanContext.java:883) 	at io.micronaut.context.DefaultBeanContext.getBeansOfType(DefaultBeanContext.java:873) 	at io.micronaut.context.event.ApplicationEventPublisherFactory$2.lambda$$1(ApplicationEventPublisherFactory.java:215) 	at io.micronaut.core.util.SupplierUtil$1.get(SupplierUtil.java:47) 	at io.micronaut.context.event.ApplicationEventPublisherFactory$2.publishEvent(ApplicationEventPublisherFactory.java:226) 	at io.micronaut.context.DefaultBeanContext.publishEvent(DefaultBeanContext.java:1831) 	at io.micronaut.context.DefaultBeanContext.start(DefaultBeanContext.java:360) 	at io.micronaut.context.DefaultApplicationContext.start(DefaultApplicationContext.java:216) 	at io.micronaut.test.extensions.AbstractMicronautExtension.startApplicationContext(AbstractMicronautExtension.java:507) 	at io.micronaut.test.extensions.AbstractMicronautExtension.beforeClass(AbstractMicronautExtension.java:346) 	at io.micronaut.test.extensions.junit5.MicronautJunit5Extension.beforeAll(MicronautJunit5Extension.java:84) 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$13(ClassBasedTestDescriptor.java:396) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:396) 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:212) 	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:85) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:153) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:160) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) 	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94) 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:568) 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92) 	at jdk.proxy1/jdk.proxy1.$Proxy4.stop(Unknown Source) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63) 	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) 	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:121) 	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71) 	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) 	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) Caused by: io.micronaut.core.io.service.SoftServiceLoader$ServiceLoadingException: java.lang.IllegalAccessError: class example.$Bar$Introspection tried to access protected method 'void java.lang.Record.<init>()' (example.$Bar$Introspection is in unnamed module of loader 'app'; java.lang.Record is in module java.base of loader 'bootstrap') 	at io.micronaut.core.io.service.SoftServiceLoader.lambda$collectDynamicServices$2(SoftServiceLoader.java:195) 	at io.micronaut.core.io.service.ServiceScanner$ServiceInstanceLoader.compute(ServiceScanner.java:235) 	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:194) 	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373) 	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182) 	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655) 	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622) 	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165) Caused by: java.lang.IllegalAccessError: class example.$Bar$Introspection tried to access protected method 'void java.lang.Record.<init>()' (example.$Bar$Introspection is in unnamed module of loader 'app'; java.lang.Record is in module java.base of loader 'bootstrap') 	at example.$Bar$Introspection.<clinit>(Unknown Source) 	at java.base/jdk.internal.misc.Unsafe.allocateInstance(Native Method) 	at java.base/java.lang.invoke.DirectMethodHandle.allocateInstance(DirectMethodHandle.java:520) 	at io.micronaut.core.io.service.SoftServiceLoader.lambda$collectDynamicServices$2(SoftServiceLoader.java:187) 	... 7 more ```  ### Steps To Reproduce  1. create a class that implements an interface. Use Lombok Builder for the class, and add an annotation on the interface  that references the builder class. 2. compile 3. start a context using those classes  See the sample project attached. Run `./gradlew test --stacktrace` to see the issue. Note that with `micronautVersion=4.6.3`, the test passes. With `micronautVersion=4.7.6`, the test fails.  [micronaut-test-case.zip](https://github.com/user-attachments/files/18854276/micronaut-test-case.zip)  ### Environment Information  Operating System: Windows 11 (but would happen with any) Java 17  ### Example Application  _No response_  ### Version  4.7.6",2025-02-18T20:46:09+00:00,2025-02-26T12:37:57+00:00,5,https://github.com/micronaut-projects/micronaut-core/issues/11598,11617.0,2025-02-26T12:37:45+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11617,0,4,0,4,52,3,0,55,183.86,type: bug;status: pr submitted;status: example attached,False,True,normal,security,"[{""filename"": ""core/src/main/java/io/micronaut/core/annotation/AnnotationClassValue.java"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""test-suite/src/test/java/io/micronaut/test/lombok/Bar.java"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test-suite/src/test/java/io/micronaut/test/lombok/Foo.java"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test-suite/src/test/java/io/micronaut/test/lombok/LombokIntrospectedBuilderTest.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1762,Correct Error Returned from DeleteObjectStore,Currently `DeleteObjectStore` incorrectly returns `ErrStreamNotFound` if the Object Store Bucket is not found,2025-01-07T11:14:09+00:00,2025-01-07T11:21:28+00:00,0,https://github.com/nats-io/nats.go/pull/1762,1798.0,2025-02-05T16:12:42+00:00,https://github.com/nats-io/nats.go/pull/1798,0,1,1,2,2,2,0,2,700.9758333333333,,False,True,normal,functional,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""nats.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
cloudwego/hertz,1179,"--bugfix=""assert.NotNil not work"" ", #1178  修复 assert的 NotNil 不生效。（参数判断有误）,2024-08-26T10:59:50+00:00,2024-08-28T10:26:11+00:00,8,https://github.com/cloudwego/hertz/pull/1179,1179.0,,https://github.com/cloudwego/hertz/pull/1179,0,4,0,4,12,8,0,20,47.439166666666665,,False,True,normal,functional,"[{""filename"": ""pkg/common/test/assert/assert.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/common/utils/ioutil_test.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/protocol/http1/req/header_test.go"", ""lines_added"": 6, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/protocol/multipart_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1733,Race in reading error from message batch,"### Observed behavior  type messageBatch has a race where the user can just check the error at any time, but there is no mutex on the read. Meanwhile any error like context ending will cause an error to be written.  ### Expected behavior  no race  ### Server and client version  client 1.37.0  ### Host environment  _No response_  ### Steps to reproduce  kill context and read error from message batch",2024-11-03T00:19:20+00:00,2024-12-17T14:05:38+00:00,0,https://github.com/nats-io/nats.go/issues/1733,1743.0,2024-12-17T14:05:37+00:00,https://github.com/nats-io/nats.go/pull/1743,0,6,0,6,312,127,0,439,1069.7713888888889,defect,False,True,normal,functional,"[{""filename"": ""jetstream/ordered.go"", ""lines_added"": 8, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""jetstream/pull.go"", ""lines_added"": 20, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""jetstream/test/ordered_test.go"", ""lines_added"": 104, ""lines_deleted"": 112, ""file_type"": ""app_code""}, {""filename"": ""jetstream/test/pull_test.go"", ""lines_added"": 85, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""js.go"", ""lines_added"": 15, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""test/js_test.go"", ""lines_added"": 80, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",ordered.go;ordered_test.go,True
nats-io/nats.go,1707,Refactor: Use errors.New instead of fmt.Errorf to improve efficiency,errors.New provides more efficiency than fmt.Errorf. 🚀 ,2024-08-27T23:08:45+00:00,2024-09-19T12:22:16+00:00,0,https://github.com/nats-io/nats.go/pull/1707,1754.0,2024-12-17T16:21:55+00:00,https://github.com/nats-io/nats.go/pull/1754,0,1,1,2,2,2,0,2,2681.2194444444444,,False,True,normal,functional,"[{""filename"": ""README.md"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""nats.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1619,[FIXED] Call ConnectedCB with RetryOnFailedConnect when initial conn failed,This fixes an issue when `ConnectedCB` is not called when using RetryOnFailedConnect and the first attempt to connect fails but subsequent retry is successful.  Previously `ReconnectedCB` was invoked in such case.  Signed-off-by: Piotr Piotrowski <piotr@synadia.com>,2024-04-19T11:32:30+00:00,2024-04-22T12:15:55+00:00,1,https://github.com/nats-io/nats.go/pull/1619,1619.0,2024-04-22T12:15:55+00:00,https://github.com/nats-io/nats.go/pull/1619,0,2,0,2,136,47,0,183,72.72361111111111,,False,True,normal,functional,"[{""filename"": ""nats.go"", ""lines_added"": 9, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""test/conn_test.go"", ""lines_added"": 127, ""lines_deleted"": 42, ""file_type"": ""app_code""}]",,False
easegress-io/easegress,1108,[Bug]: Easegress obejct status api prefix issue,"Since Easegress support multi node. When call `/status/objects/{name}` api, Easegress use prefix to fetch status from all node.   For example, when call `/status/objects/httpserver-demo` it will return `/status/objects/httpserver-demo/node1` and `/status/objects/httpserver-demo/node2`.   But if there is a object called `/status/objects/httpserver-demo-123`, when call  `/status/objects/httpserver-demo`, it will return `/status/objects/httpserver-demo/node1`, `/status/objects/httpserver-demo/node2`, `/status/objects/httpserver-demo-123/node1`, `/status/objects/httpserver-demo-123/node2`.   To solve this problem, we should use `/status/objects/httpserver-demo/` as prefix not `/status/objects/httpserver-demo`. Add `/` to the end.",2023-10-10T04:08:23+00:00,2023-12-15T10:38:08+00:00,0,https://github.com/easegress-io/easegress/issues/1108,1165.0,2023-12-15T10:38:06+00:00,https://github.com/easegress-io/easegress/pull/1165,0,1,0,1,1,1,0,2,1590.4952777777778,bug,False,True,normal,functional,"[{""filename"": ""pkg/cluster/layout.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1516,[FIXED] Invalid checkPending logic,- fixed invalid condition when `s.fetchInProgress` was ignored - fixed calculating batch size when working with MaxBytes  Signed-off-by: Piotr Piotrowski <piotr@synadia.com>,2024-01-10T22:45:47+00:00,2024-01-11T14:52:03+00:00,0,https://github.com/nats-io/nats.go/pull/1516,1516.0,2024-01-11T14:52:03+00:00,https://github.com/nats-io/nats.go/pull/1516,0,2,0,2,196,5,0,201,16.104444444444443,,False,True,normal,functional,"[{""filename"": ""jetstream/jetstream_test.go"", ""lines_added"": 182, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jetstream/pull.go"", ""lines_added"": 14, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
micronaut-projects/micronaut-core,10235,MutableHttpRequest doesn't set multiple cookies properly,"### Expected Behavior  MutableHttpRequest (specifically NettyMutableHttpRequest)'s `cookies(Set<Cookie>)` method should result in all cookies being set in the request  ### Actual Behaviour  `cookies(Set<Cookie>)` results in only one cookie being set (the last one in the set).    This appears to be an issue with the NettyMutableHttpRequest `cookie` method, which overrides any existing cookie header with the new cookie input.  ### Steps To Reproduce  invoke the following code in an http-enabled project and observe the resulting request ``` @ServerFilter(""/**"") class IncomingRequestFilter {      @RequestFilter     @ExecuteOn(TaskExecutors.BLOCKING)     void filterRequest(MutableHttpRequest<?> request) {         Cookie newCookie2 = Cookie.of(""COOKIE_1"", ""one"")         Cookie newCookie = Cookie.of(""COOKIE_2"", ""two"")          Set<Cookie> cookies = new HashSet<>()         cookies.addAll(newCookie)         cookies.addAll(newCookie2)          request.cookies(cookies)     } } ```  ### Environment Information  _No response_  ### Example Application  _No response_  ### Version  4.1.6",2023-12-08T10:32:42+00:00,2024-07-15T06:26:32+00:00,1,https://github.com/micronaut-projects/micronaut-core/issues/10235,10973.0,2024-07-15T06:26:31+00:00,https://github.com/micronaut-projects/micronaut-core/pull/10973,0,2,2,4,127,8,0,26,5275.896944444445,type: bug;info: good first issue,False,True,normal,functional,"[{""filename"": ""http-server-netty/src/test/groovy/io/micronaut/http/server/netty/binding/NettyHttpResponseSpec.groovy"", ""lines_added"": 22, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""http/src/main/java/io/micronaut/http/simple/SimpleHttpRequest.java"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""http/src/main/java/io/micronaut/http/simple/SimpleHttpResponse.java"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""http/src/test/groovy/io/micronaut/http/HttpRequestSpec.groovy"", ""lines_added"": 81, ""lines_deleted"": 6, ""file_type"": ""other""}]",http-server-netty,False
nats-io/nats.go,592,gollvm: minio/highwayhash/highwayhashAVX2_amd64.s:10: Error: no such instruction,"I was trying to pull the latest source code and build nats-server. So I did  > $ go get github.com/nats-io/nats-server And I got numerous bugs:  > # github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s: Assembler messages: > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:10: Error: no such instruction: `data ·consAVX2<>+0x00(SB)/8,$0xdbe6d5d5fe4cce2f' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:11: Error: no such instruction: `data ·consAVX2<>+0x08(SB)/8,$0xa4093822299f31d0' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:12: Error: no such instruction: `data ·consAVX2<>+0x10(SB)/8,$0x13198a2e03707344' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:13: Error: no such instruction: `data ·consAVX2<>+0x18(SB)/8,$0x243f6a8885a308d3' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:14: Error: no such instruction: `data ·consAVX2<>+0x20(SB)/8,$0x3bd39e10cb0ef593' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:15: Error: no such instruction: `data ·consAVX2<>+0x28(SB)/8,$0xc0acf169b5f18a8c' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:16: Error: no such instruction: `data ·consAVX2<>+0x30(SB)/8,$0xbe5466cf34e90c6c' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:17: Error: no such instruction: `data ·consAVX2<>+0x38(SB)/8,$0x452821e638d01377' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:18: Error: no such instruction: `globl ·consAVX2<>(SB),(NOPTR+RODATA),$64' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:20: Error: no such instruction: `data ·zipperMergeAVX2<>+0x00(SB)/8,$0xf010e05020c03' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:21: Error: no such instruction: `data ·zipperMergeAVX2<>+0x08(SB)/8,$0x70806090d0a040b' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:22: Error: no such instruction: `data ·zipperMergeAVX2<>+0x10(SB)/8,$0xf010e05020c03' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:23: Error: no such instruction: `data ·zipperMergeAVX2<>+0x18(SB)/8,$0x70806090d0a040b' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:24: Error: no such instruction: `globl ·zipperMergeAVX2<>(SB),(NOPTR+RODATA),$32' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:27: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:27: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:28: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:28: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:29: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:29: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:30: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:30: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:31: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:32: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:32: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:33: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:33: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:34: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:34: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:35: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:35: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:36: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:36: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:37: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:37: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:38: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:39: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:39: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:40: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:41: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:41: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:42: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:42: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:43: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:43: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:44: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:44: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:45: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:45: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:46: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:46: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:47: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:48: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:48: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:49: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:50: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:50: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:51: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:51: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:52: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:52: Error: invalid character '\\' in operand 2 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:53: Error: too many memory references for `xor' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:56: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:56: Error: too many memory references for `vpaddq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:57: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:57: Error: too many memory references for `vpaddq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:58: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:59: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:59: Error: invalid character '\\' in operand 3 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:60: Error: no such instruction: `byte $0xC5' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:60: Error: no such instruction: `byte $0xFD' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:60: Error: no such instruction: `byte $0xF4' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:60: Error: no such instruction: `byte $0xC2 \\//VPMULUDQ Y2,Y0,Y0' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:61: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:61: Error: too many memory references for `vpxor' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:62: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:63: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:63: Error: too many memory references for `vpaddq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:64: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:65: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:65: Error: invalid character '\\' in operand 3 > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:66: Error: no such instruction: `byte $0xC5' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:66: Error: no such instruction: `byte $0xFD' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:66: Error: no such instruction: `byte $0xF4' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:66: Error: no such instruction: `byte $0xC1 \\//VPMULUDQ Y1,Y0,Y0' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:67: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:67: Error: too many memory references for `vpxor' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:68: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:69: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:69: Error: too many memory references for `vpshufb' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:70: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:70: Error: too many memory references for `vpaddq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:71: Error: junk at end of line, first unrecognized character is `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:72: Warning: stray `\\' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:72: Error: too many memory references for `vpshufb' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:73: Error: too many memory references for `vpaddq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:76: Error: no such instruction: `text ·initializeAVX2(SB),4,$0-32' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:77: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:77: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:78: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:78: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:81: Error: junk `(BX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:81: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:82: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:84: Error: junk `(CX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:84: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:85: Error: junk `(CX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:85: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:87: Error: too many memory references for `vpxor' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:88: Error: too many memory references for `vpxor' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:90: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:91: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:92: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:93: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:98: Error: no such instruction: `text ·updateAVX2(SB),4,$0-32' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:99: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:99: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:100: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:100: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:101: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:101: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:103: Error: operand size mismatch for `cmp' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:106: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:106: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:107: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:107: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:108: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:108: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:109: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:109: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:111: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:114: Error: junk `(BX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:114: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:115: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:121: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:122: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:123: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:124: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:131: Error: no such instruction: `text ·finalizeAVX2(SB),4,$0-32' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:132: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:132: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:133: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:133: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:134: Error: junk `(FP)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:134: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:136: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:136: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:137: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:137: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:138: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:138: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:139: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:139: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:141: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:143: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:144: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:145: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:147: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:148: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:149: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:151: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:152: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:153: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:155: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:156: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:157: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:159: Error: operand size mismatch for `cmp' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:160: Error: bad expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:160: Error: junk `Just 4 rounds for 64-bit checksum' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:162: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:163: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:164: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:166: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:167: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:168: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:170: Error: operand size mismatch for `cmp' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:171: Error: bad expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:171: Error: junk `6 rounds for 128-bit checksum' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:173: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:174: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:175: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:177: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:178: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:179: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:181: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:182: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:183: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:185: Error: too many memory references for `vperm2i128' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:186: Error: too many memory references for `vpshufd' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:187: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:190: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:191: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:192: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:193: Error: too many memory references for `vmovdqu' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:196: Error: operand size mismatch for `cmp' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:198: Error: operand size mismatch for `cmp' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:202: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:202: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:203: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:203: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:204: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:204: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:205: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:205: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:206: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:206: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:207: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:207: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:208: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:208: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:209: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:209: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:211: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:212: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:213: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:215: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:215: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:216: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:216: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:217: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:217: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:218: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:218: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:219: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:219: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:220: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:220: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:221: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:221: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:222: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:222: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:224: Error: invalid character '(' in mnemonic > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:225: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:226: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:230: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:230: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:231: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:231: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:232: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:232: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:233: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:233: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:234: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:234: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:235: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:235: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:236: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:236: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:237: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:237: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:238: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:239: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:243: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:243: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:244: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:244: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:245: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:245: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:246: Error: junk `(AX)' after expression > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:246: Error: too many memory references for `add' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:247: Error: too many memory references for `movq' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:160: Error: invalid operands (.text and *ABS* sections) for `/' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:160: Error: division by zero > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:171: Error: invalid operands (.text and *ABS* sections) for `/' > go/src/github.com/nats-io/nats-server/vendor/github.com/minio/highwayhash/highwayhashAVX2_amd64.s:171: Error: division by zero  Ivan",2020-09-14T17:56:17+00:00,2023-12-12T06:17:54+00:00,8,https://github.com/nats-io/nats.go/issues/592,695.0,2021-03-30T22:54:20+00:00,https://github.com/nats-io/nats.go/pull/695,0,2,0,2,102,182,0,284,4732.9675,bug,False,True,normal,database,"[{""filename"": ""nats.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""nats_test.go"", ""lines_added"": 95, ""lines_deleted"": 182, ""file_type"": ""app_code""}]",,False
micronaut-projects/micronaut-core,11537,Fix loading primitive classes + support primitive classes in annotations,When the correct primitive type is referenced it will generate primitive class reference instead,2025-01-24T12:10:09+00:00,2025-01-24T17:48:15+00:00,1,https://github.com/micronaut-projects/micronaut-core/pull/11537,11537.0,2025-01-24T17:48:15+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11537,0,4,1,5,99,0,0,60,5.635,type: bug,False,True,normal,functional,"[{""filename"": ""core-processor/src/main/java/io/micronaut/inject/processing/JavaModelUtils.java"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core-processor/src/main/java/io/micronaut/inject/writer/AbstractClassFileWriter.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/main/java/io/micronaut/annotation/processing/JavaAnnotationMetadataBuilder.java"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/annotation/AnnotationMetadataWriterSpec.groovy"", ""lines_added"": 39, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/annotation/MyAnnotationX.java"", ""lines_added"": 33, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1468,[FIXED] Invalid PullExpiry validation,Fixed invalid option validation where pull expiry could be set to < 1s. This bug was only present in `consumer.Messages()`  Fixes #1467  Signed-off-by: Piotr Piotrowski <piotr@synadia.com>,2023-11-10T15:42:58+00:00,2023-11-11T20:18:11+00:00,0,https://github.com/nats-io/nats.go/pull/1468,1468.0,2023-11-11T20:18:11+00:00,https://github.com/nats-io/nats.go/pull/1468,0,1,0,1,2,2,0,4,28.586944444444445,,False,True,normal,functional,"[{""filename"": ""jetstream/options.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
easegress-io/easegress,1062,Add log when httpserver runtime meet error,"when start server failed, log the error, so user can check and find problems.",2023-08-03T08:30:09+00:00,2023-08-03T09:15:27+00:00,0,https://github.com/easegress-io/easegress/pull/1062,1062.0,2023-08-03T09:15:27+00:00,https://github.com/easegress-io/easegress/pull/1062,0,1,0,1,2,0,0,2,0.755,,False,True,normal,functional,"[{""filename"": ""pkg/object/httpserver/runtime.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",object,False
easegress-io/easegress,998,fix websocket bugs,,2023-05-06T08:16:08+00:00,2023-05-24T01:45:07+00:00,1,https://github.com/easegress-io/easegress/pull/998,998.0,2023-05-24T01:45:07+00:00,https://github.com/easegress-io/easegress/pull/998,0,4,3,7,178,110,0,208,425.4830555555556,,False,True,normal,functional,"[{""filename"": ""doc/reference/filters.md"", ""lines_added"": 28, ""lines_deleted"": 24, ""file_type"": ""other""}, {""filename"": ""go.mod"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""go.sum"", ""lines_added"": 27, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/filters/proxies/httpproxy/wspool.go"", ""lines_added"": 119, ""lines_deleted"": 79, ""file_type"": ""app_code""}, {""filename"": ""pkg/filters/proxies/httpproxy/wspool_test.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/filters/proxies/httpproxy/wsproxy.go"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/object/mqttproxy/mqttproxy.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
easegress-io/easegress,984,fix websocket bugs,close #981 ,2023-04-28T07:11:22+00:00,2023-05-04T09:44:29+00:00,1,https://github.com/easegress-io/easegress/pull/984,984.0,2023-05-04T09:44:29+00:00,https://github.com/easegress-io/easegress/pull/984,0,1,0,1,8,0,0,8,146.55194444444444,,False,True,normal,functional,"[{""filename"": ""pkg/filters/proxies/httpproxy/wspool.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1387,fix: remove unnecessary error handling,Error handling is repeated twice. this commit omits one of them,2023-09-04T12:37:16+00:00,2023-09-04T12:55:33+00:00,0,https://github.com/nats-io/nats.go/pull/1387,1387.0,2023-09-04T12:55:32+00:00,https://github.com/nats-io/nats.go/pull/1387,0,1,0,1,0,3,0,3,0.3044444444444444,,False,True,normal,functional,"[{""filename"": ""jetstream/jetstream.go"", ""lines_added"": 0, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
easegress-io/easegress,981,[Bug]: The WebSocketProxy Filter does not work,"**Describe the bug** A clear and concise description of what the bug is.  Using `curl` to send a request will get a successful response by following the [WebSocket Cookbook](https://github.com/megaease/easegress/blob/main/doc/cookbook/websocket.md)   But after a real WebSocket server and client are deployed, the connection is not established.  The error in easegress's log:  > ERROR   httpproxy/wspool.go:183 websocketproxy#wsproxy#main: dial to ws://127.0.0.1:8765 failed: websocket.Dial ws://127.0.0.1:8765/ws: unsupported extensions    **To Reproduce** Steps to reproduce the behavior:  1. WebSocket server: ```python import asyncio import websockets  async def hello(websocket, path):     name = await websocket.recv()     print(f""Greetings {name}!"")      greeting = f""Hello {name}!""     await websocket.send(greeting)  start_server = websockets.serve(hello, ""localhost"", 8765)  asyncio.get_event_loop().run_until_complete(start_server) asyncio.get_event_loop().run_forever() ``` 2. Websocket Client:  ```python import asyncio import websockets  async def hello():     async with websockets.connect(""ws://localhost:8765/ws"") as websocket:         name = ""John""         await websocket.send(name)         greeting = await websocket.recv()         print(f""{greeting}"")  asyncio.get_event_loop().run_until_complete(hello()) ```  the client was verified by sending to the server directly and through an Nginx reverse proxy.  4. config a Easegress http server:  ```yml kind: HTTPServer name: http-8089-example port: 8089 rules:   - paths:     - pathPrefix: /api/v1/tts       backend: http-pipeline-tts     - pathPrefix: /ws       backend: websocket-pipeline ``` 5. configure a Easegress websocket proxy:  ```yaml name: websocket-pipeline kind: Pipeline  flow: - filter: wsproxy  filters: - kind: WebSocketProxy   name: wsproxy   defaultOrigin: http://127.0.0.1:8089/ws   pools:   - servers:     - url: ws://127.0.0.1:8765 ``` 6. update the URL to `ws://localhost:8089/ws' in c.py. 7. execute c.py and it failed.    **Expected behavior** A clear and concise description of what you expected to happen.  expected c.py output 'Hello John!'   when the Easegress is used as a WebSocket proxy.  **Version** The version number of Easegress. v2.4.0  **Configuration** * Easegress Configuration ```yaml ```  * HTTP server configuration ```yaml ```  * Pipeline Configuration ```yaml ```  **Logs** ``` Easegress logs, if applicable. ```  **OS and Hardware**  - OS: [e.g. Ubuntu 20.04]  - CPU:[e.g. Intel(R) Core(TM) i5-8265U]  - Memory: [e.g. 16GB]  **Additional context** Add any other context about the problem here.   --- Thanks for contributing 🎉! ",2023-04-18T05:08:45+00:00,2023-05-04T09:44:31+00:00,0,https://github.com/easegress-io/easegress/issues/981,984.0,2023-05-04T09:44:29+00:00,https://github.com/easegress-io/easegress/pull/984,0,1,0,1,8,0,0,8,388.5955555555556,bug,False,True,normal,configuration,"[{""filename"": ""pkg/filters/proxies/httpproxy/wspool.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
nats-io/nats.go,1381,[FIXED] typos in comments and error messages,Fixes typos in comments and error message found using codespell.,2023-08-28T19:39:53+00:00,2023-08-28T22:57:22+00:00,0,https://github.com/nats-io/nats.go/pull/1381,1381.0,2023-08-28T22:57:22+00:00,https://github.com/nats-io/nats.go/pull/1381,0,10,1,11,19,19,0,28,3.2913888888888887,,False,True,normal,functional,"[{""filename"": ""jetstream/README.md"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""jetstream/consumer.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""jetstream/ordered.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""jetstream/test/message_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""jetstream/test/publish_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""js.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""nats_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/conn_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/drain_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/js_test.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""test/object_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",ordered.go,False
nats-io/nats.go,195,Should Connect return errors if it cannot connect?,"Currently, the client 'constructor' (`nats.Connect`) will return `nil, err` if it can't connect to NATS.  This behaviour puts a strict ordering on the start up sequences of my services; the NATS client is effectively instantiated in my `main()` function, such that the service cannot start if it can't contact NATS.  However my service are able to tolerate NATS not being there, and degrade gracefully - as message delivery is at most once, I kinda don't care too much if it fails.  My preference would be for connection state to be completely internal; client creation should only fail it it is passed a malformed url, for example.  The client should try to connect asynchronously in the background, failing requests until it does - the service not being there is just another failure case to me. I believe this is just a special case of the NATS coming and going, which you already have code to deal with.  This seems like something the client library should do - I can't be the first person to hit this.  (As a side note, I actually don't have control over the order in which services are started; its not needed by anything else; the golang SQL client libraries and the AWS libraries all behave in my desired way)",2016-06-08T17:32:54+00:00,2020-07-08T15:23:45+00:00,12,https://github.com/nats-io/nats.go/issues/195,581.0,2020-07-08T15:23:45+00:00,https://github.com/nats-io/nats.go/pull/581,0,3,1,4,314,54,0,353,35781.8475,,False,True,normal,networking,"[{""filename"": ""README.md"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""nats.go"", ""lines_added"": 48, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""norace_test.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/conn_test.go"", ""lines_added"": 249, ""lines_deleted"": 46, ""file_type"": ""app_code""}]",,False
easegress-io/easegress,937,fix wasm bug,"1. filter type is map[string]interface{} not map[interface{}]interface{} 2. init response object 3. call alloc function before reading wasm memory, because wasm memory maybe changes when calling alloc function.",2023-02-23T02:12:24+00:00,2023-02-23T02:38:17+00:00,1,https://github.com/easegress-io/easegress/pull/937,937.0,2023-02-23T02:38:17+00:00,https://github.com/easegress-io/easegress/pull/937,0,3,0,3,15,8,0,23,0.4313888888888889,,False,True,normal,ui,"[{""filename"": ""pkg/api/wasm.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/filters/wasmhost/hostfunc.go"", ""lines_added"": 8, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""pkg/filters/wasmhost/wasmhost.go"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
easegress-io/easegress,938,[Bug]: wasm apply data command not worked,"**Describe the bug** Hi guys, I found two kind of issues when walking through our [Flash Sale](https://github.com/megaease/easegress/blob/main/doc/cookbook/flash-sale.md) demo.  **Issue 1:** Parameters' type error in section 6.1 example spec  According to WasmHost filter's spec declaration, [parameters](https://github.com/megaease/easegress/blob/main/pkg/filters/wasmhost/wasmhost.go#L94)' value type should be `string`:  ```go type Spec struct {      Parameters     map[string]string `json:""parameters"" jsonschema:""omitempty""`  } ```  But the example in section [6.1 Parameters](https://github.com/megaease/easegress/blob/main/doc/cookbook/flash-sale.md#61-parameters) used a number value: ```yaml filters:   - name: wasm     kind: WasmHost     parameters:                                        # +       startTime: ""2021-08-08T00:00:00+00:00""           # +       blockRatio: 0.4                                  # +       maxPermission: 3                                 # + ``` So `egctl` client returned error: ```shell Error: 400: {""generalErrs"":[""*pipeline.Spec: filters: json: cannot unmarshal number into Go struct field Spec.parameters of type string""]} ``` ---  **Issue 2:** Wasm `apply-data` command not worked  When executing the command below, I got this error: ```shell $ echo ' id/user4: ""true"" id/user5: ""true""' | egctl wasm apply-data flash-sale-pipeline wasm Error: name is empty: id/user4: ""true"" id/user5: ""true"" ```  I found that we used [SpecVisitor](https://github.com/megaease/easegress/blob/main/cmd/client/command/wasm.go#L88) to get spec doc: ```go Run: func(cmd *cobra.Command, args []string) {     visitor := buildSpecVisitor(specFile, cmd)  } ```  But `SpecVisitor` [did not allow empty name](https://github.com/megaease/easegress/blob/main/cmd/client/command/visitor.go#L97): ```go func (v *specVisitor) Visit(fn func(*spec) error) error {  	err := v.v.Visit(func(yamlDoc []byte) error {  		if s.Name == """" { 			return fmt.Errorf(""name is empty: %s"", doc) 		}  	})  } ```  And another issue I found in the same command is that we used [createObject](https://github.com/megaease/easegress/blob/main/pkg/api/object.go#L51) handler to hanle apply data request instead of [wasmApplyData](https://github.com/megaease/easegress/blob/main/pkg/api/wasm.go#L159) handler: ```go Run: func(cmd *cobra.Command, args []string) {      visitor.Visit(func(s *spec) error {         handleRequest(http.MethodPost, makeURL(objectsURL), []byte(s.doc), cmd)      })  } ```  **Version** Latest main branch version. ",2023-02-24T07:10:09+00:00,2023-03-03T06:58:14+00:00,0,https://github.com/easegress-io/easegress/issues/938,939.0,2023-03-03T06:58:13+00:00,https://github.com/easegress-io/easegress/pull/939,0,1,3,4,25,25,0,6,167.8011111111111,bug,False,True,normal,configuration,"[{""filename"": ""cmd/client/command/wasm.go"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""doc/cookbook/flash-sale.md"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""other""}, {""filename"": ""doc/cookbook/security.md"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""doc/reference/filters.md"", ""lines_added"": 17, ""lines_deleted"": 17, ""file_type"": ""other""}]",,False
nats-io/nats.go,1343,New JetStream API: calling `Stop()` makes `Next()` block indefinitely or until a missing heartbeat error,"## Defect  Make sure that these boxes are checked before submitting your issue -- thank you!   - [x] Included nats.go version  - [x] Included a [Minimal, Complete, and Verifiable example] (https://stackoverflow.com/help/mcve)  #### Versions of `nats.go` and the `nats-server` if one was involved: - nats.go: `v1.27.1` - nats-server: `v2.9.19`  #### OS/Container environment: - OS: `linux` -  Arch: `amd64` - Go: `v1.20.5`  #### Steps or code to reproduce the issue: 1. Use the new JetStream API consumer with `Messages()` ```go iter, err := cons.Messages(jetstream.PullMaxMessages(1)) if err != nil { 	log.Fatal(err) } ``` 2. Call the `Next()` method in a **separate** goroutine ```go var wg sync.WaitGroup  wg.Add(1) go func() { 	defer wg.Done()  	for { 		// Next() blocks until a message is available. 		// After calling Stop() keeps blocking until we get an ErrNoHeartbeat, 		// then next call will return ErrMsgIteratorClosed because of the 		// atomic.LoadUint32(&s.closed) == 1 check in Next().                 // If ReportMissingHeartbeats is false, then it will block indefinitely. 		msg, err := iter.Next()  		if err != nil { 			if errors.Is(err, jetstream.ErrMsgIteratorClosed) { 				fmt.Println(""Iterator closed"") 				break 			}  			fmt.Println(""Next err: "", err) 			// If Stop() was called then the next Next() call will return ErrMsgIteratorClosed. 			continue 		}  		fmt.Println(string(msg.Data())) 		msg.Ack() 	} }() ``` 3. Call the `Stop()` method to stop consuming messages ```go iter.Stop() // Should make Next() return ErrMsgIteratorClosed ```  In the [example](https://github.com/nats-io/nats.go/blob/main/examples/jetstream/js-messages/main.go) showing the usage of the new JetStream API with `Messages()`, this issue is not apparent because `Next()` is called in the **main** goroutine.  Full example: https://github.com/mdawar/jetstream-api-demos/tree/main/shutdown  #### Expected result: The `Stop()` method should make `Next()` return an error (Possibly `ErrMsgIteratorClosed`).  #### Actual result: There are 2 results depending on the configuration:  ##### 1. `ReportMissingHeartbeats` is `true` (default)  After calling `Stop()` consuming messages stops (the `done` channel is closed, and `pullMessages` stops fetching messages) but `Next()` keeps on blocking until there's an error `ErrNoHeartbeat` then exiting depends on breaking the loop or returning after the error.  ##### 2. `ReportMissingHeartbeats` is `false`  ```go cons.Messages(jetstream.WithMessagesErrOnMissingHeartbeat(false)) ```  After calling `Stop()` the `Next()` call will block indefinitely (Process must be killed: `kill -9 PID`).",2023-07-10T11:22:31+00:00,2023-07-15T21:57:19+00:00,0,https://github.com/nats-io/nats.go/issues/1343,1345.0,2023-07-15T21:57:17+00:00,https://github.com/nats-io/nats.go/pull/1345,0,1,1,2,40,3,0,3,130.57944444444445,bug,False,True,normal,configuration,"[{""filename"": ""jetstream/README.md"", ""lines_added"": 38, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""jetstream/pull.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
permitio/opal,721,debug,Ignore   just debugging... script not working properly locally.... ,2024-12-23T11:57:31+00:00,2025-02-02T15:16:43+00:00,2,https://github.com/permitio/opal/pull/721,721.0,,https://github.com/permitio/opal/pull/721,0,0,1,1,8,1,0,0,987.32,,False,True,normal,functional,"[{""filename"": ""app-tests/run.sh"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""other""}]",app-tests,False
permitio/opal,656,I'm getting tons of TLS config reloaded messages when OPAL_LOG_TO_FILE is true,"Following is my docker compose `permitio/opal-client:0.7.12` simplified settings  ```yaml services:   opal-client:     image: permitio/opal-client:0.7.12     environment:       - OPAL_LOG_TO_FILE=true       - OPAL_LOG_FILE_PATH=/opal/opal-client.log       - OPAL_INLINE_OPA_CONFIG={""log_level"":""info"",""tls_cert_file"":""/opal/opa-cert.crt"",""tls_private_key_file"":""/opal/opa-private.key"",""addr"":""http://localhost:8181,https://0.0.0.0:8282""}       - OPAL_INLINE_OPA_LOG_FORMAT=http     volumes:       - $ROOT_DIR/opa-private.key:/opal/opa-private.key       - $ROOT_DIR/opa-cert.crt:/opal/opa-cert.crt ```  As long as `OPAL_LOG_TO_FILE=false` everything is fine. Once I change to `OPAL_LOG_TO_FILE=true` I'm getting a lot of TLS messages  ``` opal-client-1  | 2024-09-16T08:12:54.952078+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.952322+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.952561+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.952802+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.953033+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.953305+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.953601+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.953928+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.954264+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.954594+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.954913+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.955241+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.955590+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} opal-client-1  | 2024-09-16T08:12:54.955939+0000 | 25 | opal_client.engine.logger               | INFO  | TLS config reloaded  {""cert-file"": ""/opal/opa-cert.crt"", ""cert-key-file"": ""/opal/opa-private.key"", ""time"": ""2024-09-16T08:12:54Z""} ```",2024-09-16T08:14:45+00:00,2024-12-02T16:56:29+00:00,2,https://github.com/permitio/opal/issues/656,657.0,2024-12-02T16:56:28+00:00,https://github.com/permitio/opal/pull/657,0,1,0,1,3,0,0,3,1856.6952777777776,bug,False,True,normal,configuration,"[{""filename"": ""packages/opal-client/opal_client/engine/options.py"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4407,Error when using Eureka as a server and client at the same time and setting the parameter eureka.client.refresh.enable=false,"When using Eureka as a server and client at the same time and setting eureka.client.refresh.enable=false, I get an error message related to cyclic dependency.  Error log: ``` The dependencies of some of the beans in the application context form a cycle:  ┌─────┐ |  org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration (field private com.netflix.appinfo.ApplicationInfoManager org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration.applicationInfoManager) ↑     ↓ |  org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration$EurekaClientConfiguration (field private com.netflix.discovery.AbstractDiscoveryClientOptionalArgs org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration$EurekaClientConfiguration.optionalArgs) └─────┘ ```  build.gradle:  `implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-server:4.2.0'`  application.yml: ``` eureka:   instance:     hostname: ${HOSTNAME:localhost}     instance-id: ${INSTANCE_ID:${random.uuid}}   client:     register-with-eureka: true     fetch-registry: true     service-url:       defaultZone: ${CONFIG_SERVERS:http://${eureka.instance.hostname}:${server.port}/eureka/}     refresh:       enable: false   ``` ",2025-02-15T16:14:12+00:00,2025-03-18T14:00:24+00:00,1,https://github.com/spring-cloud/spring-cloud-netflix/issues/4407,1576.0,,https://github.com/spring-cloud/spring-cloud-netflix/pull/1576,0,3,0,3,16,4,0,20,741.77,bug,False,True,normal,configuration,"[{""filename"": ""spring-cloud-netflix-core/src/main/java/org/springframework/cloud/netflix/zuul/filters/RouteLocator.java"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-core/src/main/java/org/springframework/cloud/netflix/zuul/filters/SimpleRouteLocator.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-core/src/main/java/org/springframework/cloud/netflix/zuul/web/ZuulHandlerMapping.java"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",web,False
nats-io/nats.go,851,Test that shows large double put bug for objectstore.,Upgrade server to pass test.  Signed-off-by: Derek Collison <derek@nats.io>,2021-10-14T18:30:13+00:00,2021-10-14T18:54:19+00:00,2,https://github.com/nats-io/nats.go/pull/851,851.0,2021-10-14T18:54:19+00:00,https://github.com/nats-io/nats.go/pull/851,0,1,2,3,24,3,0,21,0.4016666666666666,,False,True,normal,functional,"[{""filename"": ""go_test.mod"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""go_test.sum"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""test/norace_test.go"", ""lines_added"": 21, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,6157,Do not send RST_STREAM when UnprocessedRequestException is raised,"Motivation: When an `UnprocessedRequestException` is raised, the client should not send an `RST_STREAM`.  Modifications: - Updated the `Http2ResponseDecoder` to avoid sending `RST_STREAM` when `UnprocessedRequestException` occurs.  Result: - The client does not send an `RST_STREAM` when an `UnprocessedRequestException` is raised.",2025-03-12T02:24:15+00:00,2025-03-12T07:54:59+00:00,1,https://github.com/line/armeria/pull/6157,6157.0,2025-03-12T07:54:59+00:00,https://github.com/line/armeria/pull/6157,0,2,0,2,74,1,0,75,5.512222222222222,defect,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/Http2ResponseDecoder.java"", ""lines_added"": 10, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/HeaderListSizeExceptionTest.java"", ""lines_added"": 64, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,6107,Include `ConnectTimeoutException` in timeout exceptions for rules,"Motivation:  `CircuitBreakerRuleBuilder.onTimeException()` and `RetryRuleBuilder.onTimeoutException()` only handle Armeria's `com.linecorp.armeria.common.TimeoutException`. However, they do not treat Netty`s `ConnectTimeoutException` as a timeout.  Modifications:  - Treat Netty's `ConnectTimeoutException` and `ProxyConnectException(""timeout"")` as a timeout for rules.  Result:  `CircuitBreakerRuleBuilder.onTimeException()` and `RetryRuleBuilder.onTimeoutException()` now work when a connection or proxy establishment is timed out. ",2025-02-18T08:23:28+00:00,2025-03-04T10:23:12+00:00,1,https://github.com/line/armeria/pull/6107,6107.0,2025-03-04T10:23:12+00:00,https://github.com/line/armeria/pull/6107,0,4,0,4,120,4,0,124,337.99555555555554,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/AbstractRuleBuilder.java"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/ClientRequestContext.java"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/TimeoutExceptionPredicate.java"", ""lines_added"": 66, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/TimeoutExceptionPredicateTest.java"", ""lines_added"": 52, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,6003,Fix assertion exception in DefaultDnsResolver,``` java.lang.AssertionError: null 	at com.linecorp.armeria.internal.client.dns.DefaultDnsResolver.resolveOne(DefaultDnsResolver.java:88) 	at com.linecorp.armeria.internal.client.dns.DefaultDnsResolver.resolve(DefaultDnsResolver.java:81) 	at com.linecorp.armeria.client.RefreshingAddressResolver.sendQueries(RefreshingAddressResolver.java:161) 	at com.linecorp.armeria.client.RefreshingAddressResolver.access$500(RefreshingAddressResolver.java:56) 	at com.linecorp.armeria.client.RefreshingAddressResolver$CacheEntry.refresh(RefreshingAddressResolver.java:356) 	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173) 	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166) 	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) 	at io.netty.channel.kqueue.KQueueEventLoop.run(KQueueEventLoop.java:300) 	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:994) 	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 	at java.base/java.lang.Thread.run(Thread.java:829) ```,2024-11-26T08:26:18+00:00,2025-02-21T08:10:44+00:00,1,https://github.com/line/armeria/issues/6003,6092.0,2025-02-21T05:18:33+00:00,https://github.com/line/armeria/pull/6092,0,1,0,1,27,15,0,42,2084.8708333333334,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/RefreshingAddressResolver.java"", ""lines_added"": 27, ""lines_deleted"": 15, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4291,Encode url path segments & query parameters,"Rebased upon main, added author tag and edited date strings in licenses New PR of the old https://github.com/spring-cloud/spring-cloud-netflix/pull/4163  - Extend test suite with instanceId value that requires encoding - Fix url encoding in both RestTemplate & WebClient  ",2024-07-15T19:55:13+00:00,2024-07-16T09:34:14+00:00,1,https://github.com/spring-cloud/spring-cloud-netflix/pull/4291,4291.0,2024-07-16T09:34:14+00:00,https://github.com/spring-cloud/spring-cloud-netflix/pull/4291,0,3,0,3,95,72,0,167,13.650277777777776,bug,False,True,normal,database,"[{""filename"": ""spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/http/RestTemplateEurekaHttpClient.java"", ""lines_added"": 41, ""lines_deleted"": 27, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/http/WebClientEurekaHttpClient.java"", ""lines_added"": 35, ""lines_deleted"": 33, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-eureka-client/src/test/java/org/springframework/cloud/netflix/eureka/http/AbstractEurekaHttpClientTests.java"", ""lines_added"": 19, ""lines_deleted"": 12, ""file_type"": ""app_code""}]",,False
line/armeria,6046,Fix a bug where a tailing dot is not stripped for SNI,"Motivation:  A trailing dot specified in a hostname can be used to make DNS queries to avoid using search domains. However, the trailing dot should be removed for SNI.  https://datatracker.ietf.org/doc/html/rfc6066#section-3 > The hostname is represented as a byte string using ASCII encoding > without a trailing dot.  Modifications:  - Add `Endpoint.withTrailingDot()` to remove a trailing dot if present.  - Use an `Endpoint` without a trailing dot to create a remote address which is used for SNI.  Result:  - Fixed a bug where a trailing dot was included in the hostname used by SNI. - Closes #6044  ",2024-12-23T05:27:38+00:00,2025-01-02T01:26:23+00:00,1,https://github.com/line/armeria/pull/6046,6046.0,2025-01-02T01:26:23+00:00,https://github.com/line/armeria/pull/6046,0,4,0,4,102,1,0,103,235.97916666666663,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/Endpoint.java"", ""lines_added"": 16, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/HttpChannelPool.java"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/EndpointTest.java"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/TrailingDotSniTest.java"", ""lines_added"": 76, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
micronaut-projects/micronaut-core,11299,Improve displaying of circular dependency error,"For some classes `ClassA`, `ClassB`, `ClassC` and `ClassD` that have a dependency cycle, the current message is: ``` Failed to inject value for field [propA] of class: io.micronaut.inject.failures.ctorcirculardependency.MyClassB   Message: Circular dependency detected Path Taken:  new MyClassD(MyClassB propB) --> new MyClassD([MyClassB propB]) --> MyClassB.propA --> new MyClassA([MyClassC propC]) --> new MyClassC([MyClassB propB]) ^                                                                                                                                                     | |                                                                                                                                                     | |                                                                                                                                                     | +-----------------------------------------------------------------------------------------------------------------------------------------------------+ ```  After the change it would be: ``` Failed to inject value for field [propA] of class: io.micronaut.inject.failures.ctorcirculardependency.MyClassB   Message: Circular dependency detected Path Taken:  new MyClassD(MyClassB propB)       \\---> new MyClassD([MyClassB propB])             \\---> MyClassB.propA                   ^  \\---> new MyClassA([MyClassC propC])                   |        \\---> new MyClassC([MyClassB propB])                   |              |                   +--------------+ ```  This improves it by: * Showing the actual cycle, as the cycle does not always include the first element in the path. In this case it is `ClassB->ClassA->ClassC->ClassB`, so `ClassD` is ommitted * Displaying each entry on a new line to improve readability. This is important for classes with long class names and multiple injection parameters. I changed the test class names to longer, as `A`, `B`, `C` did not seem realistic.",2024-11-01T13:40:58+00:00,2024-11-04T18:12:52+00:00,5,https://github.com/micronaut-projects/micronaut-core/pull/11299,11299.0,2024-11-04T18:12:52+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11299,0,6,6,12,247,78,0,92,76.53166666666667,type: improvement,False,True,major,ui,"[{""filename"": ""inject-groovy/src/test/groovy/io/micronaut/inject/failures/ConstructorCircularDependencyFailureSpec.groovy"", ""lines_added"": 44, ""lines_deleted"": 14, ""file_type"": ""other""}, {""filename"": ""inject-groovy/src/test/groovy/io/micronaut/inject/failures/FactoryCircularDependencyFailureSpec.groovy"", ""lines_added"": 83, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""inject-groovy/src/test/groovy/io/micronaut/inject/failures/FieldCircularDependencyFailureSpec.groovy"", ""lines_added"": 7, ""lines_deleted"": 6, ""file_type"": ""other""}, {""filename"": ""inject-groovy/src/test/groovy/io/micronaut/inject/failures/PropertyCircularDependencyFailureSpec.groovy"", ""lines_added"": 11, ""lines_deleted"": 10, ""file_type"": ""other""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/ctorcirculardependency/ConstructorCircularDependencyFailureSpec.groovy"", ""lines_added"": 32, ""lines_deleted"": 13, ""file_type"": ""other""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/ctorcirculardependency/MyClassA.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/ctorcirculardependency/MyClassB.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/ctorcirculardependency/MyClassC.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/ctorcirculardependency/MyClassD.java"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""inject-java/src/test/groovy/io/micronaut/inject/failures/fieldcirculardependency/FieldCircularDependencyFailureSpec.groovy"", ""lines_added"": 7, ""lines_deleted"": 6, ""file_type"": ""other""}, {""filename"": ""inject/src/main/java/io/micronaut/context/AbstractBeanResolutionContext.java"", ""lines_added"": 39, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""inject/src/main/java/io/micronaut/context/exceptions/MessageUtils.java"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
permitio/opal,420,Health check: Fix error in opal-client's health check (Enable tracking OPA transactions without persisting),"Currently `opal-client`'s http health check endpoint raises an error whenever `OPAL_OPA_HEALTH_CHECK_POLICY_ENABLED=false`.   `OPA_HEALTH_CHECK_POLICY` is the name for the feature that makes `opal-client` configure OPA with a policy that reflects its own health status. We want to make `opal-client`'s health check always available, regardless of the flag.  For that, tracking the status of transactions being made to OPA (thus knowing if opal is actually updating OPA's state) should be separated from storing/persisting that state to OPA as a policy (which should only happen when setting `OPAL_OPA_HEALTH_CHECK_POLICY_ENABLED=true`. )",2023-03-19T16:03:22+00:00,2023-03-27T17:29:31+00:00,1,https://github.com/permitio/opal/pull/420,420.0,2023-03-27T17:29:31+00:00,https://github.com/permitio/opal/pull/420,0,5,1,6,132,104,0,216,193.4358333333333,,False,True,normal,configuration,"[{""filename"": ""documentation/docs/tutorials/monitoring_opal.mdx"", ""lines_added"": 18, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""packages/opal-client/opal_client/client.py"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-client/opal_client/policy_store/base_policy_store_client.py"", ""lines_added"": 6, ""lines_deleted"": 21, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-client/opal_client/policy_store/mock_policy_store_client.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-client/opal_client/policy_store/opa_client.py"", ""lines_added"": 99, ""lines_deleted"": 76, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-client/opal_client/policy_store/policy_store_client_factory.py"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
line/armeria,6014,Fix Bug Where Weight Is Set to 0 in Ramping-Up Strategy,"Motivation: When the original weight is less than 10, the initial weight is incorrectly set to 0, potentially leading to `EndpointSelectionTimeoutException`.  Modifications: - Ensured a minimum weight of 1 is set when the original weight is greater than 1 in the ramping-up strategy. - Added debugging logs for selector and selection strategy to facilitate troubleshooting.  Result: - Fix a bug where weights could unintentionally be set to 0 in ramping-up strategies. ",2024-12-03T08:29:56+00:00,2024-12-03T12:03:14+00:00,1,https://github.com/line/armeria/pull/6014,6014.0,2024-12-03T12:03:14+00:00,https://github.com/line/armeria/pull/6014,0,11,0,11,206,16,0,222,3.555,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/DynamicEndpointGroup.java"", ""lines_added"": 34, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/RoundRobinStrategy.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/StickyEndpointSelectionStrategy.java"", ""lines_added"": 8, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/WeightRampingUpStrategy.java"", ""lines_added"": 34, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/WeightRampingUpStrategyBuilder.java"", ""lines_added"": 11, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/WeightedRoundRobinStrategy.java"", ""lines_added"": 35, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/healthcheck/HealthCheckedEndpointGroup.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/common/metric/MeterIdPrefixFunction.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/client/endpoint/EndpointToStringUtil.java"", ""lines_added"": 61, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/client/endpoint/WeightedRandomDistributionSelector.java"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/endpoint/WeightRampingUpStrategyTest.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
line/armeria,5972,Fill the stack trace of `ResponseCompleteException` when sampled,"Motivation:  `ResponseCompleteException` is considered a safe exception for cleaning up request resources. However, it would be a good idea to leave a stack trace in case there is a bug in the implementation or the user wants to know how the error occurred.  Discord thread: https://discord.com/channels/1087271586832318494/1087272728177942629/1303562249629073520  Modifications:  - Create a new instance if `ResponseCompletionException` is sampled by `verboseExceptionSampler`  Result:  You can now sample the stack trace of `ResponseCompleteException` with `verboseExceptionSampler`.  ",2024-11-07T06:38:19+00:00,2024-11-28T05:44:07+00:00,1,https://github.com/line/armeria/pull/5972,5972.0,2024-11-28T05:44:07+00:00,https://github.com/line/armeria/pull/5972,0,1,0,1,9,3,0,12,503.0966666666667,defect,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/common/ResponseCompleteException.java"", ""lines_added"": 9, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
permitio/opal,384,fix bug in parsing and make tests test secrets,Signed-off-by: orweis <orweis@gmail.com> Closes #381 ,2023-02-12T12:26:59+00:00,2023-02-12T13:45:25+00:00,1,https://github.com/permitio/opal/pull/384,384.0,2023-02-12T13:45:25+00:00,https://github.com/permitio/opal/pull/384,0,2,0,2,82,56,0,138,1.3072222222222225,,False,True,normal,functional,"[{""filename"": ""packages/opal-server/opal_server/policy/webhook/deps.py"", ""lines_added"": 62, ""lines_deleted"": 47, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-server/opal_server/tests/policy_repo_webhook_test.py"", ""lines_added"": 20, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",packages,False
line/armeria,5980,debug log,for https://github.com/line/armeria/issues/5981,2024-11-14T21:59:49+00:00,2024-11-17T04:59:14+00:00,1,https://github.com/line/armeria/pull/5980,5980.0,,https://github.com/line/armeria/pull/5980,0,2,0,2,75,2,0,77,54.99027777777778,,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/common/logging/DefaultRequestLog.java"", ""lines_added"": 66, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/common/logging/RequestLog.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4258,Enable Connection Evictor in HttpClient (#4103),Fixes: #4103,2024-02-19T20:19:18+00:00,2024-03-26T14:29:18+00:00,2,https://github.com/spring-cloud/spring-cloud-netflix/pull/4258,4258.0,2024-03-26T14:29:18+00:00,https://github.com/spring-cloud/spring-cloud-netflix/pull/4258,0,2,1,3,65,8,0,62,858.1666666666666,bug,False,True,normal,networking,"[{""filename"": ""docs/modules/ROOT/pages/spring-cloud-netflix.adoc"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/RestTemplateTimeoutProperties.java"", ""lines_added"": 19, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-eureka-client/src/main/java/org/springframework/cloud/netflix/eureka/http/DefaultEurekaClientHttpRequestFactorySupplier.java"", ""lines_added"": 35, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
permitio/opal,339,Topic related DataUpdate bug fixes,fixes https://github.com/permitio/opal/issues/271  Also fixes redundant handlings of DataUpdate (which happened whenever more then one entry points to the same topic).,2022-12-08T13:51:03+00:00,2023-01-19T18:04:27+00:00,1,https://github.com/permitio/opal/pull/339,339.0,2023-01-19T18:04:27+00:00,https://github.com/permitio/opal/pull/339,0,3,0,3,19,10,0,29,1012.2233333333334,,False,True,normal,database,"[{""filename"": ""packages/opal-client/opal_client/data/updater.py"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-client/opal_client/tests/data_updater_test.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-server/opal_server/data/data_update_publisher.py"", ""lines_added"": 11, ""lines_deleted"": 8, ""file_type"": ""app_code""}]",packages,False
line/armeria,5724,Test failure: `com.linecorp.armeria.server.Http1ServerDelayedCloseConnectionTest.shouldWaitForDisconnectByClientSideFirst()`,``` org.opentest4j.AssertionFailedError:  expected: 1  but was: 2 	at java.base@21.0.3/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62) 	at java.base@21.0.3/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502) 	at app//com.linecorp.armeria.server.Http1ServerDelayedCloseConnectionTest.shouldWaitForDisconnectByClientSideFirst(Http1ServerDelayedCloseConnectionTest.java:112) 	at java.base@21.0.3/java.lang.reflect.Method.invoke(Method.java:580) 	at java.base@21.0.3/java.util.ArrayList.forEach(ArrayList.java:1596) 	at java.base@21.0.3/java.util.ArrayList.forEach(ArrayList.java:1596)  ``` ,2024-06-04T07:28:31+00:00,2024-10-18T01:23:11+00:00,0,https://github.com/line/armeria/issues/5724,5943.0,2024-10-18T01:23:09+00:00,https://github.com/line/armeria/pull/5943,0,3,0,3,12,5,0,17,3257.910555555556,,False,True,normal,networking,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/client/DefaultDnsCacheTest.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/common/stream/StreamMessageCollectingTest.java"", ""lines_added"": 5, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/server/Http1ServerDelayedCloseConnectionTest.java"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",armeria,False
permitio/opal,345,Bugfix/hanging redis lock,"This solves the issue of the Redis lock (around the policy git clone) staying hanging forever (preventing new workers from cloning the repo). Probably because the app crashes with segfault before releasing the lock.  My solution is giving the lock an expiration time (enforced by Redis itself), keeping extending this timeout while we wait for fetch to finish. When the app crashes, the lock would expire and not hang forever.  I got this idea from [this library](https://github.com/ionelmc/python-redis-lock) which I had used before to solve a similar issue. But that library isn't async - So I've implemented the same idea myself (Not that complicated anyway).",2022-12-20T09:34:39+00:00,2022-12-20T09:58:42+00:00,1,https://github.com/permitio/opal/pull/345,345.0,2022-12-20T09:58:42+00:00,https://github.com/permitio/opal/pull/345,0,2,0,2,42,9,0,51,0.4008333333333333,,False,True,critical,networking,"[{""filename"": ""packages/opal-common/opal_common/synchronization/expiring_redis_lock.py"", ""lines_added"": 39, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""packages/opal-server/opal_server/git_fetcher.py"", ""lines_added"": 3, ""lines_deleted"": 9, ""file_type"": ""app_code""}]",packages,False
line/armeria,5743,Test failure: `com.linecorp.armeria.server.InitiateConnectionShutdownTest.[2] path=/goaway_blocking?duration=500`,"``` org.mockito.exceptions.verification.VerificationInOrderFailure:  Verification in order failure Wanted but not invoked: clientListener.onGoAwayRead(     <any io.netty.channel.ChannelHandlerContext>,     3,     0L,     UnreleasableByteBuf(UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeHeapByteBuf(ridx: 0, widx: 13, cap: 64)) ); -> at com.linecorp.armeria.server.InitiateConnectionShutdownTest.initiateConnectionShutdownCloseBeforeDrainEndHttp2(InitiateConnectionShutdownTest.java:241) Wanted anywhere AFTER following interaction: clientListener.onDataRead(     ChannelHandlerContext(Http2ConnectionHandler#0, [id: 0x5ad6cb47, L:/127.0.0.1:36914 ! R:127.0.0.1/127.0.0.1:35817]),     3,     UnpooledSlicedByteBuf(freed),     0,     true ); -> at io.netty.handler.codec.http2.Http2FrameListenerDecorator.onDataRead(Http2FrameListenerDecorator.java:34)  	at app//com.linecorp.armeria.server.InitiateConnectionShutdownTest.initiateConnectionShutdownCloseBeforeDrainEndHttp2(InitiateConnectionShutdownTest.java:241) 	at java.base@17.0.10/java.lang.reflect.Method.invoke(Method.java:568) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) 	at java.base@17.0.10/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 	at java.base@17.0.10/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) 	at java.base@17.0.10/java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:276) 	at java.base@17.0.10/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150) 	at java.base@17.0.10/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173) 	at java.base@17.0.10/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) 	at java.base@17.0.10/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596) 	at java.base@17.0.10/java.util.ArrayList.forEach(ArrayList.java:1511) 	at java.base@17.0.10/java.util.ArrayList.forEach(ArrayList.java:1511)  ``` ",2024-06-10T08:34:19+00:00,2024-10-18T01:22:52+00:00,1,https://github.com/line/armeria/issues/5743,5944.0,2024-10-18T01:22:51+00:00,https://github.com/line/armeria/pull/5944,0,1,0,1,1,0,0,1,3112.8088888888888,,False,True,normal,networking,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/server/InitiateConnectionShutdownTest.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",armeria,False
line/armeria,3982,Test failure: InitiateConnectionShutdownTest,"``` InitiateConnectionShutdownTest > initiateConnectionShutdownCloseBeforeDrainEndHttp2(String) > com.linecorp.armeria.server.InitiateConnectionShutdownTest.initiateConnectionShutdownCloseBeforeDrainEndHttp2(String)[2] FAILED     org.mockito.exceptions.verification.VerificationInOrderFailure:      Verification in order failure     Wanted but not invoked:     clientListener.onGoAwayRead(         <any io.netty.channel.ChannelHandlerContext>,         3,         0L,         UnreleasableByteBuf(UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeHeapByteBuf(ridx: 0, widx: 13, cap: 64))     );     -> at com.linecorp.armeria.server.InitiateConnectionShutdownTest.initiateConnectionShutdownCloseBeforeDrainEndHttp2(InitiateConnectionShutdownTest.java:241)     Wanted anywhere AFTER following interaction:     clientListener.onDataRead(         ChannelHandlerContext(Http2ConnectionHandler#0, [id: 0x64b80f3b, L:/127.0.0.1:41702 ! R:127.0.0.1/127.0.0.1:41362]),         3,         AdvancedLeakAwareByteBuf(UnpooledSlicedByteBuf(ridx: 0, widx: 0, cap: 0/0, unwrapped: PooledUnsafeDirectByteBuf(ridx: 30, widx: 30, cap: 512))),         0,         true     );     -> at io.netty.handler.codec.http2.Http2FrameListenerDecorator.onDataRead(Http2FrameListenerDecorator.java:36)         at app//com.linecorp.armeria.server.InitiateConnectionShutdownTest.initiateConnectionShutdownCloseBeforeDrainEndHttp2(InitiateConnectionShutdownTest.java:241) ```",2021-12-09T01:59:15+00:00,2024-10-18T01:22:53+00:00,0,https://github.com/line/armeria/issues/3982,5944.0,2024-10-18T01:22:51+00:00,https://github.com/line/armeria/pull/5944,0,1,0,1,1,0,0,1,25055.393333333333,cleanup,False,True,normal,networking,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/server/InitiateConnectionShutdownTest.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",armeria,False
permitio/opal,188,fix bug where changes to different directories in the same commit are ignored,"This fixes #180  ### The root cause 1) when we publish a change to multiple topics at once - the pub/sub library will always publish to each topic separately.  in other words: `publish(topics=[""dir1"", ""dir2"", data=hash1]` will result in two messages sent by opal server: ``` message1 = {topic=""dir1"", data=hash1} message2 = {topic=""dir2"", data=hash1} ```  2) The client will only fetch dir1 when receiving the first message - but it will bump the latest known hash to hash1.  3) The client will do nothing for the second message, because it already thinks it has that hash (hash1).  ### Possible solutions There are two possible solutions (both of them are breaking changes - but not to users - only to the on-wire protocol). 1) publish the entire context as the message data - which is the solution i took here. 2) ignore the ""latest known hash"" in the policy store, and instead go with the ""latest known hash"" of the update message - this solution also requires us to do the same changes to the policy update message. However i tend to like it less because the ""diff"" to the policy store is now not atomic (i,e: all changes at once).",2021-11-20T19:18:44+00:00,2021-11-21T10:42:57+00:00,1,https://github.com/permitio/opal/pull/188,188.0,2021-11-21T10:42:57+00:00,https://github.com/permitio/opal/pull/188,0,3,0,3,39,17,0,56,15.403611111111111,,False,True,normal,database,"[{""filename"": ""opal_client/policy/updater.py"", ""lines_added"": 20, ""lines_deleted"": 14, ""file_type"": ""app_code""}, {""filename"": ""opal_common/schemas/policy.py"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""opal_server/policy/watcher/callbacks.py"", ""lines_added"": 13, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",opal_server,False
line/armeria,5896,Fix a bug where the parent's timeout is not cancelled in `RetryingClient`,"Motivation:  Originally, when a request is derived, the parent's timeout scheduler is not canceled. This was not a problem until the `ResponseTimeoutMode.FROM_START` feature was added because the timeout did not start at this point.   When `ResponseTimeoutMode.FROM_START` is set, a timeout task is scheduled before it is derived. So it needs to be canceled. Otherwise, the uncanceled scheduler task may cause a leak because referenced objects are not GC'd until the task is completed.  Modifications:  - Cancel the timeout scheduler of the parent request when it is derived.  Result:  Fixed a bug where timeout tasks leak when using `ResponseTimeoutMode.FROM_START` with `RetryingClient`. ",2024-09-09T07:30:40+00:00,2024-09-10T06:54:34+00:00,1,https://github.com/line/armeria/pull/5896,5896.0,2024-09-10T06:54:34+00:00,https://github.com/line/armeria/pull/5896,0,5,0,5,119,0,0,119,23.398333333333333,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/internal/client/DefaultClientRequestContext.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/CancellationScheduler.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/DefaultCancellationScheduler.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/NoopCancellationScheduler.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/retry/RetryTimeoutCancellationTest.java"", ""lines_added"": 102, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,5868,Test failure: `com.linecorp.armeria.client.ContextCancellationTest.cancel_beforeWriteFinished(TestInfo)`,``` java.lang.AssertionError:  Expecting empty but was: [[creqId=5d73bf85][http://127.0.0.1:45623/#GET]] 	at com.linecorp.armeria.client.ContextCancellationTest.validateCallbackChecks(ContextCancellationTest.java:289) 	at com.linecorp.armeria.client.ContextCancellationTest.cancel_beforeWriteFinished(ContextCancellationTest.java:259) 	at java.base/java.lang.reflect.Method.invoke(Method.java:580) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)  ``` ,2024-08-12T07:40:07+00:00,2024-08-13T02:12:08+00:00,2,https://github.com/line/armeria/issues/5868,5869.0,2024-08-12T11:27:37+00:00,https://github.com/line/armeria/pull/5869,0,1,0,1,3,14,0,17,3.7916666666666665,cleanup,False,True,normal,functional,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/client/ContextCancellationTest.java"", ""lines_added"": 3, ""lines_deleted"": 14, ""file_type"": ""app_code""}]",,False
line/armeria,5866,Test failure: `com.linecorp.armeria.common.stream.StreamMessageCollectingTest.filteredStreamMessage_exception()`,``` org.opentest4j.AssertionFailedError:  expected: 0  but was: 1 	at app//com.linecorp.armeria.common.stream.StreamMessageCollectingTest.assertRefCount(StreamMessageCollectingTest.java:390) 	at app//com.linecorp.armeria.common.stream.StreamMessageCollectingTest.filteredStreamMessage_exception(StreamMessageCollectingTest.java:168) 	at java.base@21.0.3/java.lang.reflect.Method.invoke(Method.java:580) 	at java.base@21.0.3/java.util.ArrayList.forEach(ArrayList.java:1596) 	at java.base@21.0.3/java.util.ArrayList.forEach(ArrayList.java:1596)  ``` ,2024-08-12T01:30:48+00:00,2024-08-12T04:43:54+00:00,1,https://github.com/line/armeria/issues/5866,2841.0,2020-06-26T08:29:48+00:00,https://github.com/line/armeria/pull/2841,0,2,0,2,12,0,0,12,-36185.01666666667,cleanup,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/circuitbreaker/CircuitBreakerMetrics.java"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/circuitbreaker/MetricCollectingCircuitBreakerListenerTest.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
micronaut-projects/micronaut-core,11070,Allow to handle all possible controller errors,"Including errors like incorrect HTTP method, content type etc. Right now it's only possible to catch this kind of error using a status route per status.  This is needed to align with JAX-RS where the exception handler can handle all the exceptions.",2024-08-12T09:15:48+00:00,2024-08-13T12:52:45+00:00,4,https://github.com/micronaut-projects/micronaut-core/pull/11070,11070.0,,https://github.com/micronaut-projects/micronaut-core/pull/11070,0,3,1,4,303,72,0,365,27.61583333333333,type: improvement,False,True,normal,functional,"[{""filename"": ""http-server-netty/src/test/groovy/io/micronaut/http/server/netty/cors/NettyCorsSpec.groovy"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""http-server-tck/src/main/java/io/micronaut/http/server/tck/tests/ErrorNotFoundRouteErrorRouteTest.java"", ""lines_added"": 80, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""http-server-tck/src/main/java/io/micronaut/http/server/tck/tests/ErrorNotFoundRouteExceptionHandlerTest.java"", ""lines_added"": 84, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""http-server/src/main/java/io/micronaut/http/server/RequestLifecycle.java"", ""lines_added"": 131, ""lines_deleted"": 70, ""file_type"": ""app_code""}]",http-server;http-server-netty;http-server-tck,True
micronaut-projects/micronaut-core,11077,don’t capture such a generic exception as RuntimeException,This pull request rewrites the test to avoid capturing such a generic exception as `RuntimeException`,2024-08-13T07:14:54+00:00,2024-08-13T07:51:58+00:00,0,https://github.com/micronaut-projects/micronaut-core/pull/11077,11077.0,2024-08-13T07:51:58+00:00,https://github.com/micronaut-projects/micronaut-core/pull/11077,0,0,1,1,8,2,0,0,0.6177777777777778,,False,True,normal,functional,"[{""filename"": ""http-server-netty/src/test/groovy/io/micronaut/http/server/netty/cors/NettyCorsSpec.groovy"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""other""}]",http-server-netty,False
micrometer-metrics/micrometer,5818,`Log4j2Metrics` creates more `MetricsFilter` instances than needed,"Before, we were needlessly making multiple instances of `MetricsFilter` that were functionally equivalent. This reduces the `MetricsFilter` instances to one per registry to which a `Log4j2Metrics` instance is bound. This will reduce allocations and memory usage slightly.",2025-01-23T07:56:57+00:00,2025-01-23T08:11:28+00:00,4,https://github.com/micrometer-metrics/micrometer/pull/5818,5818.0,2025-01-23T08:11:28+00:00,https://github.com/micrometer-metrics/micrometer/pull/5818,0,2,0,2,61,14,0,75,0.2419444444444444,bug;performance;module: micrometer-core;instrumentation,False,True,normal,ui,"[{""filename"": ""micrometer-core/src/main/java/io/micrometer/core/instrument/binder/logging/Log4j2Metrics.java"", ""lines_added"": 16, ""lines_deleted"": 14, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/test/java/io/micrometer/core/instrument/binder/logging/Log4j2MetricsTest.java"", ""lines_added"": 45, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,5749,Test failure: `com.linecorp.armeria.xds.client.endpoint.RampingUpTest.checkEndpointsAreRampedUp()`,"``` java.lang.AssertionError:  Expecting actual:   [null,     Endpoint{a.com:80, weight=1, attributes=[com.linecorp.armeria.xds.client.endpoint.XdsAttributeKeys#LOCALITY_LB_ENDPOINTS_KEY=locality { } lb_endpoints {   endpoint {     address {       socket_address {         address: ""a.com""         port_value: 80       }     }   }   health_status: HEALTHY   metadata {   }   load_balancing_weight {     value: 10   } } lb_endpoints {   endpoint {     address {       socket_address {         address: ""b.com""         port_value: 80       }     }   }   health_status: HEALTHY   metadata {   }   load_balancing_weight {     value: 10   } } , com.linecorp.armeria.internal.client.endpoint.RampingUpKeys#createdAtNanos=31975871750669554, com.linecorp.armeria.xds.client.endpoint.XdsAttributeKeys#LB_ENDPOINT_KEY=endpoint {   address {     socket_address {       address: ""a.com""       port_value: 80     }   } } health_status: HEALTHY metadata { } load_balancing_weight {   value: 10 } ]},     Endpoint{b.com:80, weight=1, attributes=[com.linecorp.armeria.xds.client.endpoint.XdsAttributeKeys#LOCALITY_LB_ENDPOINTS_KEY=locality { } lb_endpoints {   endpoint {     address {       socket_address {         address: ""a.com""         port_value: 80       }     }   }   health_status: HEALTHY   metadata {   }   load_balancing_weight {     value: 10   } } lb_endpoints {   endpoint {     address {       socket_address {         address: ""b.com""         port_value: 80       }     }   }   health_status: HEALTHY   metadata {   }   load_balancing_weight {     value: 10   } } , com.linecorp.armeria.internal.client.endpoint.RampingUpKeys#createdAtNanos=31975871750669554, com.linecorp.armeria.xds.client.endpoint.XdsAttributeKeys#LB_ENDPOINT_KEY=endpoint {   address {     socket_address {       address: ""b.com""       port_value: 80     }   } } health_status: HEALTHY metadata { } load_balancing_weight {   value: 10 } ]}] to contain exactly in any order:   [Endpoint{a.com:80, weight=1000}, Endpoint{b.com:80, weight=1000}] but the following elements were unexpected:   [null]  	at com.linecorp.armeria.xds.client.endpoint.RampingUpTest.checkEndpointsAreRampedUp(RampingUpTest.java:137) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at java.util.ArrayList.forEach(ArrayList.java:1259) 	at java.util.ArrayList.forEach(ArrayList.java:1259)  ``` ",2024-06-11T12:04:06+00:00,2024-07-05T04:01:51+00:00,2,https://github.com/line/armeria/issues/5749,5799.0,2024-07-05T03:58:46+00:00,https://github.com/line/armeria/pull/5799,0,4,0,4,43,28,0,71,567.9111111111112,,False,True,normal,database,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/AbstractEndpointSelector.java"", ""lines_added"": 8, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/WeightRampingUpStrategy.java"", ""lines_added"": 28, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/client/endpoint/WeightedRoundRobinStrategy.java"", ""lines_added"": 1, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/util/ReentrantShortLock.java"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
spec-first/connexion,1829,Connexion3 Failed to load API definition in the Swagger UI,"### Description It works perfectly fine locally and it works fine during some time after deployment. But after some time returns 500 error code. API itself works fine and all end-points works fine. The issue with Swagger UI only.  Before we added an exception handler we had just 500 error with the default message saying that something went wrong or app overloaded. Now we use the following error_handler: ``` def connexion_base_exception_handler(     request: ConnexionRequest, exc: Exception ) -> ConnexionResponse:     return ConnexionResponse(         status_code=500,         body=json.dumps({""error"": str(exc)}),         content_type=""application/json"",     ) ``` As a result, str(exc) is actually openapi.json content  <img width=""1496"" alt=""Screenshot 2023-12-06 at 09 36 49"" src=""https://github.com/spec-first/connexion/assets/40685761/d03f3bd6-fd80-46f6-a017-0122d422b54c"">  <img width=""974"" alt=""image"" src=""https://github.com/spec-first/connexion/assets/40685761/216f43d9-8a20-4980-822a-3ab6cf5d1b5a"">  I've found the follwoing message in logs but we do not use ""oneOf"":  `Failed validating 'oneOf' in schema['properties']['paths']['patternProperties']['^\\\\/']['patternProperties']['^(get|put|post|delete|options|head|patch|trace)$']['properties']['requestBody']:  +  ![image](https://github.com/spec-first/connexion/assets/40685761/5cbfa2d8-3e92-4949-b45b-bb5ecd3a71b7)  ` ### Expected behaviour API definition loaded correctly   ### Additional info:  Output of the commands:  - `python --version`  python:3.10 - `pip show connexion | grep ""^Version\\:""`. connexion[flask]==3.0.3 -  `starlette`. 0.33.0 or 0.32.0.post1  openapi.yaml: openapi: 3.0.0",2023-12-06T14:33:51+00:00,2023-12-07T13:54:42+00:00,8,https://github.com/spec-first/connexion/issues/1829,1830.0,2023-12-07T13:54:41+00:00,https://github.com/spec-first/connexion/pull/1830,0,3,0,3,17,1,0,18,23.34722222222222,,False,True,normal,configuration,"[{""filename"": ""connexion/spec.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/api/conftest.py"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/api/test_swagger_ui.py"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",tests,False
spec-first/connexion,2017,Response validation enforces wrong type when paths have common prefix and parameter,"### Description  Connexion v3 may be mis-matching paths if prefixes overlap?  In my case, it chooses the wrong response type, so validation fails.  My openAPI spec has paths `/user/{uid}` and `/user/all_roles`. The response type for `/user/{uid}` is an object with field `uid`, the response type for `/user/all_roles` is a list of objects, each with field `name`.  The `openapi-spec-validator` has no complaints about the spec. This worked fine in Connexion v2.  When running tox tests, I set Connexion app property `validate_responses` to True.    The test sends a request to `/user/all_roles`, Connexion returns status code 500 and internally raises an exception.  Here's a log excerpt, I hope it is reasonably clear what's going on: ``` DEBUG    my.controller:controller.py:680 response [{'name': 'role name'}] WARNING  connexion.validators.json:json.py:133 Validation error: [{'name': 'role name'}] is not of type 'object' ERROR    connexion.middleware.exceptions:exceptions.py:79 NonConformingResponseBody(status_code=500, detail=""Response body does not conform to specification. [{'name': 'role name'}] is not of type 'object'"") ```  Trying to figure this out, I changed the role endpoint response to an object (not a list), and that gave me a further clue: ``` DEBUG    my.controller:controller.py:680 response {'name': 'role name'} WARNING  connexion.validators.json:json.py:133 Validation error: 'uid' is a required property ERROR    connexion.middleware.exceptions:exceptions.py:79 NonConformingResponseBody(status_code=500, detail=""Response body does not conform to specification. 'uid' is a required property"") ```  The `uid` field is defined for a user, but not for a role.  So I'm guessing that apparently Connexion picked the response type for endpoint `/usr/{uid}` instead, possibly because of some bad path matching due to the common prefix and the path parameter?  ### Expected behaviour  Connexion matches the request path appropriately and uses the response type for `/user/all_roles` when responding toa request to that.  ### Actual behaviour  Connexion seems to feel that `/user/all_roles` matches the path pattern `/user/{uid}`, gets the response type for that endpoint, and validates the response against it.  ### Steps to reproduce  I have not yet worked up a SSCCE.  ### Additional info:  Python version 3.12 Connexion version 3.1.0  I searched the issues looking for a similar complaint to see if something was fixed, but found nothing.  I can workaround this by changing the paths so they don't share a prefix, for example changing the roles endpoint to `/user_all_roles` or something like that.",2024-12-12T16:06:04+00:00,2024-12-23T16:02:16+00:00,4,https://github.com/spec-first/connexion/issues/2017,902.0,2019-03-15T12:24:05+00:00,https://github.com/spec-first/connexion/pull/902,0,2,0,2,2,2,0,4,-50379.69972222222,,False,True,normal,configuration,"[{""filename"": ""connexion/utils.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""setup.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4105,Codacy badge is broken. Remove it or fix it.,"**Describe the bug** The codacy badge is broken and not showing correctly and opening it leads to a page that says ""Request Handler not found"" See the image below. ![image](https://user-images.githubusercontent.com/54680617/178657854-6dec977a-2441-4d84-a6e8-19fe741c74a3.png) ",2022-07-13T05:30:25+00:00,2022-09-01T13:59:51+00:00,0,https://github.com/spring-cloud/spring-cloud-netflix/issues/4105,1343.0,2016-09-15T14:15:42+00:00,https://github.com/spring-cloud/spring-cloud-netflix/pull/1343,0,2,0,2,22,0,0,22,-51039.24527777778,task,False,True,critical,functional,"[{""filename"": ""spring-cloud-netflix-core/src/main/java/org/springframework/cloud/netflix/rx/SingleReturnValueHandler.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-core/src/test/java/org/springframework/cloud/netflix/rx/SingleReturnValueHandlerTest.java"", ""lines_added"": 18, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
spec-first/connexion,1909,Splitting spec files still throws schema and file not found errors (json_schema exceptions) but still works with Connexion <= 3.0.2,"I have my yml files divided into subdirectories for paths, parameters, schemas, etc. This worked fine (and still works fine) on Connexion ~~3.0.4~~ 3.0.2. Connexion ~~3.0.5 and 3.0.6~~ 3.0.3 - 3.0.6 are still throwing errors. The prior fixes for $refs don't seem to have fixed issues. I thought this was just a Windows filesystem or jsonschema issue but it's happening all over - in my MacOS, Windows and docker container deployments running Debian.   *************** **If I downgrade to Connexion ~~3.0.4~~ 3.0.2 or prior, the error goes away, in all cases.** **************  I am working with a separate swagger UI deployment, so I can do customizations.  In my configuration, I'm initializing my app like this:  ``` basedir = pathlib.Path(__file__).parent.resolve() swagoptions = SwaggerUIOptions(swagger_ui = True, swagger_ui_template_dir = basedir / 'swagger-ui') connex_app = FlaskApp(__name__, specification_dir=basedir / ""apispecs"") ```  Paths are like this to specs, where my swagger.yml contains $refs to the proper paths of each of the divided spec files: ``` project/       │── apispecs/                |── swagger.yml                |── parameters/                          |── _index.html                │── securitySchemas/                          |── _index.html                │── schemas/                          |── _index.html ```  swagger.yml sample:  ``` components:   schemas:     $ref: ""./schemas/_index.yml""   parameters:     $ref: ""./parameters/_index.yml""   securitySchemes:     $ref: ""./security/_index.yml"" ```  Full contents of error: ``` _RefResolutionError(_cause=FileNotFoundError(2, 'No such file or directory')) Traceback (most recent call last):   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 88, in _do_resolve     retrieved = deep_get(spec, path)                 ^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/utils.py"", line 112, in deep_get     return deep_get(obj[keys[0]], keys[1:])                     ~~~^^^^^^^^^ KeyError: 'schemas'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1102, in resolve_from_url     document = self.store[url]                ~~~~~~~~~~^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/_utils.py"", line 20, in __getitem__     return self.store[self.normalize(uri)]            ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^ KeyError: './schemas/_index.yml'  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1105, in resolve_from_url     document = self.resolve_remote(url)                ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1202, in resolve_remote     result = self.handlers[scheme](uri)              ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 41, in __call__     with open(filepath) as fh:          ^^^^^^^^^^^^^^ FileNotFoundError: [Errno 2] No such file or directory: '/schemas/_index.yml'  The above exception was the direct cause of the following exception:  Traceback (most recent call last):   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app     await app(scope, receive, sender)   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/starlette/routing.py"", line 72, in app     response = await func(request)                ^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/middleware/swagger_ui.py"", line 110, in _get_openapi_json     content=jsonifier.dumps(self._spec_for_prefix(request)),                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/middleware/swagger_ui.py"", line 73, in _spec_for_prefix     return self.specification.with_base_path(base_path).raw            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/spec.py"", line 209, in with_base_path     new_spec = self.clone()                ^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/spec.py"", line 199, in clone     return type(self)(copy.deepcopy(self._raw_spec))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/spec.py"", line 83, in __init__     self._spec = resolve_refs(raw_spec, base_uri=base_uri)                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 106, in resolve_refs     res = _do_resolve(spec)           ^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 100, in _do_resolve     node[k] = _do_resolve(v)               ^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 100, in _do_resolve     node[k] = _do_resolve(v)               ^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/connexion/json_schema.py"", line 96, in _do_resolve     with resolver.resolving(node[""$ref""]) as resolved:   File ""/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py"", line 137, in __enter__     return next(self.gen)            ^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1044, in resolving     url, resolved = self.resolve(ref)                     ^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1091, in resolve     return url, self._remote_cache(url)                 ^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/eharvey71/dev/integration-bridge/venv/lib/python3.12/site-packages/jsonschema/validators.py"", line 1107, in resolve_from_url     raise exceptions._RefResolutionError(exc) from exc jsonschema.exceptions._RefResolutionError: [Errno 2] No such file or directory: '/schemas/_index.yml' INFO:     127.0.0.1:55767 - ""GET /api/openapi.json HTTP/1.1"" 500 Internal Server Error ```  Output of the commands:  - Python is 3.12 ",2024-04-08T21:28:57+00:00,2024-11-27T22:21:50+00:00,7,https://github.com/spec-first/connexion/issues/1909,2002.0,2024-11-27T22:21:49+00:00,https://github.com/spec-first/connexion/pull/2002,0,2,0,2,2,2,0,4,5592.881111111111,,False,True,normal,configuration,"[{""filename"": ""connexion/middleware/swagger_ui.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""connexion/spec.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
spec-first/connexion,1926,Log handled errors to warning,Fixes #1925.  Changes proposed in this pull request:   - Log handled errors to `warning` instead of `error`.  - Log validation errors to `info` because the intent of the log lines in informational. The error is handled by raising a new error. ,2024-05-14T09:19:39+00:00,2024-05-27T09:21:04+00:00,2,https://github.com/spec-first/connexion/pull/1926,1935.0,2024-05-28T07:42:55+00:00,https://github.com/spec-first/connexion/pull/1935,0,1,0,1,0,1,0,1,334.3877777777778,,False,True,normal,functional,"[{""filename"": ""connexion/middleware/exceptions.py"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4091,spring-cloud-netflix-eureka-server-3.1.2 frontend's navbar display wrong.,"**Describe the bug** artifact and version: spring-cloud-netflix-eureka-server-3.1.2  Since use webjars npm version and update jquery and bootstrap's version, there is something wrong with the front page's navbar.  1. In wro.xml, jquery path is not right.   2. And because of update bootstrap's version to 5.1, some navbar's css class is not exists.  **Simple**  Just setup a basic eureka server, and take a look at http://localhost:8761 , and focus on navbar. ",2022-05-11T08:24:41+00:00,2022-09-01T11:15:35+00:00,2,https://github.com/spring-cloud/spring-cloud-netflix/issues/4091,4092.0,,https://github.com/spring-cloud/spring-cloud-netflix/pull/4092,1,1,2,4,17,20,2,4,2714.8483333333334,bug,False,True,normal,database,"[{""filename"": ""spring-cloud-netflix-eureka-server/src/main/resources/templates/eureka/header.ftlh"", ""lines_added"": 11, ""lines_deleted"": 16, ""file_type"": ""other""}, {""filename"": ""spring-cloud-netflix-eureka-server/src/main/wro/header.css"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""spring-cloud-netflix-eureka-server/src/main/wro/wro.xml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""spring-cloud-netflix-eureka-server/src/test/java/org/springframework/cloud/netflix/eureka/server/ApplicationDashboardPathTests.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",spring-cloud-netflix-eureka-server,False
spec-first/connexion,1925,Error Logging,"There are several places Connexion logs errors, for example:  * When exceptions are handled in [`problem_handler`](https://github.com/spec-first/connexion/blob/fcfe151e690c8d8864605ff94923c8b8ce834bfb/connexion/middleware/exceptions.py#L73), [`http_exception`](https://github.com/spec-first/connexion/blob/fcfe151e690c8d8864605ff94923c8b8ce834bfb/connexion/middleware/exceptions.py#L80) or [`common_error_handler`](https://github.com/spec-first/connexion/blob/fcfe151e690c8d8864605ff94923c8b8ce834bfb/connexion/middleware/exceptions.py#L93) in the [`ExceptionMiddleware`](https://connexion.readthedocs.io/en/latest/middleware.html) * When validation of the input/output data fails in [`connexion.validators.json`](https://github.com/spec-first/connexion/blob/fcfe151e690c8d8864605ff94923c8b8ce834bfb/connexion/validators/json.py#L71)  Connexion does a great job of handling exceptions by retuning a problem details with the appropriate status code to the user.  Now it's pretty common for operation teams to monitor exceptions in the logs and trigger alarms.  And here comes the issue:  Operations want to be alerted about everything that results in `5xx` status codes. That indicates that a situation couldn't be handled from a user perspective. Operations don't need to be alerted if the status code is `4xx`. Because those cases are gracefully handled.  I'd propose to  1. change [`ExceptionMiddleware`](https://connexion.readthedocs.io/en/latest/middleware.html) to log to `error` if the resulting status code is `5xx` and to `warning` for `4xx` and 2. remove the error log from [`connexion.validators.json`](https://github.com/spec-first/connexion/blob/fcfe151e690c8d8864605ff94923c8b8ce834bfb/connexion/validators/json.py#L71) since the `BadRequestProblem` raised is logged via the Exception middleware anyway.  I'm happy to contribute those changes. What do you think?",2024-05-13T16:30:30+00:00,2024-05-27T09:21:05+00:00,1,https://github.com/spec-first/connexion/issues/1925,1926.0,2024-05-27T09:21:04+00:00,https://github.com/spec-first/connexion/pull/1926,0,2,0,2,16,4,0,20,328.84277777777777,,False,True,normal,configuration,"[{""filename"": ""connexion/middleware/exceptions.py"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""connexion/validators/json.py"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
line/armeria,5290,Fix a bug where a connection may be not reused when using `RetryingClient`,"Motivation:  Armeria's `HttpChannelPool` is bound to an `EventLoop`. Different `EventLoop`s have different `HttpChannelPool`s. In other words, in order to reuse a connection for an `Endpoint`, the same `EventLoop` must be selected.  When creating a derived client in `RetryingClient`, a new endpoint is selected for each retry, but since the `EventLoop` of the parent is used as is. That causes the `Endpoint` can't use the existing connection pool for multiplexing and makes a new connection.  Modifications:  - Use `EventLoopScheduler` to return constant `EventLoop`s for the same endpoint. - Allow setting `EndpointGroup` in `ClientRequestContextBuilder` for testability.  Result:  - You no longer see a connection leak when using `RetryingClient` with `EndpointGroup`. ",2023-11-07T14:59:37+00:00,2023-11-08T08:06:46+00:00,6,https://github.com/line/armeria/pull/5290,5290.0,2023-11-08T08:06:46+00:00,https://github.com/line/armeria/pull/5290,0,3,0,3,245,2,0,247,17.11916666666667,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/internal/client/DefaultClientRequestContext.java"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/client/retry/RetryingClientEventLoopSchedulerTest.java"", ""lines_added"": 110, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/internal/client/DerivedClientRequestContextClientTest.java"", ""lines_added"": 127, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,5267,Deprecate `GoogleGrpcStatusFunction` and add `GoogleGrpcExceptionHandlerFunction`,Motivation:  `GrpcStatusFunction` has been deprecated in #5046.  Modifications:  - Add `GoogleGrpcExceptionHandlerFunctionUtil` so as to share the common logic between `GoogleGrpcStatusFunction` and `GoogleGrpcExceptionHandlerFunction` - Deprecate `GoogleGrpcStatusFunction`  Result:  - `GoogleGrpcStatusFunction` has been deprecated in favor of `GoogleGrpcExceptionHandlerFunction`.  ,2023-10-25T04:01:24+00:00,2023-10-25T07:06:50+00:00,2,https://github.com/line/armeria/pull/5267,5267.0,2023-10-25T07:06:50+00:00,https://github.com/line/armeria/pull/5267,0,4,0,4,127,22,0,149,3.0905555555555555,deprecation,False,True,normal,functional,"[{""filename"": ""grpc/src/main/java/com/linecorp/armeria/common/grpc/GoogleGrpcExceptionHandlerFunction.java"", ""lines_added"": 56, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""grpc/src/main/java/com/linecorp/armeria/common/grpc/GoogleGrpcExceptionHandlerFunctionUtil.java"", ""lines_added"": 63, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""grpc/src/main/java/com/linecorp/armeria/common/grpc/GoogleGrpcStatusFunction.java"", ""lines_added"": 5, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""grpc/src/test/java/com/linecorp/armeria/common/grpc/GoogleGrpcExceptionHandlerTest.java"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
micrometer-metrics/micrometer,5688,Warn about gauge re-registration,"See https://github.com/micrometer-metrics/micrometer/issues/5616 and https://github.com/micrometer-metrics/micrometer/pull/5617  I added the warning log entries and benchmarks since the extra logging is on the hot path. The benchmarks don't test the case where the `Gauge` re-registration happens indirectly (as the result of a `MeterFilter` transformation) but since the direct and the indirect cases call the same method, I think testing only the direct case is ok.  I ran the benchmarks a couple of times but they were not as stable on my laptop as I would have liked, the difference between them was sometimes bigger between two identical executions (e.g. without logging) than between the two different ones (with vs. without logging).  So based on this either the benchmarks are wrong or the ""noise"" is bigger than the difference between the two scenarios I tested.   ## Before logging ``` Benchmark                                                   Mode      Cnt     Score   Error  Units GaugeBenchmark.baseline                                   sample  1874878     0.065 ± 0.003  us/op GaugeBenchmark.baseline:p0.00                             sample              0.001          us/op GaugeBenchmark.baseline:p0.50                             sample              0.041          us/op GaugeBenchmark.baseline:p0.90                             sample              0.048          us/op GaugeBenchmark.baseline:p0.95                             sample              0.049          us/op GaugeBenchmark.baseline:p0.99                             sample              0.084          us/op GaugeBenchmark.baseline:p0.999                            sample              2.295          us/op GaugeBenchmark.baseline:p0.9999                           sample             32.624          us/op GaugeBenchmark.baseline:p1.00                             sample           1249.280          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder             sample  1314563     0.085 ± 0.007  us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.00       sample              0.001          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.50       sample              0.046          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.90       sample              0.054          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.95       sample              0.056          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.99       sample              0.082          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.999      sample             15.680          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.9999     sample             51.722          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p1.00       sample           1472.512          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder          sample  1871879     0.081 ± 0.007  us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.00    sample              0.004          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.50    sample              0.053          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.90    sample              0.062          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.95    sample              0.065          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.99    sample              0.110          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.999   sample              4.201          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.9999  sample             37.748          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p1.00    sample           3993.600          us/op ```  ## After logging ``` Benchmark                                                   Mode      Cnt     Score   Error  Units GaugeBenchmark.baseline                                   sample  1901391     0.062 ± 0.002  us/op GaugeBenchmark.baseline:p0.00                             sample              0.001          us/op GaugeBenchmark.baseline:p0.50                             sample              0.040          us/op GaugeBenchmark.baseline:p0.90                             sample              0.044          us/op GaugeBenchmark.baseline:p0.95                             sample              0.047          us/op GaugeBenchmark.baseline:p0.99                             sample              0.066          us/op GaugeBenchmark.baseline:p0.999                            sample              1.829          us/op GaugeBenchmark.baseline:p0.9999                           sample             32.220          us/op GaugeBenchmark.baseline:p1.00                             sample            114.432          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder             sample  1568405     0.090 ± 0.003  us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.00       sample              0.008          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.50       sample              0.057          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.90       sample              0.066          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.95       sample              0.069          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.99       sample              0.093          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.999      sample             14.704          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p0.9999     sample             42.598          us/op GaugeBenchmark.gaugeReRegistrationWithBuilder:p1.00       sample            192.000          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder          sample  1667600     0.083 ± 0.004  us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.00    sample              0.010          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.50    sample              0.055          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.90    sample              0.064          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.95    sample              0.067          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.99    sample              0.086          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.999   sample             14.080          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p0.9999  sample             32.783          us/op GaugeBenchmark.gaugeReRegistrationWithoutBuilder:p1.00    sample           1423.360          us/op ```  /cc: @keith-turner",2024-11-16T01:09:38+00:00,2024-12-04T21:10:06+00:00,1,https://github.com/micrometer-metrics/micrometer/pull/5688,5688.0,2024-12-04T21:10:06+00:00,https://github.com/micrometer-metrics/micrometer/pull/5688,1,3,0,4,122,0,26,96,452.0077777777778,bug,False,True,normal,ui,"[{""filename"": ""benchmarks/benchmarks-core/src/jmh/java/io/micrometer/benchmark/core/CounterBenchmark.java"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""benchmarks/benchmarks-core/src/jmh/java/io/micrometer/benchmark/core/GaugeBenchmark.java"", ""lines_added"": 72, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""benchmarks/benchmarks-core/src/jmh/resources/logback.xml"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""micrometer-core/src/main/java/io/micrometer/core/instrument/MeterRegistry.java"", ""lines_added"": 14, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
micrometer-metrics/micrometer,5733,Deprecate AggregationTemporality#toOtlpAggregationTemporality,"It was not intended to be public, users should not use it.",2024-12-04T20:52:04+00:00,2024-12-04T21:03:21+00:00,0,https://github.com/micrometer-metrics/micrometer/pull/5733,5733.0,2024-12-04T21:03:21+00:00,https://github.com/micrometer-metrics/micrometer/pull/5733,0,1,0,1,4,0,0,4,0.1880555555555555,bug;registry: otlp,False,True,normal,functional,"[{""filename"": ""implementations/micrometer-registry-otlp/src/main/java/io/micrometer/registry/otlp/AggregationTemporality.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,5194,Fix a bug where `ClientOptions` of derived clients is duplicated,"Motivation:  If a client is derived with `ClientOptions` configurator, client options containing collection values such as `decorators`, `rpcDecorators` are duplicated. For example, the original client has `LoggingClient -> CircuitBreakerClient` decorators, and the derived client will have `LoggingClient -> CircuitBreakerClient -> LoggingClient -> CircuitBreakerClient`.  Because `configurator` builds a new `ClientOptions` and appends it to the existing `ClientOptions`.  Modifications:  - Do not set the existing client options when using `ClientOptions` configurator.  Result:  You no longer see duplicate `ClientOptions` when creating a derived client with `<type://Clients#newDerivedClient(T,Function)>`. ",2023-09-20T06:57:31+00:00,2023-09-25T10:01:14+00:00,2,https://github.com/line/armeria/pull/5194,5194.0,2023-09-25T10:01:13+00:00,https://github.com/line/armeria/pull/5194,0,2,0,2,81,9,0,90,123.06166666666668,defect,False,True,normal,configuration,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/client/Clients.java"", ""lines_added"": 9, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""thrift/thrift0.13/src/test/java/com/linecorp/armeria/client/thrift/ThriftClientDerivedClientTest.java"", ""lines_added"": 72, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,5122,Fix flaky `GrpcServiceImplErrorTest`,Motivation:  See #5120  Modifications:  - Used `whenComplete().join()` instead of `ensureComplete()` - Renamed `GrpcServiceImplErrorTest` to `InvalidGrpcMetadataTest` to easily identify the purpose of the tests  Result:  Closes #5120 ,2023-08-18T06:29:47+00:00,2023-08-18T06:47:24+00:00,0,https://github.com/line/armeria/pull/5122,5122.0,2023-08-18T06:47:24+00:00,https://github.com/line/armeria/pull/5122,0,1,0,1,2,2,0,4,0.2936111111111111,cleanup,False,True,normal,database,"[{""filename"": ""grpc/src/test/java/com/linecorp/armeria/server/grpc/InvalidGrpcMetadataTest.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",armeria,False
micrometer-metrics/micrometer,5609,Default ObservationConventions for Grpc do not always use a consistent set of keyvalues ,"**Describe the bug** DefaultGrpcServerObservationConvention and DefaultGrpcClientObservationConvention does not set 'grpc.status_code' when status code is missing (e.g. at the start of the call).  Micrometer Observation is invoking ""addLowCardinalityKeyValues"" at the start of the observation and at the end of the observation. At the start, value for status code is null.  In such case, the label is not added: https://github.com/micrometer-metrics/micrometer/blob/main/micrometer-core/src/main/java/io/micrometer/core/instrument/binder/grpc/DefaultGrpcServerObservationConvention.java#L51  If the call is interrupted (e.g. cancelled) and does not finish with the status code, the metric is collected with 4 labels, e.g: `[tag(error=none),tag(rpc.method=hello),tag(rpc.service=Greeter),tag(rpc.type=UNARY)]`  When the next call is finished, the metric is collected with 5 labels e.g.: `[tag(error=none),tag(grpc.status_code=OK),tag(rpc.method=hello),tag(rpc.service=Greeter),tag(rpc.type=UNARY)]`  For many MeterRegistries this is not a problem, but when using PrometheusMeterRegistry it creates exception: > Prometheus requires that all meters with the same name have the same set of tag keys. There is already an existing meter named 'grpc.server' containing tag keys [**error, rpc.method, rpc.service, rpc.type**]. The meter you are attempting to register has keys [**error, grpc.status_code, rpc.method, rpc.service, rpc.type**].  https://github.com/micrometer-metrics/micrometer/blob/main/implementations/micrometer-registry-prometheus/src/main/java/io/micrometer/prometheusmetrics/PrometheusMeterRegistry.java#L583  Different components are reporting ""UNKNOWN"" value for such cases, instead of omitting label e.g.: https://github.com/spring-projects/spring-framework/blob/main/spring-web/src/main/java/org/springframework/http/server/observation/DefaultServerRequestObservationConvention.java#L125 https://github.com/spring-projects/spring-framework/blob/main/spring-web/src/main/java/org/springframework/http/server/reactive/observation/DefaultServerRequestObservationConvention.java#L126   **Environment** <!-- In what environment did the bug happen? --> <!-- If you are not using the latest patch version of a supported Micrometer line, please upgrade to see if the issue happens on the latest patch version for that line (e.g. 1.6.x). See https://micrometer.io/support/ -->  - Micrometer version: 1.13.6  - Micrometer registry: prometheus  - OS: Windows  - Java version: 21  **To Reproduce** How to reproduce the bug: Create request to GrpcService and cancel it before it is finished (artificial Sleep in the Service may be required), then create request and let it be completed. ``` GreeterGrpc.GreeterFutureStub stub = GreeterGrpc.newFutureStub(inProcChannel); stub.hello(helloRequest).cancel(true); stub.hello(helloRequest).get(); ``` Set Debug breakpoint here: https://github.com/micrometer-metrics/micrometer/blob/main/implementations/micrometer-registry-prometheus/src/main/java/io/micrometer/prometheusmetrics/PrometheusMeterRegistry.java#L583  or add MeterRegistrationFailedListener to see the exception.  **Expected behavior** DefaultGrpcServerObservationConvention and DefaultGrpcClientObservationConvention should not cause error in PrometheusMeterRegistry in any scenario.  ",2024-10-28T12:54:11+00:00,2024-11-29T06:24:58+00:00,2,https://github.com/micrometer-metrics/micrometer/issues/5609,5752.0,2024-12-23T00:55:13+00:00,https://github.com/micrometer-metrics/micrometer/pull/5752,0,2,0,2,12,6,0,18,1332.0172222222222,bug;module: micrometer-core;instrumentation,False,True,normal,ui,"[{""filename"": ""micrometer-core/src/main/java/io/micrometer/core/instrument/binder/grpc/DefaultGrpcClientObservationConvention.java"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/main/java/io/micrometer/core/instrument/binder/grpc/DefaultGrpcServerObservationConvention.java"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",grpc,False
spring-cloud/spring-cloud-netflix,2831,may be found a bug for httpclient connection leak,"refer #2814   I use a subclass of DefaultErrorAttributes  to deal with exceptions  ```  @Component(""unifyErrorAttributes"") public class UnifyErrorAttributes extends DefaultErrorAttributes { ```  But I found that when exception generated,the http client connection can not be released.  here is normal routed request log,there are leased and released both exist,it's normal:  ![image](https://user-images.githubusercontent.com/37093837/38555755-f252f072-3cf9-11e8-9c53-29d42729a0c0.png)  But when route failed,only the http connection leased action exists but no released action,bcz the coming exception disturbed the release action in future:  ![qq 20180410200139](https://user-images.githubusercontent.com/37093837/38555948-93a0b400-3cfa-11e8-8551-5abfdaff6619.jpg)  the PostErrorHandlerFilter.java is a post filter,it throw an exception when route failed.  ```     @Override         public Object filter() {         HttpServletResponse response = RequestContext.getCurrentContext().getResponse();         int cde = response.getStatus();         String uri=getOriginalRequest().getRequestURI();         HttpStatus status=HttpStatus.valueOf(cde);         String req_detail= (String)          ThreadLocalCache.get(GatewayFilterConstants.CacheKey.HTTP_REQ_DETAIL);         String body= (String)          ThreadLocalCache.get(GatewayFilterConstants.CacheKey.HTTP_REQ_BODY_DETAIL);           if(status.isError()){//||status.is3xxRedirection()){             runtimeURLCollection.errorUrls(uri,"";""+cde+""[route response error]"");             throw new GateWayException(BasicErrorEnum.ERROR_1001,""status:""+cde);         }         return null;     } ```  so, at last,all of the connections are not released,my app is down.'netstat' shows all TCP  in CLOSE_WAIT my question: 1、is this a problem? 2、how to deal with that?",2018-04-10T12:14:49+00:00,2018-04-25T02:42:54+00:00,28,https://github.com/spring-cloud/spring-cloud-netflix/issues/2831,2849.0,,https://github.com/spring-cloud/spring-cloud-netflix/pull/2849,0,4,0,4,93,7,0,100,350.4680555555556,,False,True,normal,networking,"[{""filename"": ""spring-cloud-netflix-zuul/src/main/java/org/springframework/cloud/netflix/zuul/filters/ProxyRequestHelper.java"", ""lines_added"": 12, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-zuul/src/main/java/org/springframework/cloud/netflix/zuul/filters/support/FilterConstants.java"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-zuul/src/main/java/org/springframework/cloud/netflix/zuul/util/RequestUtils.java"", ""lines_added"": 51, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-zuul/src/main/java/org/springframework/cloud/netflix/zuul/web/ZuulController.java"", ""lines_added"": 27, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
line/armeria,4864,"Fix a bug where ""connection: close"" header is stripped  in a HTTP/1.1 response"," Motivation:  `connection: close` header that should be preserved is removed while an Armeria server response is converted to a Netty HTTP/1 response. We allowed users to send `connection: close` #4531. The connection was correctly closed after fully writing the response but `connection: close` wasn't sent correctly.  Modifications:  - Make `KeepAliverHandler` closed when `connection: close` header is specified in the response headers.  Result:  In HTTP/1.1, if `connection: close` is set in response headers, the value is correctly sent to clients. ",2023-05-08T07:37:10+00:00,2023-06-09T02:37:40+00:00,2,https://github.com/line/armeria/pull/4864,4864.0,2023-06-09T02:37:40+00:00,https://github.com/line/armeria/pull/4864,0,2,0,2,53,1,0,54,763.0083333333333,defect,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/server/AbstractHttpResponseHandler.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/server/Http1ServerKeepAliveTest.java"", ""lines_added"": 52, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",armeria,False
micrometer-metrics/micrometer,5709,Fix missing error on context in MicrometerHttpClient.sendAsync(),This PR fixes missing error on context in the `MicrometerHttpClient.sendAsync()` that was introducted in https://github.com/micrometer-metrics/micrometer/pull/5708.  See https://github.com/micrometer-metrics/micrometer/pull/5704#discussion_r1857537259,2024-11-26T01:23:25+00:00,2024-11-26T02:47:36+00:00,0,https://github.com/micrometer-metrics/micrometer/pull/5709,5709.0,2024-11-26T02:47:36+00:00,https://github.com/micrometer-metrics/micrometer/pull/5709,0,2,0,2,38,4,0,42,1.4030555555555555,,False,True,normal,functional,"[{""filename"": ""micrometer-java11/src/main/java/io/micrometer/java11/instrument/binder/jdk/MicrometerHttpClient.java"", ""lines_added"": 5, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""micrometer-java11/src/test/java/io/micrometer/java11/instrument/binder/jdk/MicrometerHttpClientTests.java"", ""lines_added"": 33, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
micrometer-metrics/micrometer,5705,Polish MicrometerHttpClientTests.shouldThrowErrorFromSendAsync(),"This PR polishes `MicrometerHttpClientTests.shouldThrowErrorFromSendAsync()` a bit.  The `uri` tag is already working without a response, and the `status` tag doesn't seem to be possible to be set without a response.  I'm not sure if I'm missing something here.  See gh-5553",2024-11-23T07:40:46+00:00,2024-11-25T04:45:17+00:00,0,https://github.com/micrometer-metrics/micrometer/pull/5705,5705.0,2024-11-25T04:45:17+00:00,https://github.com/micrometer-metrics/micrometer/pull/5705,0,1,0,1,6,3,0,9,45.07527777777778,polish,False,True,normal,functional,"[{""filename"": ""micrometer-java11/src/test/java/io/micrometer/java11/instrument/binder/jdk/MicrometerHttpClientTests.java"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
line/armeria,5620,Expose unexpected decoding exception reason,"Related issue: #5177 Motivation:  `AbstractStreamDecoder#decode` silently ignores the failure to decode messages. We probably want to let the caller know a decoding failure has occurred, instead of silently ignoring and returning an empty result.  Modifications:  - Modified `AbstractStreamDecoder` to rethrow the caught exception for all cases.  Result:  - Closes #5177 - (defect) Content decompression exceptions are now properly propagated. ",2024-04-18T13:00:14+00:00,2024-06-20T14:11:17+00:00,6,https://github.com/line/armeria/pull/5620,5620.0,2024-06-20T14:11:17+00:00,https://github.com/line/armeria/pull/5620,0,2,0,2,14,2,0,16,1513.184166666667,defect;sprint,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/common/encoding/AbstractStreamDecoder.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/common/encoding/DefaultHttpDecodedResponseTest.java"", ""lines_added"": 12, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
spring-cloud/spring-cloud-netflix,4048,Disabled tests due to boot test env ordering issue,caused by https://github.com/spring-projects/spring-boot/issues/28776 ,2021-11-22T19:51:43+00:00,2021-11-29T17:30:19+00:00,0,https://github.com/spring-cloud/spring-cloud-netflix/issues/4048,1294.0,2016-08-24T23:29:38+00:00,https://github.com/spring-cloud/spring-cloud-netflix/pull/1294,1,1,0,2,8,9,2,15,-45980.36805555556,bug,False,True,normal,functional,"[{""filename"": ""spring-cloud-netflix-core/src/test/java/org/springframework/cloud/netflix/hystrix/HystrixStreamEndpointTests.java"", ""lines_added"": 7, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""spring-cloud-netflix-dependencies/pom.xml"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""config""}]",,False
line/armeria,5739,Fix `CorsServerErrorHandler` to work with dynamic `CorsService`,Motivation:  This PR is follow-up of https://github.com/line/armeria/pull/5632#discussion_r1595038769 and #5670 that `ctx.findService()` should be used to find `CorsService` added with route decorators.  Modifications:  - Use `ctx.findService()` to find `CorsService` in the service chain of a request  Result:  CorsServerErrorHandler now correctly handles response exceptions even if `CorsService` is added as a route decorator. (Not need to be mentioned in the release notes)  ,2024-06-07T09:26:11+00:00,2024-06-07T16:46:40+00:00,1,https://github.com/line/armeria/pull/5739,5739.0,2024-06-07T16:46:40+00:00,https://github.com/line/armeria/pull/5739,0,2,0,2,38,16,0,54,7.341388888888889,defect,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/server/CorsServerErrorHandler.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/server/cors/CorsServerErrorHandlerTest.java"", ""lines_added"": 36, ""lines_deleted"": 14, ""file_type"": ""app_code""}]",armeria,False
micrometer-metrics/micrometer,5584,Aspects' tagsBasedOnJoinPoint may throw uncaught exception,We should catch the exception to avoid the aspect handling from causing an exception when calling user code. Originally found in https://github.com/micrometer-metrics/micrometer/pull/2462#discussion_r1796500121 courtesy of @kinddevil.,2024-10-11T08:20:23+00:00,2024-10-11T22:04:31+00:00,1,https://github.com/micrometer-metrics/micrometer/issues/5584,5585.0,2024-10-11T21:24:16+00:00,https://github.com/micrometer-metrics/micrometer/pull/5585,0,4,0,4,74,5,0,79,13.064722222222224,bug;module: micrometer-core,False,True,normal,functional,"[{""filename"": ""micrometer-core/src/main/java/io/micrometer/core/aop/CountedAspect.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/main/java/io/micrometer/core/aop/TimedAspect.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/test/java/io/micrometer/core/aop/CountedAspectTest.java"", ""lines_added"": 18, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/test/java/io/micrometer/core/aop/TimedAspectTest.java"", ""lines_added"": 20, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
line/armeria,5583,Record causes of failed attempts for RetryingClient,Motivation: #5283  Modifications: - Separating `onResponse` into `onResponseOnClient` and `onResponseOnServer` as refactoring - Add `failureAttempts(Throwable error)` and `failureAttempts(HttpStatus httpstatus)` at `ClientRequestMetrics` and make us enable to record causes of failed attempts  Result:  - Closes #5283 - The metrics for failed requests while retrying now include the cause of the failure.,2024-04-08T13:16:39+00:00,2024-06-04T12:12:14+00:00,2,https://github.com/line/armeria/pull/5583,5583.0,2024-06-04T12:12:14+00:00,https://github.com/line/armeria/pull/5583,0,2,0,2,76,5,0,81,1366.9263888888888,improvement,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/metric/RequestMetricSupport.java"", ""lines_added"": 23, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/internal/common/metric/RequestMetricSupportTest.java"", ""lines_added"": 53, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
line/armeria,5283,Record causes of failed attempts for `RetryingClient`,"Only the number of failure attempts is recorded, it is difficult to know why the attempts failed. https://github.com/line/armeria/blob/90969d1fec7a8d47957daf958474339f6e11f523/core/src/main/java/com/linecorp/armeria/internal/common/metric/RequestMetricSupport.java#L171  It would be good if the cause was recorded as a tag of the meter. ``` ""actual.requests.attempts{result=failure, cause=ResponseTimeoutException}=NN"" ```",2023-11-03T02:07:23+00:00,2024-06-04T12:12:15+00:00,0,https://github.com/line/armeria/issues/5283,5583.0,2024-06-04T12:12:14+00:00,https://github.com/line/armeria/pull/5583,0,2,0,2,76,5,0,81,5146.080833333333,new feature,False,True,normal,networking,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/internal/common/metric/RequestMetricSupport.java"", ""lines_added"": 23, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/internal/common/metric/RequestMetricSupportTest.java"", ""lines_added"": 53, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
micrometer-metrics/micrometer,5585,Catch runtime exception thrown from pjp function in aspects,"Before, this would result in an exception bubbling up to user code (CountedAspect) or the metric recording not happening (TimedAspect). This changes the behavior to consistently record the metric without the tags from the pjp function if it throws. This aligns the behavior between the two and is safer (in the case of CountedAspect). An alternative that would also be safe and consistent would be to align to the existing behavior of TimedAspect - not recording any metric when an exception is thrown.  Resolves gh-5584",2024-10-11T08:53:39+00:00,2024-10-11T21:24:16+00:00,0,https://github.com/micrometer-metrics/micrometer/pull/5585,5585.0,2024-10-11T21:24:16+00:00,https://github.com/micrometer-metrics/micrometer/pull/5585,0,4,0,4,74,5,0,79,12.510277777777778,,False,True,normal,functional,"[{""filename"": ""micrometer-core/src/main/java/io/micrometer/core/aop/CountedAspect.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/main/java/io/micrometer/core/aop/TimedAspect.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/test/java/io/micrometer/core/aop/CountedAspectTest.java"", ""lines_added"": 18, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""micrometer-core/src/test/java/io/micrometer/core/aop/TimedAspectTest.java"", ""lines_added"": 20, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
spec-first/connexion,1708,Add traceback info to common error handler,"Still up for discussion: right now, when an internal server error occurs, the logged error is not really useful for the user. For example, a `KeyError` will only print the missing key instead of the full stacktrace and error.  Adding it to the common error handler as that one is meant to catch all remaining errors, `HTTPException`s are handled by another error handler.  Another option would be to do like `Flask` and `Starlette` by adding a `debug` option, but that seems a lot of work while this current solution doesn't really have big downsides (?) ",2023-06-01T08:32:58+00:00,2023-06-01T09:07:00+00:00,2,https://github.com/spec-first/connexion/pull/1708,1708.0,2023-06-01T09:07:00+00:00,https://github.com/spec-first/connexion/pull/1708,0,1,0,1,1,1,0,2,0.5672222222222222,,False,True,normal,functional,"[{""filename"": ""connexion/middleware/exceptions.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9791,Extended metrics do not change to snake case,"## Environment Details * Helidon Version: 4.1.6 * Helidon SE * JDK version: 21 * OS:  ----------  ## Problem Description Metrics settings:  ```yaml metrics:   built-in-meter-name-format: SNAKE   gc-time-type: gauge   key-performance-indicators:     extended: true ```  When setting the meter format to `SNAKE`, most settings take snake format except for the KPI metrics:  ``` # HELP requests_inFlight   # TYPE requests_inFlight gauge ```  Additionally, also note that there is no description as to what it captures. Steps to reproduce: https://helidon.io/docs/latest/se/guides/metrics#basic-and-extended-kpi   ",2025-02-13T08:40:45+00:00,2025-03-05T18:49:32+00:00,2,https://github.com/helidon-io/helidon/issues/9791,9800.0,2025-03-05T18:49:31+00:00,https://github.com/helidon-io/helidon/pull/9800,1,3,0,4,151,22,27,146,490.1461111111111,bug;metrics;P2;4.x,False,True,minor,configuration,"[{""filename"": ""webserver/observe/metrics/pom.xml"", ""lines_added"": 26, ""lines_deleted"": 1, ""file_type"": ""config""}, {""filename"": ""webserver/observe/metrics/src/main/java/io/helidon/webserver/observe/metrics/KeyPerformanceIndicatorMetricsImpls.java"", ""lines_added"": 41, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""webserver/observe/metrics/src/main/java/io/helidon/webserver/observe/metrics/MetricsFeature.java"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""webserver/observe/metrics/src/test/java/io/helidon/webserver/observe/metrics/TestKpiMeterNameCase.java"", ""lines_added"": 81, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",webserver,False
line/armeria,5681,Test failure: `com.linecorp.armeria.server.InvalidPathWithDataTest.invalidPath()`,``` java.io.IOException: RST_STREAM received 	at java.net.http/jdk.internal.net.http.HttpClientImpl.send(HttpClientImpl.java:964) 	at java.net.http/jdk.internal.net.http.HttpClientFacade.send(HttpClientFacade.java:133) 	at com.linecorp.armeria.server.InvalidPathWithDataTest.invalidPath(InvalidPathWithDataTest.java:76) 	at java.base/java.lang.reflect.Method.invoke(Method.java:580) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) 	at java.base/java.util.ArrayList.forEach(ArrayList.java:1596) Caused by: java.io.IOException: RST_STREAM received 	at java.net.http/jdk.internal.net.http.Stream.incoming_reset(Stream.java:586) 	at java.net.http/jdk.internal.net.http.Stream.otherFrame(Stream.java:501) 	at java.net.http/jdk.internal.net.http.Stream.incoming(Stream.java:494) 	at java.net.http/jdk.internal.net.http.Http2Connection.processFrame(Http2Connection.java:936) 	at java.net.http/jdk.internal.net.http.frame.FramesDecoder.decode(FramesDecoder.java:155) 	at java.net.http/jdk.internal.net.http.Http2Connection$FramesController.processReceivedData(Http2Connection.java:307) 	at java.net.http/jdk.internal.net.http.Http2Connection.asyncReceive(Http2Connection.java:778) 	at java.net.http/jdk.internal.net.http.Http2Connection$Http2TubeSubscriber.processQueue(Http2Connection.java:1594) 	at java.net.http/jdk.internal.net.http.common.SequentialScheduler$LockingRestartableTask.run(SequentialScheduler.java:182) 	at java.net.http/jdk.internal.net.http.common.SequentialScheduler$CompleteRestartableTask.run(SequentialScheduler.java:149) 	at java.net.http/jdk.internal.net.http.common.SequentialScheduler$SchedulableTask.run(SequentialScheduler.java:207) 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) 	at java.base/java.lang.Thread.run(Thread.java:1583)  ``` ,2024-05-16T08:02:54+00:00,2024-05-24T04:21:42+00:00,1,https://github.com/line/armeria/issues/5681,5694.0,2024-05-24T04:21:41+00:00,https://github.com/line/armeria/pull/5694,0,2,0,2,11,0,0,11,188.3130555555556,,False,True,normal,networking,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/common/stream/ByteStreamMessageOutputStreamTest.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java12/com/linecorp/armeria/server/InvalidPathWithDataTest.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",armeria,False
line/armeria,4553,Test failure: `com.linecorp.armeria.common.stream.ByteStreamMessageOutputStreamTest.writeAfterStreamClosed()`,``` java.util.concurrent.TimeoutException: writeAfterStreamClosed() timed out after 60 seconds 	at org.junit.jupiter.engine.extension.TimeoutExceptionFactory.create(TimeoutExceptionFactory.java:29) 	at org.junit.jupiter.engine.extension.SameThreadTimeoutInvocation.proceed(SameThreadTimeoutInvocation.java:58) 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92) 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.util.ArrayList.forEach(ArrayList.java:1259) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.util.ArrayList.forEach(ArrayList.java:1259) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineE ``` ,2022-11-29T02:40:21+00:00,2024-05-24T04:21:42+00:00,1,https://github.com/line/armeria/issues/4553,5694.0,2024-05-24T04:21:41+00:00,https://github.com/line/armeria/pull/5694,0,2,0,2,11,0,0,11,13009.68888888889,,False,True,normal,networking,"[{""filename"": ""core/src/test/java/com/linecorp/armeria/common/stream/ByteStreamMessageOutputStreamTest.java"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java12/com/linecorp/armeria/server/InvalidPathWithDataTest.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",armeria,False
helidon-io/helidon,9833,Document new error handling configuration,## Description  Document new error handling configuration. See PR #9836.   ,2025-02-25T13:52:44+00:00,2025-02-25T19:16:00+00:00,0,https://github.com/helidon-io/helidon/issues/9833,9836.0,2025-02-25T19:15:49+00:00,https://github.com/helidon-io/helidon/pull/9836,0,1,1,2,87,1,0,35,5.384722222222222,enhancement;docs,False,True,normal,configuration,"[{""filename"": ""docs/src/main/asciidoc/se/webserver/webserver.adoc"", ""lines_added"": 53, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""docs/src/main/java/io/helidon/docs/se/WebServerSnippets.java"", ""lines_added"": 34, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",se,False
line/armeria,5578,Consider making `ContentTooLargeException` extend `HttpStatusException`,"Suggested by @dlvenable (https://github.com/line/armeria/discussions/5565) > The ContentTooLargeException does not inherit from HttpStatusException. We currently have a block of code that handles HttpStatusException exceptions by looking to see if it is a client or server error. Can we make ContentTooLargeException inherit from HttpStatusException?  `ContentTooLargeException` represents 413 status so, I think, we can make `ContentTooLargeException` extend `HttpStatusException` to streamline exception handling logic.",2024-04-08T07:11:25+00:00,2024-05-22T02:51:55+00:00,2,https://github.com/line/armeria/issues/5578,5624.0,,https://github.com/line/armeria/pull/5624,0,3,0,3,49,5,0,54,1051.675,defect,False,True,normal,functional,"[{""filename"": ""core/src/main/java/com/linecorp/armeria/common/ContentTooLargeException.java"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""core/src/main/java/com/linecorp/armeria/server/HttpStatusException.java"", ""lines_added"": 10, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""core/src/test/java/com/linecorp/armeria/common/ContentTooLargeExceptionTest.java"", ""lines_added"": 34, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",armeria,False
spec-first/connexion,945,[bug] connexion.exceptions shadowed in __init__.py,### Description The root `__init__.py` contains this line: ``` import werkzeug.exceptions as exceptions  # NOQA ```  This confuses IntelliJ (and probably other tools) when you want to import `connexion/exceptions.py` because the import `connexion.exceptions` can refer to both the imported module or the file.   ### Suggestion Change the line to ``` import werkzeug.exceptions  # NOQA ``` and use the full path everywhere. OR import as `w_exceptions` or something like that.   ### Additional info:  Connexion 1.1.15. ,2019-05-15T01:50:32+00:00,2023-02-22T23:31:57+00:00,0,https://github.com/spec-first/connexion/issues/945,1690.0,2023-04-22T20:03:01+00:00,https://github.com/spec-first/connexion/pull/1690,0,1,0,1,4,17,0,21,34530.20805555556,,False,True,normal,functional,"[{""filename"": ""connexion/security.py"", ""lines_added"": 4, ""lines_deleted"": 17, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9736,4.x: WebClient.readContinueTimeout does not work with cached connections,"## Environment Details * Helidon Version: 4.x * Helidon SE  ----------  ## Problem Description  The idle thread starts the read before `SO_TIMEOUT` is set to the [continue timeout](https://github.com/helidon-io/helidon/blob/dec95f3c6d40de9da339b192413eda069fa9c9b0/webclient/http1/src/main/java/io/helidon/webclient/http1/Http1CallOutputStreamChain.java#L393). Thus, the timeout for the `100 Continue` response is the read timeout (defaults to 30 seconds).  I.e. If the server does not handle `Expect: 100-continue`, the request takes a long time on cached connections.  ---  The client logic that [handles](https://github.com/helidon-io/helidon/blob/dec95f3c6d40de9da339b192413eda069fa9c9b0/webclient/http1/src/main/java/io/helidon/webclient/http1/Http1CallOutputStreamChain.java#L399) socket read timeout does not work with cached connections. The idle task completion [wraps](https://github.com/helidon-io/helidon/blob/dec95f3c6d40de9da339b192413eda069fa9c9b0/common/socket/src/main/java/io/helidon/common/socket/IdleInputStream.java#L126) all exceptions with a RuntimeException which bubbles up and closes the connection.  I.e. If the server does not handle `Expect: 100-continue`, every other request takes a very long time. The 2nd request times-out on the read timeout, and then fail.  ---  I did some experimentation to ""cancel"" the idle thread.  It does seem to fix the use-case of a server with bad support for `Expect: 100-continue`.  ```java private void handle() {     while (!canceled) {         try {             socket.setSoTimeout(500);             next = upstream.read();             if (next <= 0) {                 closed = true;             }             // restore SO_TIMEOUT             socket.setSoTimeout((int) readTimeout.toMillis());             return;         } catch(SocketTimeoutException ignored) {         } catch (IOException e) {             closed = true;             throw new UncheckedIOException(e);         }     } } ```  ## Steps to reproduce  See #9731  ",2025-02-06T04:29:33+00:00,2025-02-25T08:47:46+00:00,0,https://github.com/helidon-io/helidon/issues/9736,9837.0,2025-02-25T19:27:30+00:00,https://github.com/helidon-io/helidon/pull/9837,0,6,0,6,83,13,0,96,470.9658333333333,bug;P1;webclient;4.x,False,True,major,networking,"[{""filename"": ""common/socket/src/main/java/io/helidon/common/socket/IdleInputStream.java"", ""lines_added"": 44, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""common/socket/src/main/java/io/helidon/common/socket/PlainSocket.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""webclient/api/src/main/java/io/helidon/webclient/api/ClientConnection.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""webclient/api/src/main/java/io/helidon/webclient/api/TcpClientConnection.java"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""webclient/http1/src/main/java/io/helidon/webclient/http1/Http1CallOutputStreamChain.java"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""webserver/tests/webserver/src/test/java/io/helidon/webserver/tests/FollowRedirectTest.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",webclient;webserver,True
helidon-io/helidon,9731,OIDC Token Validation Fails on Every Alternate Request,"**Issue Description:**  ---  We are integrating OIDC security provider with Oracle Access Manager (OAM) as the identity provider. When we make the first request to helidon server, the token is validated correctly, and we receive a response immeditely (~ 1 sec) . However, on the second request with the same token, there is no response for 25-30 seconds, and it eventually fails due to a timeout error.  The issue follows a consistent pattern:  1st request → ✅ Passes 2nd request → ❌ Fails (timeout) 3rd request → ✅ Passes 4th request → ❌ Fails (timeout) 5th request → ✅ Passes (Repeats in the same cycle)  We found same pattern when we used sample OIDC project with branch helidon-4.x with below configutaion.  link - https://github.com/helidon-io/helidon-examples/tree/helidon-4.x/examples/microprofile/oidc      - oidc:         # use a custom name, so it does not clash with other examples         cookie-name: ""OIDC_EXAMPLE_COOKIE""         # support for ""Authorization"" header with bearer token         header-use: true         # the default redirect-uri, where the webserver listens on redirects from identity server         redirect-uri: ""/oidc/redirect""         audience: ""ResourceServer""         client-id: OAUTH2CLIENT         client-secret: ***************         identity-uri: http://xxx:7777/         frontend-uri: ""${security.properties.frontend-uri}""         introspect-endpoint-uri: http:/xxx:14100/oauth2/rest/token/introspect?identityDomain=IDStore         # We want to redirect to login page (and token can be received either through cookie or header)         redirect: false   **Environment Details:**  ---  **Product's Environment Details**   Client Version: v1.28.2 Server Version: v1.28.2 Helidon : v4.1.6 JDK : 21 System Version: ""Oracle Linux Server 8.9""  **Sample OIDC environment details**  Helidon : 4.1.6 JDK : 21  System Version: macOS 14.7.1 (23H222)   **Error Logs:**  ---  [Logs.txt](https://github.com/user-attachments/files/18667473/Logs.txt)   **Additional Information:**  ---  1. If a request is made for the first time, it is successfully validated. However, if another request is sent with the same token within ~30 seconds, it fails with a 401 Unauthorized error. If we wait for approximately 30 seconds before making the next request, the token is validated successfully again. This pattern repeats consistently.   2. We also verified whether the issue originates from the identity provider (OAM). To do this, we tested the Helidon 3.x OIDC sample project with the exact same configuration. However, we did not encounter the same behavior in that setup and things are working fine with Helidon 3.x.  3. We also tested the introspection URL back to back within a Kubernetes pod and on the machine where we ran the sample OIDC project. In both cases, the response was received quickly without any delay.  ",2025-02-05T07:14:00+00:00,2025-02-25T08:47:47+00:00,3,https://github.com/helidon-io/helidon/issues/9731,9837.0,2025-02-25T19:27:30+00:00,https://github.com/helidon-io/helidon/pull/9837,0,6,0,6,83,13,0,96,492.225,bug;security;4.x,False,True,normal,configuration,"[{""filename"": ""common/socket/src/main/java/io/helidon/common/socket/IdleInputStream.java"", ""lines_added"": 44, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""common/socket/src/main/java/io/helidon/common/socket/PlainSocket.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""webclient/api/src/main/java/io/helidon/webclient/api/ClientConnection.java"", ""lines_added"": 18, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""webclient/api/src/main/java/io/helidon/webclient/api/TcpClientConnection.java"", ""lines_added"": 12, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""webclient/http1/src/main/java/io/helidon/webclient/http1/Http1CallOutputStreamChain.java"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""webserver/tests/webserver/src/test/java/io/helidon/webserver/tests/FollowRedirectTest.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",webclient;webserver,True
spec-first/connexion,590,Inaccurate error message for ResolverErrors,"### Description I misspelled the name of the module in swagger.yaml and got this traceback:  ``` Failed to add operation for GET /api/block/forecast Traceback (most recent call last):   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apis/abstract.py"", line 250, in add_paths     self.add_operation(method, path, endpoint, path_parameters)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apis/abstract.py"", line 203, in add_operation     pythonic_params=self.pythonic_params)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/operation.py"", line 223, in __init__     resolution = resolver.resolve(self)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/resolver.py"", line 40, in resolve     return Resolution(self.resolve_function_from_operation_id(operation_id), operation_id)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/resolver.py"", line 65, in resolve_function_from_operation_id     raise ResolverError(msg, sys.exc_info()) connexion.exceptions.ResolverError: <ResolverError: Cannot resolve operationId ""with_flas.block""! Import error was ""No module named 'with_flas'"">  During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File ""/Users/ptz/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/181.4892.64/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1664, in <module>     main()   File ""/Users/ptz/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/181.4892.64/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1658, in main     globals = debugger.run(setup['file'], None, None, is_module)   File ""/Users/ptz/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/181.4892.64/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1068, in run     pydev_imports.execfile(file, globals, locals)  # execute the script   File ""/Users/ptz/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/181.4892.64/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile     exec(compile(contents+""\\n"", file, 'exec'), glob, loc)   File ""/Users/ptz/stuff/with_connexion/with_flask.py"", line 9, in <module>     app.add_api('swagger.yml')   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apps/flask_app.py"", line 54, in add_api     api = super(FlaskApp, self).add_api(specification, **kwargs)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apps/abstract.py"", line 159, in add_api     options=api_options.as_dict())   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apis/abstract.py"", line 139, in __init__     self.add_paths()   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apis/abstract.py"", line 257, in add_paths     self._handle_add_operation_error(path, method, err.exc_info)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/apis/abstract.py"", line 271, in _handle_add_operation_error     six.reraise(*exc_info)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/six.py"", line 693, in reraise     raise value   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/resolver.py"", line 62, in resolve_function_from_operation_id     return self.function_resolver(operation_id)   File ""/Users/ptz/.virtualenvs/with_connexion/lib/python3.6/site-packages/connexion/utils.py"", line 36, in get_function_from_name     module = importlib.import_module(module_name)   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/__init__.py"", line 126, in import_module     return _bootstrap._gcd_import(name[level:], package, level)   File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import   File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load   File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked ModuleNotFoundError: No module named 'with_flas' ```  This happens due to passing `ResolverError.exc_info` to `AbstractAPI._handle_add_operation_error` at https://github.com/zalando/connexion/blob/master/connexion/apis/abstract.py#L257. IIUC `sys.exc_info()` should be passed so that `ResolverError` is reraised.   ### Expected behaviour It would be much nicer to have `ResolverError` message at the bottom and get rid of original `ImportError` altogether. The `ResolverError` message is very nice and helpful, it shouldn't be shaded be low-level `ImportError` (:   ### Steps to reproduce Just follow the docs and misspell the module name.   ### Additional info:  Output of the commands:  - `python --version` 3.6.3 - `pip show connexion | grep ""^Version\\:""` 1.4.2 ",2018-05-26T08:50:53+00:00,2023-02-22T23:31:57+00:00,1,https://github.com/spec-first/connexion/issues/590,1649.0,2023-02-22T23:31:55+00:00,https://github.com/spec-first/connexion/pull/1649,0,22,0,22,180,231,0,411,41606.68388888889,,False,True,normal,configuration,"[{""filename"": ""connexion/__init__.py"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""connexion/apps/flask.py"", ""lines_added"": 3, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""connexion/exceptions.py"", ""lines_added"": 99, ""lines_deleted"": 134, ""file_type"": ""app_code""}, {""filename"": ""connexion/handlers.py"", ""lines_added"": 2, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""connexion/middleware/abstract.py"", ""lines_added"": 6, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""connexion/middleware/exceptions.py"", ""lines_added"": 15, ""lines_deleted"": 33, ""file_type"": ""app_code""}, {""filename"": ""connexion/middleware/response_validation.py"", ""lines_added"": 3, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""connexion/mock.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""connexion/problem.py"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""connexion/resolver.py"", ""lines_added"": 4, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""connexion/security.py"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""connexion/validators/form_data.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""connexion/validators/json.py"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""connexion/validators/parameter.py"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/api/test_errors.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/api/test_headers.py"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""tests/api/test_secure_api.py"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""tests/decorators/test_security.py"", ""lines_added"": 12, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""tests/fakeapi/hello/__init__.py"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/test_api.py"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""tests/test_cli.py"", ""lines_added"": 4, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""tests/test_json_validation.py"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",tests;connexion,True
camunda/camunda,29726,Mismatch of `RESOURCE`-related column family values between `stable/8.7` and `main`,"### Description  Reported on [Slack](https://camunda.slack.com/archives/C07DA2WBMAL/p1742216888664449):  > We've spotted a potential issue with `ZbColumnFamilies` values differing between `stable/8.7` and `main/8.8-SNAPSHOT` versions.  > Specifically, `RESOURCE`-related column families have different values: > - In 8.7, `RESOURCES` starts from **100** ([link](https://github.com/camunda/camunda/blob/6dc9b47d9e5ed73e59af0b3fb5b6564f5fa86861/zeebe/protocol/src/main/java/io/camunda/zeebe/protocol/ZbColumnFamilies.java#L201-L205)). > - In 8.8, `RESOURCES` starts from **114** ([link](https://github.com/camunda/camunda/blob/6f8a54072c397bc60b3f55972c610db5fc98fb3c/zeebe/protocol/src/main/java/io/camunda/zeebe/protocol/ZbColumnFamilies.java#L226-L230)).  > Since [`ZbColumnFamilies#getValue`](https://github.com/camunda/camunda/blob/32c6735cf97f3e790996c5404b50a1eefe8b35ba/zeebe/protocol/src/main/java/io/camunda/zeebe/protocol/EnumValue.java#L18-L38) is used internally by [TransactionalColumnFamily](https://github.com/camunda/camunda/blob/6f8a54072c397bc60b3f55972c610db5fc98fb3c/zeebe/zb-db/src/main/java/io/camunda/zeebe/db/impl/rocksdb/transaction/TransactionalColumnFamily.java#L75) -> [ColumnFamilyContext](https://github.com/camunda/camunda/blob/6f8a54072c397bc60b3f55972c610db5fc98fb3c/zeebe/zb-db/src/main/java/io/camunda/zeebe/db/impl/rocksdb/transaction/ColumnFamilyContext.java#L36) as `columnFamilyPrefix`, having different values between versions does not seem safe. It could potentially lead to data corruption or incorrect lookups when migrating from 8.7 to 8.8.  > It looks like this happened due to manual backporting, which might have caused an unintended mismatch.  ### Additional Context The changes were introduced in: - [8b41d1f1](https://github.com/camunda/camunda/commit/8b41d1f17c4a637649ed371164dd1072ab8baf23) on `main` - [ef969aab](https://github.com/camunda/camunda/commit/ef969aab1fbad7f53ceb3c60da0230d37889f26c) on `stable/8.7`  ### Acceptance Criteria - [x] All `RESOURCE`-related entries in `ZbColumnFamilies` on `main` have the same values as on `stable/8.7`.",2025-03-18T06:34:41+00:00,2025-03-19T16:43:19+00:00,0,https://github.com/camunda/camunda/issues/29726,29727.0,2025-03-19T16:43:18+00:00,https://github.com/camunda/camunda/pull/29727,0,2,0,2,21,24,0,45,34.14361111111111,kind/bug;component/zeebe,False,True,normal,database,"[{""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/state/ProcessExecutionCleanStateTest.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""zeebe/protocol/src/main/java/io/camunda/zeebe/protocol/ZbColumnFamilies.java"", ""lines_added"": 21, ""lines_deleted"": 23, ""file_type"": ""app_code""}]",,False
camunda/camunda,29679,(WORK IN PROGRESS) Trying to discover a bug in the ProcessTestExtensionIT,"This is a tentative PR for [#19182](https://github.com/camunda/camunda/issues/19182), but unfortunately we've run into a really difficult bug. This branch is just for testing purposes and does not represent a complete PR.",2025-03-17T15:06:10+00:00,2025-03-19T16:26:16+00:00,0,https://github.com/camunda/camunda/pull/29679,29679.0,,https://github.com/camunda/camunda/pull/29679,0,7,0,7,84,28,0,112,49.335,,False,True,normal,functional,"[{""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/api/CamundaProcessTestExtension.java"", ""lines_added"": 25, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/assertions/CamundaDataSource.java"", ""lines_added"": 10, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/client/CamundaManagementClient.java"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/client/purge/MinimalLastChangeDto.java"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/client/purge/MinimalTopologyResponseDto.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/test/java/io/camunda/process/test/api/CamundaProcessTestExtensionIT.java"", ""lines_added"": 21, ""lines_deleted"": 11, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/test/java/io/camunda/process/test/api/JunitExtensionTest.java"", ""lines_added"": 15, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",test,False
camunda/camunda,29518,Access to <default> tenant is hardcoded for all users and mappings,"**Describe the bug**  <!-- A clear and concise description of what the bug is. -->  With the original Multi-tenancy implementation (with versions [8.3 - 8.7]), we had the following behavior: - If MT is disabled, data is mapped to the `<default>` tenant. - If MT is ~disabled~ enabled, the `<default>` tenant is ~created~ treated as any other tenant, i.e. users need to be granted access to it.  With the current re-architecture of Identity, access to the `<default>` tenant when MT is enabled is granted by default to all users and mappings (see code [here](https://github.com/camunda/camunda/blob/5df6e9f92ce85bf3f06ce33a853f1f5ab9e9693f/zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/identity/AuthorizationCheckBehavior.java#L145-L147) and [here](https://github.com/camunda/camunda/blob/5df6e9f92ce85bf3f06ce33a853f1f5ab9e9693f/zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/identity/AuthorizationCheckBehavior.java#L159-L161)). This access is hardcoded and can't be revoked.  **To Reproduce**  <!-- Steps to reproduce the behavior --> Execute the `MultiTenancyIT#shouldDenyDeployProcessWhenUnauthorizedForDefaultTenant()`test case.  ℹ The test class will be restored with https://github.com/camunda/camunda/issues/29233  **Expected behavior**  <!-- A clear and concise description of what you expected to happen. --> The `<default>` tenant should not be explicitly available when MT is enabled.  Be sure to re-enable the disabled tests in the `MultiTenancyOverIdentityIT`  **Log/Stacktrace**  <!-- If appropriate, add the full stacktrace which contains the issue. -->  <details><summary>Full Stacktrace</summary>  <p>  ``` <STACKTRACE> ```  </p> </details>  **Environment:** - Camunda Version: <!-- [e.g. 0.20.0] --> - Configuration: <!-- [e.g. exporters etc.] --> ",2025-03-12T15:36:31+00:00,2025-03-19T15:11:01+00:00,0,https://github.com/camunda/camunda/issues/29518,29839.0,2025-03-19T15:11:00+00:00,https://github.com/camunda/camunda/pull/29839,0,7,0,7,719,522,0,1241,167.57472222222222,kind/bug;component/identity,False,True,normal,configuration,"[{""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/identity/AuthorizationCheckBehavior.java"", ""lines_added"": 63, ""lines_deleted"": 35, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/identity/IdentitySetupInitializeProcessor.java"", ""lines_added"": 78, ""lines_deleted"": 38, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/authorization/DbMappingState.java"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/authorization/PersistedMapping.java"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/processing/authorization/AuthorizationCheckBehaviorMultiTenancyTest.java"", ""lines_added"": 549, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/processing/authorization/AuthorizationCheckBehaviorTest.java"", ""lines_added"": 0, ""lines_deleted"": 422, ""file_type"": ""app_code""}, {""filename"": ""zeebe/qa/integration-tests/src/test/java/io/camunda/zeebe/it/multitenancy/MultiTenancyIT.java"", ""lines_added"": 16, ""lines_deleted"": 26, ""file_type"": ""app_code""}]",authorization;authorizationcheckbehavior.java,True
camunda/camunda,29054,v2/authorizations/search with sort by resourceId throws IllegalArgumentException,"**Describe the bug**  When user tries to query existing authorizations by sorting for resourceId the API throws 400 error code with an exception IllegalArgumentException.  Note: [ownerId, ownerType, resourceType] - works fine.  **To Reproduce**  `curl -L 'http://localhost:8080/v2/authorizations/search' \\ -H 'Content-Type: application/json' \\ -H 'Accept: application/json' \\ -H ""Authorization: Basic ZGVtbzpkZW1v"" \\ -d '{     ""sort"": [         {             ""field"": ""resourceId"",             ""order"": ""ASC""         }     ] }'`  **Expected behavior**  User should be able to see the full list of the existing authorizations.   **Actual response**  ``` {   ""type"": ""about:blank"",   ""title"": ""java.lang.IllegalArgumentException"",   ""status"": 400,   ""detail"": ""Expected to handle REST API request, but JSON property was invalid"",   ""instance"": ""/v2/authorizations/search"" } ```  **Environment:** - local build from main as of 10:30 of 3.03 ",2025-03-03T09:40:25+00:00,2025-03-19T06:18:41+00:00,3,https://github.com/camunda/camunda/issues/29054,29749.0,2025-03-19T06:18:40+00:00,https://github.com/camunda/camunda/pull/29749,0,3,0,3,5,3,0,8,380.6375,kind/bug;component/identity,False,True,normal,configuration,"[{""filename"": ""search/search-client-query-transformer/src/main/java/io/camunda/search/clients/transformers/filter/AuthorizationFilterTransformer.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""search/search-client-query-transformer/src/main/java/io/camunda/search/clients/transformers/sort/AuthorizationFieldSortingTransformer.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""webapps-schema/src/main/java/io/camunda/webapps/schema/descriptors/usermanagement/index/AuthorizationIndex.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",authorizationfieldsortingtransformer.java;webapps-schema;authorizationfiltertransformer.java,True
kumahq/kuma,12719,Kuma allows creation of secrets on zone CP but deletes them on restart,"### Kuma Version  2.9.3  ### Describe the bug  Currently, Kuma allows users to create secrets on a zone control plane (ZCP), but these secrets are deleted when the ZCP restarts and syncs resources with the global control plane (GCP). If secrets are not supported at the zone level, Kuma should reject their creation instead of silently removing them later.  **Actual Behavior**  - Kuma allows users to create secrets on a ZCP.   - The secrets are removed when the ZCP restarts.   - No warning or error is given during creation, leading to unexpected failures.    ### To Reproduce   1. Deploy a TLS certificate for MeshGateway 2. Configure the MeshGateway to secure the listener with mentioned secret 3. Restart the ZCP. 4. The secret is deleted during startup since it does not exist in the GCP.  ### Expected behavior  Until Kuma does not support zone-scoped secrets, it should reject their creation at the ZCP level and provide a clear error message.  ### Additional context (optional)  Related: https://github.com/kumahq/kuma/issues/12718",2025-01-31T08:09:53+00:00,2025-02-17T10:42:03+00:00,0,https://github.com/kumahq/kuma/issues/12719,12857.0,2025-02-17T10:39:48+00:00,https://github.com/kumahq/kuma/pull/12857,0,6,0,6,128,65,0,193,410.49861111111113,triage/accepted;kind/bug,False,True,normal,configuration,"[{""filename"": ""pkg/core/resources/model/resource.go"", ""lines_added"": 17, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kds/v2/client/kds_client.go"", ""lines_added"": 27, ""lines_deleted"": 13, ""file_type"": ""app_code""}, {""filename"": ""pkg/kds/v2/client/stream.go"", ""lines_added"": 16, ""lines_deleted"": 17, ""file_type"": ""app_code""}, {""filename"": ""pkg/kds/v2/store/sync.go"", ""lines_added"": 36, ""lines_deleted"": 34, ""file_type"": ""app_code""}, {""filename"": ""pkg/kds/v2/store/sync_test.go"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/util/xds/logging_callbacks.go"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
camunda/camunda,28173,"Camunda Exporter is not creating indices for templates, when templates already exist","## Description  We ran into this during implementing the new IT foundation #26896 as we detected that indices are not cleaned up correctly, and later some indices are not re-created.  It turned out that if the templates were not cleaned up as well, the corresponding indices haven't been recreated (even if they were missing). Causing endless loops of waiting for all indices.  In an production environment this might be less likely, but in general I think it makes sense to de-couple this logic to always create the index (and log the failure if it already exists).  Happy to discuss this further @EuroLew ",2025-02-14T11:52:51+00:00,2025-03-19T09:13:37+00:00,2,https://github.com/camunda/camunda/issues/28173,28557.0,2025-03-19T09:13:36+00:00,https://github.com/camunda/camunda/pull/28557,0,2,0,2,76,34,0,110,789.3458333333333,kind/bug;severity/mid;area/project;impact/low;likelihood/low,False,True,normal,functional,"[{""filename"": ""zeebe/exporters/camunda-exporter/src/main/java/io/camunda/exporter/schema/SchemaManager.java"", ""lines_added"": 40, ""lines_deleted"": 32, ""file_type"": ""app_code""}, {""filename"": ""zeebe/exporters/camunda-exporter/src/test/java/io/camunda/exporter/schema/SchemaManagerIT.java"", ""lines_added"": 36, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
camunda/camunda,13018,Endless errors on gateway because the writer is full,"**Describe the bug**  We see several new error in the gateway, but the reason is not 100% clear to me (maybe leader change?). Seem to be introduced with https://github.com/camunda/zeebe/pull/12910/files  Error group: https://console.cloud.google.com/errors/detail/CKC158H9-Ia22wE;service=zeebe;time=P7D?project=camunda-saas-int-chaos  <!-- A clear and concise description of what the bug is. -->  **To Reproduce**  Run recent main qa  <!-- Steps to reproduce the behavior  If possible add a minimal reproducer code sample - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java  -->  **Expected behavior** If it is a valid error, more context and maybe clarify what the operator has to do. If it is not an error, then reduce log level.  <!-- A clear and concise description of what you expected to happen. -->  **Log/Stacktrace**  <!-- If possible add the full stacktrace or Zeebe log which contains the issue. --> [Full LOG](https://console.cloud.google.com/logs/query;query=%0AlogName:%22stdout%22%0Aresource.type%3D%22k8s_container%22%0Aresource.labels.location%3D%22europe-west1%22%0Aresource.labels.cluster_name%3D%22worker-chaos-1%22%0Aresource.labels.project_id%3D%22camunda-saas-int-chaos%22%0Aresource.labels.namespace_name%3D%22a6dc0794-2fa7-40ea-9870-3cee7c4c9d07-zeebe%22%0Aresource.labels.pod_name%3D%22zeebe-gateway-77bc5f6df6-9f84s%22%0Aresource.labels.container_name%3D%22zeebe-gateway%22;timeRange=2023-06-06T06:33:04.024Z%2F2023-06-06T07:33:04.024Z;pinnedLogId=2023-06-06T06:53:11.119221635Z%2Fidk7b8hg2ety3966;cursorTimestamp=2023-06-06T06:41:02.123132721Z?project=camunda-saas-int-chaos)  <details><summary>Details</summary>  <p>  ``` io.camunda.zeebe.gateway.cmd.BrokerErrorException: Received error from broker (INTERNAL_ERROR): Failed to write client request to partition '3', because the writer is full.  at io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.handleResponse ( [io/camunda.zeebe.gateway.impl.broker/BrokerRequestManager.java:194](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.gateway.impl.broker%2FBrokerRequestManager.java&line=194&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.gateway.impl.broker.BrokerRequestManager.lambda$sendRequestInternal$2 ( [io/camunda.zeebe.gateway.impl.broker/BrokerRequestManager.java:143](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.gateway.impl.broker%2FBrokerRequestManager.java&line=143&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.future.FutureContinuationRunnable.run ( [io/camunda.zeebe.scheduler.future/FutureContinuationRunnable.java:28](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler.future%2FFutureContinuationRunnable.java&line=28&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorJob.invoke ( [io/camunda.zeebe.scheduler/ActorJob.java:94](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=94&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorJob.execute ( [io/camunda.zeebe.scheduler/ActorJob.java:45](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorJob.java&line=45&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorTask.execute ( [io/camunda.zeebe.scheduler/ActorTask.java:119](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorTask.java&line=119&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorThread.executeCurrentTask ( [io/camunda.zeebe.scheduler/ActorThread.java:106](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=106&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorThread.doWork ( [io/camunda.zeebe.scheduler/ActorThread.java:87](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=87&project=camunda-saas-int-chaos) ) at io.camunda.zeebe.scheduler.ActorThread.run ( [io/camunda.zeebe.scheduler/ActorThread.java:198](https://console.cloud.google.com/debug?referrer=fromlog&file=io%2Fcamunda.zeebe.scheduler%2FActorThread.java&line=198&project=camunda-saas-int-chaos) ) ```  </p> </details>  **Environment:** - OS: saas - chaos - Zeebe Version: [zeebe:8.3.0-SNAPSHOT-main-7ad25a56](https://console.cloud.google.com/errors/detail/CKC158H9-Ia22wE;service=zeebe;version=8.3.0-SNAPSHOT-main-7ad25a56;time=P7D?project=camunda-saas-int-chaos) [zeebe:8.3.0-SNAPSHOT-main-bc9de50d](https://console.cloud.google.com/errors/detail/CKC158H9-Ia22wE;service=zeebe;version=8.3.0-SNAPSHOT-main-bc9de50d;time=P7D?project=camunda-saas-int-chaos) - Configuration: <!-- [e.g. exporters etc.] --> ",2023-06-08T11:13:51+00:00,2024-05-21T09:34:26+00:00,7,https://github.com/camunda/camunda/issues/13018,18675.0,2024-05-22T06:45:41+00:00,https://github.com/camunda/camunda/pull/18675,0,1,0,1,4,1,0,5,8371.530555555555,kind/bug;good first issue;severity/low;scope/gateway;component/gateway;component/zeebe;version:8.5.2;version:8.3.12;version:8.2.28;version:8.4.8;version:8.6.0-alpha2;version:8.6.0,False,True,normal,configuration,"[{""filename"": ""broker/src/main/java/io/camunda/zeebe/broker/transport/ErrorResponseWriter.java"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
camunda/camunda,26115,[Operate REST API v1] The flow node instances search API doesn't return the flow node name with the CamundaExporter,"### Description (required on creation)  I send a request to the flow-node instance search API: https://docs.camunda.io/docs/apis-tools/operate-api/specifications/search-4/.  Previously, I used the `ElasticsearchExporter`. After replacing the exporter with the `CamundaExporter`, the API response changed. It doesn't contain the flow-node name of the start event anymore.    ### Steps to reproduce (required on creation)  1. Deploy a BPMN process with a start event 1. Create a process instance 1. Send a POST request to `http://localhost:8080/v1/flownode-instances/search` and filter by the process instance key.  <details>   <summary>BPMN process XML</summary>  ``` <definitions xmlns:bpmndi=""http://www.omg.org/spec/BPMN/20100524/DI"" xmlns:dc=""http://www.omg.org/spec/DD/20100524/DC"" xmlns:di=""http://www.omg.org/spec/DD/20100524/DI"" xmlns:ns0=""http://camunda.org/schema/zeebe/1.0"" exporter=""Zeebe BPMN Model"" exporterVersion=""8.6.0"" id=""definitions_3cc2374a-54b8-454c-bdcc-7d8a781963c3"" xmlns:modeler=""http://camunda.org/schema/modeler/1.0"" modeler:executionPlatform=""Camunda Cloud"" modeler:executionPlatformVersion=""8.6.0"" targetNamespace=""http://www.omg.org/spec/BPMN/20100524/MODEL"" xmlns=""http://www.omg.org/spec/BPMN/20100524/MODEL"">   <process id=""process"" isExecutable=""true"">     <startEvent id=""startEvent_c773bcd7-95b9-4e59-bd7b-a2202c5e98e9"" name=""start"">       <extensionElements>         <ns0:ioMapping>           <ns0:output source=""=&quot;active&quot;"" target=""status""/>         </ns0:ioMapping>       </extensionElements>       <outgoing>sequenceFlow_8d5c79c5-bf3b-4e2c-8adb-dab22de99ddb</outgoing>     </startEvent>     <manualTask id=""manualTask_44607436-a8d5-4db9-a1bf-978dd882b635"" name=""task"">       <incoming>sequenceFlow_8d5c79c5-bf3b-4e2c-8adb-dab22de99ddb</incoming>       <outgoing>sequenceFlow_48905556-ea94-4b7a-9eb9-1c7358712ca5</outgoing>     </manualTask>     <sequenceFlow id=""sequenceFlow_8d5c79c5-bf3b-4e2c-8adb-dab22de99ddb"" sourceRef=""startEvent_c773bcd7-95b9-4e59-bd7b-a2202c5e98e9"" targetRef=""manualTask_44607436-a8d5-4db9-a1bf-978dd882b635""/>     <endEvent id=""endEvent_faa6fec0-23cf-466e-9a72-29d3e8a419ba"" name=""end"">       <extensionElements>         <ns0:ioMapping>           <ns0:output source=""=&quot;ok&quot;"" target=""result""/>         </ns0:ioMapping>       </extensionElements>       <incoming>sequenceFlow_48905556-ea94-4b7a-9eb9-1c7358712ca5</incoming>     </endEvent>     <sequenceFlow id=""sequenceFlow_48905556-ea94-4b7a-9eb9-1c7358712ca5"" sourceRef=""manualTask_44607436-a8d5-4db9-a1bf-978dd882b635"" targetRef=""endEvent_faa6fec0-23cf-466e-9a72-29d3e8a419ba""/>   </process>   <bpmndi:BPMNDiagram id=""BPMNDiagram_0955a688-c259-4987-a38a-7eadec167475"">     <bpmndi:BPMNPlane bpmnElement=""process"" id=""BPMNPlane_c263e981-c5c5-4024-a90f-3e12e0e02917"">       <bpmndi:BPMNShape bpmnElement=""startEvent_c773bcd7-95b9-4e59-bd7b-a2202c5e98e9"" id=""BPMNShape_5e567e12-1072-44a9-8adf-b9edcb5ddb9e"">         <dc:Bounds height=""36.0"" width=""36.0"" x=""100.0"" y=""100.0""/>       </bpmndi:BPMNShape>       <bpmndi:BPMNShape bpmnElement=""manualTask_44607436-a8d5-4db9-a1bf-978dd882b635"" id=""BPMNShape_7169980f-1e28-4241-bce7-d0c9b8c34cbe"">         <dc:Bounds height=""80.0"" width=""100.0"" x=""186.0"" y=""78.0""/>       </bpmndi:BPMNShape>       <bpmndi:BPMNEdge bpmnElement=""sequenceFlow_8d5c79c5-bf3b-4e2c-8adb-dab22de99ddb"" id=""BPMNEdge_44985a97-4812-45c9-b27d-89ab100ae2be"">         <di:waypoint x=""136.0"" y=""118.0""/>         <di:waypoint x=""186.0"" y=""118.0""/>       </bpmndi:BPMNEdge>       <bpmndi:BPMNShape bpmnElement=""endEvent_faa6fec0-23cf-466e-9a72-29d3e8a419ba"" id=""BPMNShape_439d4a46-9d8e-4449-a60e-d8a7251f09d3"">         <dc:Bounds height=""36.0"" width=""36.0"" x=""336.0"" y=""100.0""/>       </bpmndi:BPMNShape>       <bpmndi:BPMNEdge bpmnElement=""sequenceFlow_48905556-ea94-4b7a-9eb9-1c7358712ca5"" id=""BPMNEdge_0e0b11ae-f21c-415e-8ef7-e55f9c693007"">         <di:waypoint x=""286.0"" y=""118.0""/>         <di:waypoint x=""336.0"" y=""118.0""/>       </bpmndi:BPMNEdge>     </bpmndi:BPMNPlane>   </bpmndi:BPMNDiagram> </definitions> ```  </details>  ### Current behavior (required on creation)  Response with the `CamundaExporter`:  ``` {    ""items"":[       {          ""key"":2251799813685254,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:39:56.958+0000"",          ""endDate"":""2024-12-16T09:39:56.958+0000"",          ""flowNodeId"":""startEvent_05a48dd8-b06a-4b86-bb8c-7c0d69cff540"",          ""type"":""START_EVENT"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       },       {          ""key"":2251799813685257,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:39:56.958+0000"",          ""endDate"":""2024-12-16T09:39:56.958+0000"",          ""flowNodeId"":""manualTask_267d7dfa-c32c-403e-a86e-aefa0758f3e8"",          ""flowNodeName"":""task"",          ""type"":""MANUAL_TASK"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       },       {          ""key"":2251799813685259,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:39:56.958+0000"",          ""endDate"":""2024-12-16T09:39:56.958+0000"",          ""flowNodeId"":""endEvent_32ede224-d320-4a73-8d9d-44f3868e5080"",          ""flowNodeName"":""end"",          ""type"":""END_EVENT"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       }    ],    ""sortValues"":[       2251799813685259    ],    ""total"":3 } ```  The first item doesn't contain a field `flowNodeName`.  ### Expected behavior (required on creation)  <!-- In comparison to the current behavior, describe what should happen instead for the product to work correctly -->  Response with the `ElasticseachExporter`:  ``` {    ""items"":[       {          ""key"":2251799813685254,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:49:18.537+0000"",          ""endDate"":""2024-12-16T09:49:18.537+0000"",          ""flowNodeId"":""startEvent_b77c7bdf-31d5-4017-8ef5-4805b83e2fc5"",          ""flowNodeName"":""start"",          ""type"":""START_EVENT"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       },       {          ""key"":2251799813685257,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:49:18.537+0000"",          ""endDate"":""2024-12-16T09:49:18.537+0000"",          ""flowNodeId"":""manualTask_bf1ca3a2-e23f-49ef-879e-e8473bcadb5b"",          ""flowNodeName"":""task"",          ""type"":""MANUAL_TASK"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       },       {          ""key"":2251799813685259,          ""processInstanceKey"":2251799813685252,          ""processDefinitionKey"":2251799813685251,          ""startDate"":""2024-12-16T09:49:18.537+0000"",          ""endDate"":""2024-12-16T09:49:18.537+0000"",          ""flowNodeId"":""endEvent_1c10629b-0d86-4a5b-a08b-37841ea40e8e"",          ""flowNodeName"":""end"",          ""type"":""END_EVENT"",          ""state"":""COMPLETED"",          ""incident"":false,          ""tenantId"":""<default>""       }    ],    ""sortValues"":[       2251799813685259    ],    ""total"":3 } ```  ### Environment (required on creation)  <!-- Please provide details about the environment you were in when the problem occurred. --> - OS: [e.g. MacOS] - Browser: [e.g. chrome, safari] - Operate Version: `8.7.0-alpha2` - Database: Elasticsearch with Camunda Exporter  ### Rootcause (required on prioritization)  ### Solution ideas  ### Additional context  <!-- Please add any other context about the problem. Here you can also provide us some data that you used while the bug happen like **json** file or specific **BPMN**. -->  ### Handover Dev to QA (required before manual testing)  <!-- To be filled out by the implementation DRI so that the QA tester can efficiently test the feature --> - Resources:   <!-- e.g. BPMN/DMN models, documentation, REST API endpoints + example payload, example projects, etc--> - Versions to validate:   <!-- Share the versions of components that contain the implemented change; include versions of dependencies as necessary; in case of Docker images, add the concrete image names and tags, e.g. camunda/operate:8.7.0-alpha1 --> - Release version (in which version this feature will be released)   <!-- Add here -->  ### Link to the test case  <!-- please add test case link for this bug if there is any if not after testing QA will  create a test case for it and add it here. -->  ### Links  <!-- - https://jira.camunda.com/browse/SUPPORT-12398 -->  ```[tasklist] ### Pull Requests ```  ",2024-12-16T09:59:28+00:00,2025-03-18T14:09:05+00:00,3,https://github.com/camunda/camunda/issues/26115,26108.0,2024-12-17T12:30:36+00:00,https://github.com/camunda/camunda/pull/26108,0,4,0,4,14,27,0,41,26.51888888888889,kind/bug;component/operate,False,True,normal,configuration,"[{""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/containers/CamundaContainer.java"", ""lines_added"": 6, ""lines_deleted"": 15, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/main/java/io/camunda/process/test/impl/runtime/ContainerRuntimeEnvs.java"", ""lines_added"": 8, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-java/src/test/java/io/camunda/process/test/api/CamundaProcessTestExtensionIT.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""testing/camunda-process-test-spring/src/test/java/io/camunda/process/test/api/CamundaSpringProcessTestListenerIT.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",test,False
helidon-io/helidon,9694,4.x fix status from deadlock health check if invoking the MBean fails,"### Description Resolves #9691   Release Note ____ Helidon's built-in deadlock health check now reports the state `ERROR` instead of `DOWN` if the invocation of the MBean method to retrieve deadlocked thread IDs throws an exception. (It used to report `DOWN`.)   Reporting `ERROR` is more accurate, and in such cases Helidon returns the HTTP status 500 for the health response. This lets a deployment environment, such as Kubernetes, allow the pod to continue running. In such error cases Helidon (as it has for some time) logs a `TRACE`-level message reporting the exception it encountered. ____  The code for the deadlock health check used to report `DOWN` if it failed to invoke the threads MBean to retrieve the list of deadlocked thread IDs. (The method returns null if no deadlocks exist.)  This PR changes that code to report `ERROR` instead of `DOWN`. By reporting `DOWN` the previous code incorrectly implied that there was a deadlock and the server instance should be considered ""unwell."" In fact, in that case the health check code does not know whether there is or is not a deadlock because of the error invoking the MBean method and it should report that way.  The PR also adds a test.  ### Documentation No impact.",2025-01-24T23:09:54+00:00,2025-01-27T17:19:52+00:00,0,https://github.com/helidon-io/helidon/pull/9694,9694.0,2025-01-27T17:19:52+00:00,https://github.com/helidon-io/helidon/pull/9694,0,2,0,2,24,9,0,33,66.1661111111111,OCA Verified,False,True,normal,ui,"[{""filename"": ""health/health-checks/src/main/java/io/helidon/health/checks/DeadlockHealthCheck.java"", ""lines_added"": 14, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""health/health-checks/src/test/java/io/helidon/health/checks/DeadlockHealthCheckTest.java"", ""lines_added"": 10, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9691,4.x Deadlock health check reports `DOWN` when it cannot access the JMX MBean to find deadlocks,"## Description  Among the built-in health checks, only the deadlock check queries a JMX MBean to find out if it knows about any deadlocks.  If for some reason that code cannot successfully invoke the MBean it reports `DOWN`. This is incorrect because the code does not know there _are_ deadlocks, only that it cannot find out whether there are or not.   If the check were to report ERROR then Helidon would return a 500 HTTP status. That's preferable to returning `DOWN`.  From a quick search it looks as if K8s, for example, warns about health responses with a status of 500 but does not discard the pod as a result.   (Another option would be to report `UP` if the MBean is there but fails when invoked. Again, the reasoning is that we do not know for sure that _there are_ deadlocks, only that we cannot tell. But this would give an overly-optimistic picture of the state of the server.) ",2025-01-24T20:35:11+00:00,2025-01-27T17:19:53+00:00,0,https://github.com/helidon-io/helidon/issues/9691,9764.0,2025-02-10T20:42:57+00:00,https://github.com/helidon-io/helidon/pull/9764,0,2,0,2,24,9,0,33,408.12944444444446,bug;health;4.x,False,True,normal,security,"[{""filename"": ""health/health-checks/src/main/java/io/helidon/health/checks/DeadlockHealthCheck.java"", ""lines_added"": 14, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""health/health-checks/src/test/java/io/helidon/health/checks/DeadlockHealthCheckTest.java"", ""lines_added"": 10, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
moby/swarmkit,3087,manager/state/raft/storage: fix TestCreateOpenInvalidDirFails,"- partially addresses https://github.com/moby/swarmkit/issues/2479 - relates to https://github.com/moby/swarmkit/pull/1701 (which introduced the test)   This test failed on macOS, but passed in CI:      --- FAIL: TestCreateOpenInvalidDirFails (0.00s)         walwrap_test.go:205:                 Error Trace:	walwrap_test.go:205                 Error:      	An error is expected but got nil.                 Test:       	TestCreateOpenInvalidDirFails     time=""2022-11-19T14:36:00Z"" level=info msg=""repaired WAL error"" error=""unexpected EOF""     FAIL  It looks like this test was always broken; this test was added in 3e2cebe76b8c7e1ae67b89de3b5fa063234edc85 and running the test from that commit and with the Go version used at the time showed the test was failing;      git checkout 3e2cebe76b8c7e1ae67b89de3b5fa063234edc85      docker run -it --rm -v $(pwd):/go/src/github.com/docker/swarmkit -w /go/src/github.com/docker/swarmkit golang:1.7 sh -c 'go test -v -run TestCreateOpenInvalidDirFails ./manager/state/raft/storage/'      === RUN   TestCreateOpenInvalidDirFails     --- FAIL: TestCreateOpenInvalidDirFails (0.01s)             Error Trace:    walwrap_test.go:193         Error:		An error is expected but got nil.      FAIL     exit status 1     FAIL	github.com/docker/swarmkit/manager/state/raft/storage	0.031s  It took some digging to understand why, but when debugging the error in CI (this part of the test _expected_ an error), it became apparent:      --- FAIL: TestCreateOpenInvalidDirFails (0.00s)         walwrap_test.go:207:                 Error Trace:	walwrap_test.go:207                 Error:      	Received unexpected error:                                 mkdir /not: permission denied                 Test:       	TestCreateOpenInvalidDirFails     time=""2022-11-19T16:55:42Z"" level=info msg=""repaired WAL error"" error=""unexpected EOF""  So what happened was that;  - `walCryptor.Create()` is called, which calls `wal.Create()` (from go.etcd.io/etcd/server/v3/wal/wal.go). - `wal.Create()` creates a new `.tmp` directory at the same location as the specified path (see https://github.com/etcd-io/etcd/blob/server/v3.5.1/server/wal/wal.go#L110-L127) - in our case, the directory passed was `/not/existing/directory`, so it tries to create  `/not/existing/directory.tmp`. - which fails, because CI doesn't run as `root` and doesn't have permissions to create the path.  On macOS, running the test inside a container failed, as the tests are run as `root` inside the container. Similarly, when updating the test to use `t.TempDir()`, and using `/not/existing/directory` within that directory, the test failed as well (as there's no permissions issue when creating the `.tmp` directory inside the temp-dir).  So, in short, the test was broken from the start; calling `was.Create()` using a non-existing directory is allowed, and not an error condition. (If we _do_ want it to be an error-condition, we should implement code to disallow this).  This patch removes the broken part from the test, and updates the remaining part to use `t.TempDir()` and a directory inside that's certain to be empty.  Also renaming the test to `TestOpenInvalidDirFails`, as the test now only covers the `Open` case.  ",2022-11-19T17:37:38+00:00,2022-11-19T22:17:34+00:00,3,https://github.com/moby/swarmkit/pull/3087,3087.0,2022-11-19T22:17:34+00:00,https://github.com/moby/swarmkit/pull/3087,0,1,0,1,10,11,0,21,4.665555555555556,,False,True,critical,security,"[{""filename"": ""manager/state/raft/storage/walwrap_test.go"", ""lines_added"": 10, ""lines_deleted"": 11, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9686,4.x Meter with a colon in its name does not appear in Prometheus output,"## Environment Details * Helidon Version: 4.x * Helidon SE or Helidon MP * JDK version: * OS: * Docker version (if applicable):  ----------  ## Problem Description Registering a meter with a `:` in its name causes that meter to be excluded from the Prometheus output.  ## Steps to reproduce In the Helidon MP QuickStart app (for example) in the `GreetResource` class: 1. Add this field:        ```java    private final AtomicInteger getCounter = new AtomicInteger();    ``` 2. Add this line in the constructor:        ```java    RegistryFactory.getInstance().getRegistry(""application"").gauge(""my:Gauge"", getCounter, AtomicInteger::intValue);    ``` 3. Build and run the app.  4. Access the `/greet` endpoint. 5. Run `curl http://localhost:8080/metrics`. The output does not contain `my:Gauge`. 6. Run `curl -H ""Accept: application/json"" http://localhost:8080/metrics`. The output _does_ contain `my:Gauge`.",2025-01-24T17:46:47+00:00,2025-01-24T19:35:12+00:00,0,https://github.com/helidon-io/helidon/issues/9686,9763.0,2025-02-10T20:42:21+00:00,https://github.com/helidon-io/helidon/pull/9763,0,2,0,2,39,3,0,42,410.9261111111111,bug;metrics;4.x,False,True,normal,configuration,"[{""filename"": ""metrics/providers/micrometer/src/main/java/io/helidon/metrics/providers/micrometer/MicrometerPrometheusFormatter.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""metrics/providers/micrometer/src/test/java/io/helidon/metrics/providers/micrometer/TestPrometheusFormatting.java"", ""lines_added"": 37, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
moby/swarmkit,2909,Improve information if IsStateDirty() fails,"this was a commit that was left over in https://github.com/docker/swarmkit/pull/2837, and thought it could still be useful when debugging problems.  Before:      panic: unexpected field type in StoreSnapshot  After:      panic: unexpected field type in StoreSnapshot: XXX_NoUnkeyedLiteral struct ",2019-10-20T12:49:01+00:00,2022-07-21T17:37:00+00:00,5,https://github.com/moby/swarmkit/pull/2909,2909.0,2022-07-21T17:37:00+00:00,https://github.com/moby/swarmkit/pull/2909,0,1,0,1,2,1,0,3,24124.799722222226,,False,True,normal,functional,"[{""filename"": ""manager/dirty.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9670,4.x Health endpoint lost its no-cache settings from 3.x,"## Environment Details * Helidon Version: 4.x * Helidon SE or Helidon MP * JDK version: * OS: * Docker version (if applicable):  ----------  ## Problem Description See [#4241](https://github.com/helidon-io/helidon/issues/4241)  Apparently this fix in 3.x for health was lost in the conversion to observers.  ## Steps to reproduce Run a Helidon server (such as the SE or MP QuickStart app) with health support, access the health endpoint, and inspect the returned headers.  ```list < HTTP/1.1 204 No Content < Date: Mon, 20 Jan 2025 16:53:35 -0600 < Connection: keep-alive < Content-Length: 0 < ```  Compare to the headers returned from the metrics endpoint: ```list < HTTP/1.1 200 OK < Date: Mon, 20 Jan 2025 16:52:49 -0600 < Cache-Control: no-cache < Cache-Control: no-store < Cache-Control: must-revalidate < Cache-Control: no-transform < Connection: keep-alive < Content-Length: 7118 < Content-Type: text/plain < ``` ",2025-01-20T22:54:31+00:00,2025-01-21T15:59:56+00:00,0,https://github.com/helidon-io/helidon/issues/9670,9756.0,2025-02-10T16:07:37+00:00,https://github.com/helidon-io/helidon/pull/9756,0,2,0,2,56,1,0,57,497.2183333333333,bug;health;P2;4.x,False,True,minor,configuration,"[{""filename"": ""webserver/observe/health/src/main/java/io/helidon/webserver/observe/health/HealthHandler.java"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""webserver/observe/health/src/test/java/io/helidon/webserver/observe/health/TestNoCacheHeaders.java"", ""lines_added"": 53, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",webserver,False
moby/swarmkit,2722,"[18.03 backport] Avoid predefined error log, and Test if error is nil before to log it",Backport of https://github.com/docker/swarmkit/pull/2561 and https://github.com/docker/swarmkit/pull/2720 for the bump_v18.03 branch  ``` git checkout -b 18.03-backport-2719-fix-error-message-logging upstream/bump_v18.03 git cherry-pick -s -S -x f6e6e694ed86f6ca470bf67112722eb5ade17bf5 git cherry-pick -s -S -x 99a0f5842bdd05e50de0a5fd386202a8b34220f4 git push -u origin ```  cherry-pick was clean; no conflicts ,2018-08-03T16:32:39+00:00,2021-06-05T21:08:56+00:00,5,https://github.com/moby/swarmkit/pull/2722,2722.0,,https://github.com/moby/swarmkit/pull/2722,0,1,0,1,3,5,0,8,24892.604722222222,,False,True,normal,functional,"[{""filename"": ""manager/manager.go"", ""lines_added"": 3, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9587,Fix incorrect throw of exception in global tracer assignment,"### Description Resolves #9586   The OpenTelemetry tracing provider code correctly checks the runtime type of the parameter before accepting it. But after setting the global tracer that branch failed to return or otherwise skip the code which throws the exception for a _mismatch_ of the type.  The PR just adds a `return` after the successful type check and assignment of the global tracer, skipping the line that throws the exception.  The PR also adds a test.  ### Documentation No impact.",2024-12-11T20:09:58+00:00,2024-12-11T20:38:45+00:00,0,https://github.com/helidon-io/helidon/pull/9587,9587.0,2024-12-11T20:38:45+00:00,https://github.com/helidon-io/helidon/pull/9587,0,2,0,2,37,0,0,37,0.4797222222222222,OCA Verified,False,True,normal,functional,"[{""filename"": ""tracing/providers/opentelemetry/src/main/java/io/helidon/tracing/providers/opentelemetry/OpenTelemetryTracerProvider.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tracing/providers/opentelemetry/src/test/java/io/helidon/tracing/providers/opentelemetry/TestGlobalTracerAssignment.java"", ""lines_added"": 36, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9586,4.x Assigning `Tracer.global(tracer)` with the OpenTelemetry provider throws a spurious exception,"## Environment Details * Helidon Version: 4.x * Helidon SE or Helidon MP SE * JDK version: * OS: * Docker version (if applicable):  ----------  ## Problem Description The Helidon tracing API allows developer code to set the global tracer by invoking `Tracer.global(tracer)`.   Doing so when using the OpenTelemetry tracing provider causes an incorrect exception to be thrown:  ```list Exception in thread ""main"" java.lang.IllegalArgumentException: Tracer must be an instance of Helidon OpenTelemetry tracer. Please use HelidonOpenTelemetry to create such instance         at io.helidon.tracing.providers.opentelemetry.OpenTelemetryTracerProvider.global(OpenTelemetryTracerProvider.java:133)         at io.helidon.tracing.TracerProviderHelper.global(TracerProviderHelper.java:80)         at io.helidon.tracing.Tracer.global(Tracer.java:50) ``` This happens even if the provided `tracer` is indeed a `HelidonOpenTelemetry` object.  ## Steps to reproduce With a dependency on `helidon-tracing-providers-opentelemetry` and `helidon-tracing-api` add code something like this: ```java         io.opentelemetry.api.trace.Tracer otelTracer = openTelemetry.getTracer(serviceName);         Tracer helidonTracer = HelidonOpenTelemetry.create(openTelemetry, otelTracer, Map.of());         Tracer.global(helidonTracer); ``` The last line triggers the exception even though the provided tracer is of the correct type.",2024-12-11T19:59:30+00:00,2024-12-11T20:38:46+00:00,0,https://github.com/helidon-io/helidon/issues/9586,9588.0,2024-12-11T21:32:32+00:00,https://github.com/helidon-io/helidon/pull/9588,0,2,0,2,37,0,0,37,1.5505555555555557,bug;tracing;P2;4.x,False,True,minor,functional,"[{""filename"": ""tracing/providers/opentelemetry/src/main/java/io/helidon/tracing/providers/opentelemetry/OpenTelemetryTracerProvider.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tracing/providers/opentelemetry/src/test/java/io/helidon/tracing/providers/opentelemetry/TestGlobalTracerAssignment.java"", ""lines_added"": 36, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
camunda/camunda,12946,Slow processing of large multi instance collection,"**Describe the bug**  As discovered in the [recent chaos day](https://zeebe-io.github.io/zeebe-chaos/2023/06/02/Using-Large-Multi-Instance/) it seems that the current batching of large multi-instance slows the processing down by a lot. It worked in general which is great!, but it was quite slow.  It might happen that a large input collection could consume the complete processing of a partition.  In the current state, it is not clear where the issue lies, but it seems that the batch processing is limited to just 2-4 commands per batch, which slows down the creation of instances.  We should further investigate whether this issue is related to batch processing itself, or based on new instance batching. <!-- A clear and concise description of what the bug is. -->  **To Reproduce**  Use https://github.com/camunda-cloud/game-day-cookbook/tree/main/mains/large-multi-instance in order to reproduce this <!-- Steps to reproduce the behavior  If possible add a minimal reproducer code sample - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java  -->  **Expected behavior**  I somehow would expect that the instance creation is faster and makes more use of the batch processing.  <!-- A clear and concise description of what you expected to happen. --> For further details take a look at the chaos day summary. ",2023-06-02T18:40:44+00:00,2025-03-13T16:55:07+00:00,16,https://github.com/camunda/camunda/issues/12946,29310.0,2025-03-13T16:55:05+00:00,https://github.com/camunda/camunda/pull/29310,0,10,0,10,142,78,0,220,15598.239166666666,kind/bug;severity/high;support;component/engine;component/stream-platform;component/zeebe;target:2025-may-patch;target:8.8,False,True,normal,ui,"[{""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/processinstance/ProcessInstanceBatchActivateProcessor.java"", ""lines_added"": 41, ""lines_deleted"": 37, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/processing/processinstance/ProcessInstanceBatchTerminateProcessor.java"", ""lines_added"": 40, ""lines_deleted"": 34, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/appliers/EventAppliers.java"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/immutable/ElementInstanceState.java"", ""lines_added"": 21, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/instance/DbElementInstanceState.java"", ""lines_added"": 15, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/processing/bpmn/activity/AdHocSubProcessTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/processing/bpmn/boundary/BoundaryEventTest.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/engine/src/test/java/io/camunda/zeebe/engine/processing/bpmn/escalation/EscalationEventTest.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""zeebe/protocol/src/main/java/io/camunda/zeebe/protocol/record/intent/ProcessInstanceBatchIntent.java"", ""lines_added"": 14, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""zeebe/qa/integration-tests/src/test/java/io/camunda/zeebe/it/processing/MultiInstanceLargeInputCollectionTest.java"", ""lines_added"": 5, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",state,False
camunda/camunda,28833,Camunda Exporter need to respect old Zeebe indices only for brownfield detection,"## Description  We recently fixed some issues with the detection mechanism https://github.com/camunda/camunda/pull/28105/files to make sure that if there are no Zeebe ES indices to assume we are in greenfield.  This is in general only slightly correct, as for Optimize we still need to run the ES Exporter. If we have only 8.8 indices than it is fine to assume green field in the detection. But if we have 8.7 indices we can assume brownfield  We need to adjust the detection mechanism to consider this   Impact/Severity: **mid** as we are more pessimistic right now, so no data corruption happens, but we might block forever if no importers are running, and we wait for them",2025-02-26T15:12:21+00:00,2025-03-13T11:26:09+00:00,0,https://github.com/camunda/camunda/issues/28833,29446.0,2025-03-13T11:26:08+00:00,https://github.com/camunda/camunda/pull/29446,0,2,0,2,43,8,0,51,356.2297222222222,kind/bug;kind/toil;severity/mid;impact/medium,False,True,normal,database,"[{""filename"": ""zeebe/exporters/camunda-exporter/src/main/java/io/camunda/exporter/CamundaExporter.java"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""zeebe/exporters/camunda-exporter/src/test/java/io/camunda/exporter/CamundaExporterIT.java"", ""lines_added"": 38, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
helidon-io/helidon,9538,OTEL Context not being propagated to @Asynchronous component,"## Environment Details * Helidon Version: 4.1.4 * Helidon MP * OpenJDK 64-Bit Server VM (build 23+37-2369, mixed mode, sharing) * OS: Ubuntu 24.04.1 LTS ----------  ## Problem Description We want to propagate OpenTelemetry context to an `@Asynchronous` component, but it looks like no context is propagated. `Span.current()` reports `Optional.empty()`. ",2024-11-27T11:01:18+00:00,2024-12-05T15:21:19+00:00,1,https://github.com/helidon-io/helidon/issues/9538,9555.0,2024-12-05T15:21:10+00:00,https://github.com/helidon-io/helidon/pull/9555,0,2,0,2,29,6,0,35,196.3311111111111,bug;P2;4.x;telemetry,False,True,minor,ui,"[{""filename"": ""fault-tolerance/fault-tolerance/src/main/java/io/helidon/faulttolerance/FaultTolerance.java"", ""lines_added"": 7, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""fault-tolerance/fault-tolerance/src/test/java/io/helidon/faulttolerance/AsyncTest.java"", ""lines_added"": 22, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
camunda/camunda,29404,Ad-hoc subprocess intra tree path values are not cached,"### Description (required on creation)  * With 8.7 we are adding support for ad-hoc subprocesses (e.g. https://github.com/camunda/camunda/issues/28304); Similar to regular subprocesses, we need to treat it as a container type in the importer * Applies to main (8.7 importer) and 8.7 (not 8.6 importer, because we only support adhoc subprocesses from 8.7)  ### Steps to reproduce (required on creation)  1. Deploy a process model with an ad-hoc subprocess 1. Execute activities within it  ### Current behavior (required on creation)  * The importer makes a tree path query for every contained flow node instances, leading to a performance degradation  ### Expected behavior (required on creation)  * The importer resolves the intra tree path of the ad hoc subprocess from the tree path cache  ### Environment (required on creation)  ### Root cause (required on prioritization)  * Ad hoc subprocess is not considered a container node here: https://github.com/camunda/camunda/blob/762c0f8a726f1957bfa8f890d656fd380a690a47/operate/importer-8_7/src/main/java/io/camunda/operate/zeebeimport/v8_7/processors/processors/fni/FNITransformer.java#L38-L43  ### Solution ideas  * Add adhoc subprocess to enum set  ### Additional context  <!-- Please add any other context about the problem. Here you can also provide us some data that you used while the bug happen like **json** file or specific **BPMN**. -->  ### Handover Dev to QA (required before manual testing)  <!-- To be filled out by the implementation DRI so that the QA tester can efficiently test the feature --> - Resources:   <!-- e.g. BPMN/DMN models, documentation, REST API endpoints + example payload, example projects, etc--> - Versions to validate:   <!-- Share the versions of components that contain the implemented change; include versions of dependencies as necessary; in case of Docker images, add the concrete image names and tags, e.g. camunda/operate:8.7.0-alpha1 --> - Release version (in which version this feature will be released)   <!-- Add here -->  ### Link to the test case  <!-- please add test case link for this bug if there is any if not after testing QA will  create a test case for it and add it here. -->  ### Links  <!-- - Link to JIRA support case -->  ```[tasklist] ### Pull Requests - [ ] main: https://github.com/camunda/camunda/pull/29485 - [ ] 8.7: https://github.com/camunda/camunda/pull/29405 ```  ",2025-03-10T17:06:23+00:00,2025-03-13T09:06:06+00:00,0,https://github.com/camunda/camunda/issues/29404,29485.0,2025-03-12T15:24:59+00:00,https://github.com/camunda/camunda/pull/29485,0,2,0,2,16,3,0,19,46.31,kind/bug;component/operate,False,True,normal,configuration,"[{""filename"": ""operate/importer-8_7/src/main/java/io/camunda/operate/zeebeimport/v8_7/processors/processors/fni/FNITransformer.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""operate/importer-8_7/src/test/java/io/camunda/operate/zeebeimport/v8_7/processors/fni/fni/FNITransformerCacheTreePathTest.java"", ""lines_added"": 14, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
moby/swarmkit,2789,"Fix missing return before ""success"" message, and minor rephrasing of errors/logs","There was a ""return"" missing, therefore the ""success"" message would always be logged, even in case of a failure.  Also rephrased the messages a bit;  - Use ""failed to"", as it's used in many other locations, so more consistent - Removed ""Swarm successfully ..."", as it should be clear from the rest of the message",2018-11-24T13:56:28+00:00,2019-04-12T19:22:51+00:00,2,https://github.com/moby/swarmkit/pull/2789,2789.0,2019-04-12T19:22:51+00:00,https://github.com/moby/swarmkit/pull/2789,0,1,0,1,3,2,0,5,3341.4397222222224,,False,True,normal,functional,"[{""filename"": ""node/node.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
camunda/camunda,29114,CamundaExporter on OS fails with ISM creation for multiple partitions,"## Description  Running CamundaExporter with retention enabled causes setting up an ILM on ES or ISM on OS.  While ILM creation is fine, done by multiple partitions, as it is simply updated/overridden, on OpenSearch duplicate creation fails with:  ``` io.camunda.exporter.exceptions.OpensearchExporterException: Failed to create index state management policy [shouldOpenDifferentPartitionsWithRetention] 	at io.camunda.exporter.schema.opensearch.OpensearchEngineClient.putIndexLifeCyclePolicy(OpensearchEngineClient.java:224) 	at io.camunda.exporter.schema.SchemaManager.startup(SchemaManager.java:83) 	at io.camunda.exporter.CamundaExporter.open(CamundaExporter.java:132)      [...] Caused by: org.opensearch.client.transport.httpclient5.ResponseException: method [PUT], host [http://localhost:34149], URI [_plugins/_ism/policies/shouldOpenDifferentPartitionsWithRetention], status line [HTTP/1.1 409 Conflict] {""error"":{""root_cause"":[{""type"":""status_exception"",""reason"":""index [.opendistro-ism-config/XRW-5UaZR2qXP5Pm4vJBwg] already exists""}],""type"":""status_exception"",""reason"":""index [.opendistro-ism-config/XRW-5UaZR2qXP5Pm4vJBwg] already exists""},""status"":409}  ```  ## Impact  The Camunda Exporter is failing in a loop, and not able to start.  ## Context  This was encountered in https://github.com/camunda/camunda/pull/28997 while HistoryCleanupIT was retried due to failure.",2025-03-04T08:52:50+00:00,2025-03-12T17:29:42+00:00,0,https://github.com/camunda/camunda/issues/29114,29120.0,2025-03-12T17:29:41+00:00,https://github.com/camunda/camunda/pull/29120,0,2,0,2,62,7,0,69,200.61416666666668,kind/bug;severity/high;impact/high,False,True,normal,configuration,"[{""filename"": ""zeebe/exporters/camunda-exporter/src/main/java/io/camunda/exporter/schema/opensearch/OpensearchEngineClient.java"", ""lines_added"": 10, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""zeebe/exporters/camunda-exporter/src/test/java/io/camunda/exporter/CamundaExporterIT.java"", ""lines_added"": 52, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
moby/swarmkit,2600,TestRaftLeaderLeave test failure,"Saw this CI failure recently:  ``` ?   	github.com/docker/swarmkit/manager/state	[no test files] time=""2018-04-10T19:22:55Z"" level=warning msg=""no certificate expiration specified, using default"" cluster.id=ozshsp4o3xqscrng7cnbi0cow method=""(*Server).UpdateRootCA"" module=ca testHasExternalCA=false 2018-04-10 19:22:57.921708 E | snap: failed to remove broken snapshot file /tmpfs/TestRaftEncryptionKeyRotationWait083709319/snap-v3-encrypted/0000000000000003-0000000000000008.snap 2018-04-10 19:22:58.185794 E | snap: failed to remove broken snapshot file /tmpfs/TestRaftEncryptionKeyRotationWait083709319/snap-v3-encrypted/0000000000000003-0000000000000009.snap 2018-04-10 19:23:04.060198 I | wal: segmented wal file /tmpfs/TestGCWAL091763934/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created 2018-04-10 19:23:04.067474 I | wal: segmented wal file /tmpfs/TestGCWAL743283743/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created 2018-04-10 19:23:04.074972 I | wal: segmented wal file /tmpfs/TestGCWAL288698533/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created 2018-04-10 19:23:04.638134 I | wal: segmented wal file /tmpfs/TestGCWAL020502770/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created 2018-04-10 19:23:04.689130 I | wal: segmented wal file /tmpfs/TestGCWAL574682868/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created 2018-04-10 19:23:04.690575 I | wal: segmented wal file /tmpfs/TestGCWAL620039593/wal-v3-encrypted/0000000000000001-000000000000000d.wal is created --- FAIL: TestRaftLeaderLeave (10.22s) 	Error Trace:	testutils.go:90 			raft_test.go:427 	Error:		Received unexpected error did not find a ready leader in member list 			github.com/docker/swarmkit/manager/state/raft/testutils.WaitForCluster.func1 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/manager/state/raft/testutils/testutils.go:74 			github.com/docker/swarmkit/testutils.PollFuncWithTimeout 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/testutils/poll.go:22 			github.com/docker/swarmkit/testutils.PollFunc 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/testutils/poll.go:36 			github.com/docker/swarmkit/manager/state/raft/testutils.WaitForCluster 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/manager/state/raft/testutils/testutils.go:62 			github.com/docker/swarmkit/manager/state/raft_test.TestRaftLeaderLeave 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/manager/state/raft/raft_test.go:427 			testing.tRunner 				/usr/local/go/src/testing/testing.go:746 			runtime.goexit 				/usr/local/go/src/runtime/asm_amd64.s:2337 			polling failed 			github.com/docker/swarmkit/testutils.PollFuncWithTimeout 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/testutils/poll.go:28 			github.com/docker/swarmkit/testutils.PollFunc 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/testutils/poll.go:36 			github.com/docker/swarmkit/manager/state/raft/testutils.WaitForCluster 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/manager/state/raft/testutils/testutils.go:62 			github.com/docker/swarmkit/manager/state/raft_test.TestRaftLeaderLeave 				/home/ubuntu/.go_workspace/src/github.com/docker/swarmkit/manager/state/raft/raft_test.go:427 			testing.tRunner 				/usr/local/go/src/testing/testing.go:746 			runtime.goexit 				/usr/local/go/src/runtime/asm_amd64.s:2337 		  FAIL coverage: 70.2% of statements FAIL	github.com/docker/swarmkit/manager/state/raft	40.667s make: *** [coverage] Error 1  cd ""$WORKDIR"" && TMPDIR=/tmpfs make ci returned exit code 2 ```  Wondering if this could be related to the election tick increase (maybe the test fails more spuriously?) in which case maybe we can just increase the timeout for the test?",2018-04-10T19:26:41+00:00,2018-10-18T10:03:29+00:00,2,https://github.com/moby/swarmkit/issues/2600,2762.0,2018-10-17T18:59:46+00:00,https://github.com/moby/swarmkit/pull/2762,0,3,0,3,13,4,0,17,4559.551388888889,area/tests,False,True,critical,networking,"[{""filename"": ""agent/agent_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""manager/orchestrator/testutils/testutils.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""manager/state/raft/testutils/testutils.go"", ""lines_added"": 11, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
camunda/camunda,28393,Content hash is always the same,"**Describe the bug**  When uploading a document to any store type, the API response provides a `contentHash`. The value seems to always be the same, regardless of the file or store type used.  **To Reproduce**  1. Upload a file using any store type via API 2. Check the `contentHash` in the API response 3. Upload a different file using the same or a different store type 4. Check the `contentHash` in the API response  **Expected behavior**  The `contentHash` in the 2nd API response should be different from the one in the 1st response  **Actual behavior**  The `contentHash` in the 2nd API response is the same as the one in the 1st response  **Environment:** - OS: any - Zeebe Version: 8.8, 8.7 ",2025-02-19T12:00:40+00:00,2025-03-10T13:10:57+00:00,0,https://github.com/camunda/camunda/issues/28393,29346.0,2025-03-10T13:08:24+00:00,https://github.com/camunda/camunda/pull/29346,1,9,0,10,234,111,6,339,457.12888888888887,kind/bug;component/document-handling,False,True,normal,functional,"[{""filename"": ""document/store/pom.xml"", ""lines_added"": 6, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""document/store/src/main/java/io/camunda/document/store/DocumentHashProcessor.java"", ""lines_added"": 0, ""lines_deleted"": 37, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/main/java/io/camunda/document/store/InputStreamHashCalculator.java"", ""lines_added"": 39, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/main/java/io/camunda/document/store/aws/AwsDocumentStore.java"", ""lines_added"": 32, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/main/java/io/camunda/document/store/gcp/GcpDocumentStore.java"", ""lines_added"": 12, ""lines_deleted"": 21, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/main/java/io/camunda/document/store/localstorage/LocalStorageDocumentStore.java"", ""lines_added"": 20, ""lines_deleted"": 16, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/test/java/io/camunda/document/store/InputStreamHashCalculatorTest.java"", ""lines_added"": 45, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/test/java/io/camunda/document/store/aws/AwsDocumentStoreTest.java"", ""lines_added"": 34, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/test/java/io/camunda/document/store/gcp/GcpDocumentStoreTest.java"", ""lines_added"": 39, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""document/store/src/test/java/io/camunda/document/store/localstorage/LocalStorageDocumentStoreTest.java"", ""lines_added"": 7, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
camunda/camunda,26935,[backport stable/8.6] fix 19883 bug on REST connection pooling (#26317),## Related issues  backports #26317  relates https://github.com/camunda/camunda/issues/19883 ,2025-01-15T10:55:44+00:00,2025-01-15T15:49:45+00:00,3,https://github.com/camunda/camunda/pull/26935,26935.0,2025-01-15T15:49:45+00:00,https://github.com/camunda/camunda/pull/26935,0,3,0,3,37,13,0,50,4.900277777777778,backport main;component/zeebe,False,True,normal,networking,"[{""filename"": ""clients/java/src/main/java/io/camunda/zeebe/client/impl/http/ApiEntityConsumer.java"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/ResponseMapper.java"", ""lines_added"": 31, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""zeebe/qa/integration-tests/src/test/java/io/camunda/zeebe/it/client/command/LongPollingActivateJobsTest.java"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",rest;http,True
camunda/camunda,17147,Retention policy is not applied to existing indices when enabled,"### Describe the bug  If you run the exporter with ILM disabled and enable it later on, previously existing indices don't get the policy applied. This leads to those indices staying ""forever"" and require manual intervention, e.g. reapplication of the policy to those.  ### To Reproduce  Steps to reproduce the behavior: 1. Run Operate with the Elasticsearch/OpenSearch exporter and with disabled ILM/ISM 2. Let it run and create indices 3. Enable the ILM/ISM and restart 4. Already existing indices lack the policy  ### Expected behavior The policy needs to be applied to all existing indices when enabled. When policy is disabled, it should also remove it from all the indices.  (Refer to Zeebe issue as mentioned below for more context)  ### Environment - 8.2.0+  ### Additional context Issue as reported from incident: https://github.com/camunda/camunda-platform/issues/486#issuecomment-2014024612 Zeebe issue and fix: https://github.com/camunda/zeebe/issues/16720  ---------------------------------------------------------------------------------------------  <!-- As the creator of the issue, you don't have to fill anything below this line, but the assignee will take care of this as part. -->  ### Acceptance Criteria  <!-- the assignee will fill the Acceptance Criteria. -->  ### Definition of Ready - Checklist  <!-- the assignee will check the DOR. --> - [ ] The bug has been reproduced by the assignee in the environment compatible with the provided one; otherwise, the issue is closed with a comment - [ ] The issue has a meaningful title, description, and testable acceptance criteria - [ ] The issue has been labeled with an appropriate `Bug-area` label - [ ] Necessary screenshots, screen recordings, or files are attached to the bug report  For UI changes required to solve the bug:  - [ ] Design input has been collected by the assignee  ### Implementation  #### :mag: Root Cause Analysis  <!-- Explain the underlying cause for the issue. -->  #### :thought_balloon: Proposed Solution  <!-- Provide a high level overview of the proposed solution. (Technical details will be available in the associated PR) -->  ### :point_right: Handover Dev to QA  <!--As a team, we have settled on a checklist to remind the DRI what information to provide to help the QA Engineer perform a frictionless and targeted QA test. The information requested by the checklist can be added before review/moving the ticket to the QA test column as a comment on the ticket.--> - Changed components: - Side effects on other components: - Handy resources:   BPMN/DMN models, plugins, scripts, REST API endpoints + example payload, etc :   <!-- Add here --> - Example projects:  <!-- Add here --> - Commands/Steps needed to test; Versions to validate:  <!-- Add here --> - Docker file / HELM chart : in case that it needed to be tested via docker share the version contain the fixed along with version of other services .  <!--elasticsearch: 16.2.2 identitiy:alpha3 zeebe:alpha3 Operate:alpha3 tasklist:alpha3--> - Release version ( in which version this fixed/feature will be released):  <!-- Add here -->  ### :green_book: Link to the test case  <!-- please add test case link for this bug if there is any if not after testing QA will  create a test case for it and add it here. --> ",2024-03-27T07:24:21+00:00,2025-03-10T08:50:46+00:00,6,https://github.com/camunda/camunda/issues/17147,20617.0,2024-08-07T09:51:49+00:00,https://github.com/camunda/camunda/pull/20617,1,8,0,9,0,257,6,251,3194.4577777777777,kind/bug;support;backport stable/8.3;backport stable/8.4;component/operate;backport stable/8.5;operate:prioritized-backlog;version:8.6.0,False,True,normal,ui,"[{""filename"": ""operate/schema/pom.xml"", ""lines_added"": 0, ""lines_deleted"": 6, ""file_type"": ""config""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/schema/SchemaManager.java"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/schema/SchemaStartup.java"", ""lines_added"": 0, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/schema/elasticsearch/ElasticsearchSchemaManager.java"", ""lines_added"": 0, ""lines_deleted"": 24, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/schema/opensearch/OpensearchSchemaManager.java"", ""lines_added"": 0, ""lines_deleted"": 31, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/store/elasticsearch/RetryElasticsearchClient.java"", ""lines_added"": 0, ""lines_deleted"": 13, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/main/java/io/camunda/operate/store/opensearch/client/sync/OpenSearchISMOperations.java"", ""lines_added"": 0, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/test/java/io/camunda/operate/schema/elasticsearch/ElasticsearchSchemaManagerTest.java"", ""lines_added"": 0, ""lines_deleted"": 80, ""file_type"": ""app_code""}, {""filename"": ""operate/schema/src/test/java/io/camunda/operate/schema/opensearch/OpensearchSchemaManagerTest.java"", ""lines_added"": 0, ""lines_deleted"": 87, ""file_type"": ""app_code""}]",,False
camunda/camunda,29339,fix: Return detail for unexpected batch upload failures,"## Description  When the batch upload API is used to upload files and an unknown error occurs, the API response only contains the filename of the file that we failed to upload to the store.  ``` {     ""createdDocuments"": [],     ""failedDocuments"": [         {             ""fileName"": ""example.pdf""         }     ] } ``` However, the API for a single file upload returns some more information. ``` {     ""type"": ""about:blank"",     ""title"": ""io.camunda.document.api.DocumentError$UnknownDocumentError"",     ""status"": 500,     ""instance"": ""/v2/documents"" } ``` Given the above, I thought it would be good to at least return the information that an unknown error was recorded. ``` {     ""createdDocuments"": [],     ""failedDocuments"": [         {             ""fileName"": ""example.pdf"",             ""detail"": ""An error occurred: io.camunda.document.api.DocumentError$UnknownDocumentError""         }     ] } ``` There are logs in place that should help us troubleshoot `UnknownDocumentError`s so this detail is just an indication of what went wrong. ",2025-03-07T08:47:35+00:00,2025-03-09T11:54:36+00:00,1,https://github.com/camunda/camunda/pull/29339,29339.0,2025-03-09T11:54:36+00:00,https://github.com/camunda/camunda/pull/29339,0,1,0,1,1,1,0,2,51.11694444444444,component/zeebe;component/c8-api;component/document-handling;backport stable/8.7,False,True,normal,functional,"[{""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/ResponseMapper.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",rest,False
camunda/camunda,29194,Check provided tenants on job activation,"**Describe the bug**  The C8 REST API to activate jobs allows activating jobs for other tenants.  **To Reproduce**  1. Enable multi-tenancy and define two tenants, `A` and `B` 2. Register an API client `XYZ` for tenant `A` 3. Deploy a model for tenant `B` that contains a service task with job type `foo` and create an instance of that model 4. Using a token for client `XYZ`, call the `POST /v2/jobs/activation` endpoint with job type `foo` and `tenantIds` as `[""B""]` 5. Receive a response with an activated job for type `foo` and tenant `B`  **Expected behavior**  No jobs are activated  **Environment:** - OS: all - Zeebe Version: 8.6 - Configuration: multi-tenancy enabled ",2025-03-05T07:15:45+00:00,2025-03-07T11:14:47+00:00,0,https://github.com/camunda/camunda/issues/29194,29250.0,2025-03-07T11:14:46+00:00,https://github.com/camunda/camunda/pull/29250,0,9,0,9,294,97,0,391,51.98361111111111,kind/bug;component/zeebe,False,True,normal,configuration,"[{""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/RequestMapper.java"", ""lines_added"": 16, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/controller/JobController.java"", ""lines_added"": 6, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/validator/ErrorMessages.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/main/java/io/camunda/zeebe/gateway/rest/validator/MultiTenancyValidator.java"", ""lines_added"": 40, ""lines_deleted"": 18, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/test/java/io/camunda/zeebe/gateway/impl/job/LongPollingActivateJobsRestTest.java"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/test/java/io/camunda/zeebe/gateway/rest/controller/JobControllerLongPollingTest.java"", ""lines_added"": 105, ""lines_deleted"": 50, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/test/java/io/camunda/zeebe/gateway/rest/controller/JobControllerRoundRobinTest.java"", ""lines_added"": 9, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/test/java/io/camunda/zeebe/gateway/rest/controller/JobControllerTest.java"", ""lines_added"": 100, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""zeebe/gateway-rest/src/test/java/io/camunda/zeebe/gateway/rest/controller/MessageControllerTest.java"", ""lines_added"": 14, ""lines_deleted"": 14, ""file_type"": ""app_code""}]",rest,False
moby/swarmkit,436,raft: incorrect cluster state on node stop (using Ctrl-C),"First time seeing this happen but on a 3 nodes cluster, I stopped the leader and tried to restart it after a reelection occured and got:  ``` $ sudo swarmd manager init INFO[0000] 140a1e949e67049a became follower at term 3    INFO[0000] newRaft 140a1e949e67049a [peers: [], term: 3, commit: 3, applied: 0, lastindex: 4, lastterm: 2]  INFO[0000] 140a1e949e67049a became follower at term 3    INFO[0000] newRaft 140a1e949e67049a [peers: [], term: 3, commit: 3, applied: 0, lastindex: 4, lastterm: 2]  INFO[0000] listening                                     addr=[::]:4242 proto=tcp INFO[0001] raft.node: 140a1e949e67049a elected leader 6240b3d5b7b8e560 at term 3  Error: incorrect cluster state Usage:   swarmd manager [flags]  Flags:       --join-cluster string   Join cluster with a node at this address       --listen-addr string    Listen address (default ""0.0.0.0:4242"")  Global Flags:   -l, --log-level string   Log level (options ""debug"", ""info"", ""warn"", ""error"", ""fatal"", ""panic"") (default ""info"")   -d, --state-dir string   State directory (default ""/var/lib/docker/cluster"")   -t, --token string       Specifies the token necessary to join the cluster securely  FATA[0001] incorrect cluster state ```  What's interesting is that it catches up to the logs up to now (seeing the new leader), but crashes when trying to recover from the state.  I'm not sure we can avoid it in case of a corrupt state but maybe this is a symptom of another kind of error on recovery.  /cc @aaronlehmann  ",2016-04-25T21:22:44+00:00,2016-05-06T23:15:21+00:00,16,https://github.com/moby/swarmkit/issues/436,1539.0,2016-09-15T04:59:50+00:00,https://github.com/moby/swarmkit/pull/1539,0,2,0,2,9,7,0,16,3415.6183333333333,kind/bug;priority/P1;area/raft,False,True,critical,functional,"[{""filename"": ""agent/agent.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""agent/session.go"", ""lines_added"": 7, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
SatoshiPortal/cyphernode,66,Fixed resolve conflict bug at new setup,,2019-02-16T11:39:05+00:00,2019-02-16T15:46:15+00:00,0,https://github.com/SatoshiPortal/cyphernode/pull/66,66.0,2019-02-16T15:46:15+00:00,https://github.com/SatoshiPortal/cyphernode/pull/66,0,1,0,1,1,1,0,2,4.1194444444444445,,False,True,normal,functional,"[{""filename"": ""install/generator-cyphernode/generators/app/index.js"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",generators,False
SatoshiPortal/cyphernode,65,Fixed prune size bug,,2019-02-09T13:17:50+00:00,2019-02-09T13:18:16+00:00,0,https://github.com/SatoshiPortal/cyphernode/pull/65,65.0,2019-02-09T13:18:16+00:00,https://github.com/SatoshiPortal/cyphernode/pull/65,0,1,0,1,1,1,0,2,0.0072222222222222,,False,True,normal,functional,"[{""filename"": ""install/generator-cyphernode/generators/app/index.js"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",generators,False
SatoshiPortal/cyphernode,51,v0.1.1 bugfixes,Fixes in installation and added data in logs.,2019-01-21T16:12:52+00:00,2019-01-21T16:13:04+00:00,0,https://github.com/SatoshiPortal/cyphernode/pull/51,51.0,2019-01-21T16:13:04+00:00,https://github.com/SatoshiPortal/cyphernode/pull/51,0,2,7,9,126,55,0,82,0.0033333333333333,,False,True,normal,database,"[{""filename"": ""README.md"", ""lines_added"": 32, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""api_auth_docker/trace.sh"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""build.sh"", ""lines_added"": 21, ""lines_deleted"": 17, ""file_type"": ""other""}, {""filename"": ""dist/setup.sh"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""install/generator-cyphernode/generators/app/index.js"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""install/generator-cyphernode/generators/app/prompters/999_installer.js"", ""lines_added"": 50, ""lines_deleted"": 30, ""file_type"": ""app_code""}, {""filename"": ""otsclient_docker/script/trace.sh"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""proxy_docker/app/script/trace.sh"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""pycoin_docker/script/trace.sh"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}]",proxy_docker;api_auth_docker;generators,True
camunda/camunda,28109,MessagingMetricsImpl is not thread-safe,"MessagingMetricsImpl must be thread-safe, because it's used by different threads. This may cause random messages drop, especially during startup ",2025-02-13T09:47:18+00:00,2025-02-13T10:45:22+00:00,0,https://github.com/camunda/camunda/issues/28109,28114.0,2025-03-04T09:04:00+00:00,https://github.com/camunda/camunda/pull/28114,1,3,0,4,21,10,5,26,455.2783333333333,kind/bug;component/zeebe;version:8.8.0-alpha2;version:8.6.11,False,True,normal,functional,"[{""filename"": ""zeebe/atomix/cluster/pom.xml"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""zeebe/atomix/cluster/src/main/java/io/atomix/cluster/messaging/impl/MessagingMetricsImpl.java"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""zeebe/atomix/cluster/src/main/java/io/atomix/cluster/messaging/impl/NettyDnsMetrics.java"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""zeebe/atomix/cluster/src/main/java/io/atomix/cluster/protocol/SwimMembershipProtocolMetrics.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
camunda/camunda,26926,Message correlation on a non-interrupting message event is not idempotent,"## Describe the bug  <!-- A clear and concise description of what the bug is. For existing Camunda Customers, please also use our [Support Channels](https://camunda.com/services/support/) to get priority assistance -->  When a message is published with a message id we[ claim that it is only processed once](https://docs.camunda.io/docs/components/concepts/messages/#message-uniqueness). This is not always true. When we have non-interrupting message event like in the process below there is a chance it gets correlated multiple times by a single message.  <img width=""442"" alt=""Image"" src=""https://github.com/user-attachments/assets/864552be-db84-4bee-879d-5d01cbbf6c63"" />  **To Reproduce**  <!-- Steps to reproduce the behavior  If possible add a minimal reproducer code sample - when using the Java client: https://github.com/zeebe-io/zeebe-test-template-java  -->  I've reproduced it in a test. For this test I had to ""hack"" some things in the code as I wasn't sure how else to do it.  First we need to modify the timeout and interval of the `MessageObserver`:  ```java   public static final Duration SUBSCRIPTION_TIMEOUT = Duration.ofSeconds(1);   public static final Duration SUBSCRIPTION_CHECK_INTERVAL = Duration.ofSeconds(1); ```  Next we need to make it possible to create a process instance on a specific partition in our test case. We add a method to the `ProcessInstanceClient`:  ```java     public long createOnPartition(final int partitionId) {       final long position =           writer.writeCommandOnPartition(               partitionId, ProcessInstanceCreationIntent.CREATE, processInstanceCreationRecord);        final var resultingRecord = expectation.apply(position);       return resultingRecord.getValue().getProcessInstanceKey();     } ```  Finally we run the test case below:  ```java   @Test   public void test() throws InterruptedException { // TODO rename     // given     final var processId = Strings.newRandomValidBpmnId();     final var messageName = ""event_message"";     final var correlationKey = CORRELATION_KEYS.get(START_PARTITION_ID);     final var eventSubProcessStartId = ""eventSubProcessStart"";     engine         .deployment()         .withXmlResource(             Bpmn.createExecutableProcess(processId)                 .eventSubProcess(                     ""subprocess"",                     s ->                         s.startEvent(eventSubProcessStartId)                             .interrupting(false)                             .message(                                 m ->                                     m.name(messageName)                                         .zeebeCorrelationKeyExpression(""correlationKey""))                             .userTask()                             .endEvent())                 .startEvent()                 .userTask()                 .endEvent()                 .done())         .deploy();     final var processInstanceKey =         engine             .processInstance()             .ofBpmnProcessId(processId)             .withVariable(""correlationKey"", correlationKey)             .createOnPartition(2);      // when     engine.pauseProcessing(2);     engine         .message()         .withId(""messageId"")         .withName(messageName)         .withCorrelationKey(correlationKey)         .withTimeToLive(Duration.ofMinutes(30))         .onPartition(1)         .publish();      // This sleep can replaced by waiting for multiple CORRELATE commands on partition 2     Thread.sleep(3000);     engine.resumeProcessing(2);      // then     RecordingExporter.processMessageSubscriptionRecords(ProcessMessageSubscriptionIntent.CREATED)         .withMessageName(messageName)         .withCorrelationKey(correlationKey)         .withPartitionId(2)         .await();     assertThat(             RecordingExporter.processInstanceRecords(ProcessInstanceIntent.ELEMENT_ACTIVATED)                 .withProcessInstanceKey(processInstanceKey)                 .withElementId(eventSubProcessStartId)                 .limit(2)                 .count())         .isEqualTo(1);   } ```  The test will start a process instance on partition 2. The process will have a subscription for messages due to the non-interrupting message start event in the event subprocess. The correlation key we use ensures the messasge gets published to partition 1. It's important this is a different partition!  We simulate a scenario where partition 2 is very busy and it takes a while to process new commands. We do this by pausing processing on partition 2. This means new commands can be written to the log, but they won't be processed yet.  Whilst partition 2 is working through the backlog of commands on the queue, the `PendingMessageSubscriptionChecker` of partition 1 will keep sending new `CORRELATE` command to partition 2. These all end up on the log.  Once partition 2 catches up with the `CORRELATE` commands (we resume processing) it will process them all. Since the event is non-interrupting the subscription is not cleared after the first correlation. Instead all of correlations send by partition 1 correlate and trigger the message event. As a result a single published message is correlated multiple times to the same event.   ## Expected behavior  <!-- A clear and concise description of what you expected to happen. --> The non-interrupting event should only trigger a single time per message. Subsequent correlations on the log should not succeed.  ## Log/Stacktrace  <!-- If possible add the full stacktrace or Zeebe log which contains the issue. -->  <details><summary>Logged records from the test</summary>  <p>  ``` 1 C DPLY         CREATE         - #01-> -1    -1 -  1 E PROC         CREATED        - #02->#01 P1K01 - process.xml -> ""id-6a03..ac4affc"" (version:1) 2 C DPLY         CREATE         - #01-> -1 P1K02 - process.xml 3 C DPLY         CREATE         - #01-> -1 P1K02 - process.xml 1 E DPLY         CREATED        - #03->#01 P1K02 - process.xml 1 E DSTR         STARTED        - #04->#01 P1K02 - DEPLOYMENT CREATE on partition 1 1 E DSTR         DISTRIBUTING   - #05->#01 P1K02 - DEPLOYMENT CREATE to partition 2 1 E DSTR         DISTRIBUTING   - #06->#01 P1K02 - DEPLOYMENT CREATE to partition 3 2 E PROC         CREATED        - #02->#01 P1K01 - process.xml -> ""id-6a03..ac4affc"" (version:1) 1 C DSTR         ACKNOWLEDGE    - #07-> -1 P1K02 - DEPLOYMENT CREATE for partition 2 2 E DPLY         CREATED        - #03->#01 P1K02 - process.xml 1 E DSTR         ACKNOWLEDGED   - #08->#07 P1K02 - DEPLOYMENT CREATE for partition 2 3 E PROC         CREATED        - #02->#01 P1K01 - process.xml -> ""id-6a03..ac4affc"" (version:1) 1 C DSTR         ACKNOWLEDGE    - #09-> -1 P1K02 - DEPLOYMENT CREATE for partition 3 3 E DPLY         CREATED        - #03->#01 P1K02 - process.xml 1 E DSTR         ACKNOWLEDGED   - #10->#09 P1K02 - DEPLOYMENT CREATE for partition 3 1 E DSTR         FINISHED       - #11->#09 P1K02 - DEPLOYMENT CREATE on partition 1 2 C CREA         CREATE         - #04-> -1    -1 - new <process ""id-6a03..ac4affc""> (default start)  with variables: {correlationKey=item-2} 2 E VAR          CREATED        - #05->#04 P2K02 - correlationKey->""item-2"" in <process [P2K01]> 1 C MSG_SUB      CREATE         - #12-> -1    -1 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 C PI           ACTIVATE       - #06->#04 P2K01 - PROCESS ""id-6a03..ac4affc"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E CREA         CREATED        - #07->#04 P2K03 - new <process ""id-6a03..ac4affc""> (default start)  with variables: {correlationKey=item-2} 2 E PI           ACTIVATING     - #08->#04 P2K01 - PROCESS ""id-6a03..ac4affc"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #09->#04 P2K01 - PROCESS ""id-6a03..ac4affc"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #10->#04    -1 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #11->#04 P2K04 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #12->#04 P2K04 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           COMPLETE       - #13->#04 P2K04 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           COMPLETING     - #14->#04 P2K04 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_MSG_SUB CREATING       - #15->#04 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 E PI           COMPLETED      - #16->#04 P2K04 - START_EVENT ""startEv..6ec9b27"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           SEQ_FLOW_TAKEN - #17->#04 P2K06 - SEQUENCE_FLOW ""sequenc..a549079"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #18->#04 P2K07 - USER_TASK ""userTas..96cb32f"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #19->#04 P2K07 - USER_TASK ""userTas..96cb32f"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E JOB          CREATED        - #20->#04 P2K08 - P2K08 ""io.camunda.zeebe:userTask"" @""userTas..96cb32f""[P2K07] (BPMN_ELEMENT), 1 retries, in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #21->#04 P2K07 - USER_TASK ""userTas..96cb32f"" in <process ""id-6a03..ac4affc""[P2K01]> 1 E MSG_SUB      CREATED        - #13->#12 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 C PROC_MSG_SUB CREATE         - #22-> -1    -1 - ""event_message"" @[P2K01] in <process ?[P2K01]> (no vars) (tenant: <default>) 1 C MSG          PUBLISH        - #14-> -1    -1 - ""event_message"" correlationKey: item-2 (no vars) 1 E MSG          PUBLISHED      - #15->#14 P1K04 - ""event_message"" correlationKey: item-2 (no vars) 2 C PROC_MSG_SUB CORRELATE      - #23-> -1    -1 - ""event_message"" (inter.) correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 1 E MSG_SUB      CORRELATING    - #16->#14 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 C PROC_MSG_SUB CORRELATE      - #24-> -1    -1 - ""event_message"" (inter.) correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 C PROC_MSG_SUB CORRELATE      - #25-> -1    -1 - ""event_message"" (inter.) correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 C PROC_MSG_SUB CORRELATE      - #26-> -1    -1 - ""event_message"" (inter.) correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 E PROC_MSG_SUB CREATED        - #27->#22 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 1 C MSG_SUB      CORRELATE      - #17-> -1    -1 - ""event_message"" (inter.) @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PROC_MSG_SUB CORRELATED     - #28->#23 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 E PROC_EVNT    TRIGGERING     - #29->#23 P2K09 -  @""eventSu..ssStart""[P2K01] in <process P1K01[P2K01]> (no vars) 2 C PI           ACTIVATE       - #30->#23    -1 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #31->#23 P2K10 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #32->#23 P2K10 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #33->#23    -1 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #34->#23 P2K11 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #35->#23 P2K11 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           COMPLETE       - #36->#23 P2K11 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           COMPLETING     - #37->#23 P2K11 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_EVNT    TRIGGERED      - #38->#23 P2K09 -  @""eventSu..ssStart""[P2K11] in <process P1K01[P2K01]> (no vars) 2 E PI           COMPLETED      - #39->#23 P2K11 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           SEQ_FLOW_TAKEN - #40->#23 P2K12 - SEQUENCE_FLOW ""sequenc..36f2c26"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #41->#23 P2K13 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #42->#23 P2K13 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E JOB          CREATED        - #43->#23 P2K14 - P2K14 ""io.camunda.zeebe:userTask"" @""userTas..62687fb""[P2K13] (BPMN_ELEMENT), 1 retries, in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #44->#23 P2K13 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_MSG_SUB CORRELATED     - #45->#24 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 E PROC_EVNT    TRIGGERING     - #46->#24 P2K15 -  @""eventSu..ssStart""[P2K01] in <process P1K01[P2K01]> (no vars) 2 C PI           ACTIVATE       - #47->#24    -1 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #48->#24 P2K16 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #49->#24 P2K16 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #50->#24    -1 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #51->#24 P2K17 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 1 C MSG_SUB      CORRELATE      - #18-> -1    -1 - ""event_message"" (inter.) @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 1 E MSG_SUB      CORRELATED     - #19->#17 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #52->#24 P2K17 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           COMPLETE       - #53->#24 P2K17 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 1 E MSG_SUB      CORRELATED     - #20->#18 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           COMPLETING     - #54->#24 P2K17 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_EVNT    TRIGGERED      - #55->#24 P2K15 -  @""eventSu..ssStart""[P2K17] in <process P1K01[P2K01]> (no vars) 2 E PI           COMPLETED      - #56->#24 P2K17 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           SEQ_FLOW_TAKEN - #57->#24 P2K18 - SEQUENCE_FLOW ""sequenc..36f2c26"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #58->#24 P2K19 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #59->#24 P2K19 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E JOB          CREATED        - #60->#24 P2K20 - P2K20 ""io.camunda.zeebe:userTask"" @""userTas..62687fb""[P2K19] (BPMN_ELEMENT), 1 retries, in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #61->#24 P2K19 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 1 C MSG_SUB      CORRELATE      - #21-> -1    -1 - ""event_message"" (inter.) @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 1 E MSG_SUB      CORRELATED     - #22->#21 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PROC_MSG_SUB CORRELATED     - #62->#25 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 2 E PROC_EVNT    TRIGGERING     - #63->#25 P2K21 -  @""eventSu..ssStart""[P2K01] in <process P1K01[P2K01]> (no vars) 2 C PI           ACTIVATE       - #64->#25    -1 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #65->#25 P2K22 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #66->#25 P2K22 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #67->#25    -1 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #68->#25 P2K23 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #69->#25 P2K23 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           COMPLETE       - #70->#25 P2K23 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           COMPLETING     - #71->#25 P2K23 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_EVNT    TRIGGERED      - #72->#25 P2K21 -  @""eventSu..ssStart""[P2K23] in <process P1K01[P2K01]> (no vars) 2 E PI           COMPLETED      - #73->#25 P2K23 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           SEQ_FLOW_TAKEN - #74->#25 P2K24 - SEQUENCE_FLOW ""sequenc..36f2c26"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #75->#25 P2K25 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #76->#25 P2K25 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E JOB          CREATED        - #77->#25 P2K26 - P2K26 ""io.camunda.zeebe:userTask"" @""userTas..62687fb""[P2K25] (BPMN_ELEMENT), 1 retries, in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #78->#25 P2K25 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_MSG_SUB CORRELATED     - #79->#26 P2K05 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) (tenant: <default>) 1 C MSG_SUB      CORRELATE      - #23-> -1    -1 - ""event_message"" (inter.) @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PROC_EVNT    TRIGGERING     - #80->#26 P2K27 -  @""eventSu..ssStart""[P2K01] in <process P1K01[P2K01]> (no vars) 2 C PI           ACTIVATE       - #81->#26    -1 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #82->#26 P2K28 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #83->#26 P2K28 - EVENT_SUB_PROCESS ""subprocess"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #84->#26    -1 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #85->#26 P2K29 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATED      - #86->#26 P2K29 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           COMPLETE       - #87->#26 P2K29 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           COMPLETING     - #88->#26 P2K29 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PROC_EVNT    TRIGGERED      - #89->#26 P2K27 -  @""eventSu..ssStart""[P2K29] in <process P1K01[P2K01]> (no vars) 2 E PI           COMPLETED      - #90->#26 P2K29 - START_EVENT ""eventSu..ssStart"" in <process ""id-6a03..ac4affc""[P2K01]> 1 E MSG_SUB      CORRELATED     - #24->#23 P1K03 - ""event_message"" correlationKey: item-2 @[P2K01] in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           SEQ_FLOW_TAKEN - #91->#26 P2K30 - SEQUENCE_FLOW ""sequenc..36f2c26"" in <process ""id-6a03..ac4affc""[P2K01]> 2 C PI           ACTIVATE       - #92->#26 P2K31 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E PI           ACTIVATING     - #93->#26 P2K31 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> 2 E JOB          CREATED        - #94->#26 P2K32 - P2K32 ""io.camunda.zeebe:userTask"" @""userTas..62687fb""[P2K31] (BPMN_ELEMENT), 1 retries, in <process ""id-6a03..ac4affc""[P2K01]> (no vars) 2 E PI           ACTIVATED      - #95->#26 P2K31 - USER_TASK ""userTas..62687fb"" in <process ""id-6a03..ac4affc""[P2K01]> ```  </p> </details>  In these record we see that: - P1 created a message subscription (position 13) - P1 receives a published message (position 15) - P2 has 4 `CORRELATE` commands on the log (position 23, 24, 25, 26). This is due to the `PendingMessageSubscriptionChecker` - P2 resumes processing and triggers the message event 4 times (position 38, 55, 72, 89)   This problem arose in a support case: https://jira.camunda.com/browse/SUPPORT-25086. We received some logging that clearly showcases that the extra correlations are triggered by the `PendingMessageSubscriptionChecker`:  ``` 2025-01-09 13:42:29.355 [Broker-2] [zb-fs-workers-1] [Exporter-3] INFO        io.camunda.zeebe.broker.exporter.trace - Exporting record with key 6755399441584969 at position 1258771 on partition 3 of type EVENT with value type PROCESS_MESSAGE_SUBSCRIPTION and intent CORRELATED 2025-01-09 13:42:32.139 [Broker-2] [zb-fs-workers-1] [Exporter-3] INFO        io.camunda.zeebe.broker.exporter.trace - Exporting record with key 6755399441584969 at position 1259289 on partition 3 of type EVENT with value type PROCESS_MESSAGE_SUBSCRIPTION and intent CORRELATED ```  This specific log occurred 194 times for the key `6755399441584969`, but with different positions. This indicates that the same message is correlated multiple times. Aside from that there's logging like:  ``` 2025-01-09 14:51:40.615 [Broker-2] [zb-actors-0] [AsyncProcessingScheduleActor-3] WARN        io.camunda.zeebe.broker.logstreams - Expected to find a subscription with key 11258999068657018 and message name ..., but none found. The state is inconsistent. 2025-01-09 14:51:40.615 [Broker-2] [zb-actors-0] [AsyncProcessingScheduleActor-3] WARN        io.camunda.zeebe.broker.logstreams - Expected to find a subscription with key 9007199254944655 and message name ..., but none found. The state is inconsistent. 2025-01-09 14:51:40.615 [Broker-2] [zb-actors-0] [AsyncProcessingScheduleActor-3] WARN        io.camunda.zeebe.broker.logstreams - Expected to find a subscription with key 2251799813878699 and message name ..., but none found. The state is inconsistent. ```  The only place where we log this message is [here](https://github.com/camunda/camunda/blob/fba75d7aacf19b1ebebf4a371fc96c3a2f022e3a/zeebe/engine/src/main/java/io/camunda/zeebe/engine/state/message/DbMessageSubscriptionState.java#L280-L283). This method is called in 1 place, by the `PendingMessageSubscriptionChecker`.  ## Environment: - Zeebe Version: 8.5.4 & 8.5.7, but likely exists on all versions.  ## Support cases: - https://jira.camunda.com/browse/SUPPORT-25086 - https://jira.camunda.com/browse/SUPPORT-25352",2025-01-15T09:54:52+00:00,2025-02-13T16:48:02+00:00,11,https://github.com/camunda/camunda/issues/26926,28143.0,2025-02-17T11:13:14+00:00,https://github.com/camunda/camunda/pull/28143,0,14,0,14,310,90,0,400,793.3061111111111,kind/bug;severity/mid;support;likelihood/mid;component/zeebe;version:8.3.21;version:8.7.0-alpha5;version:8.4.17;version:8.8.0-alpha2;version:8.6.10;version:8.5.16,False,True,major,networking,"[{""filename"": ""engine/src/main/java/io/camunda/zeebe/engine/processing/message/MessageSubscriptionCorrelateProcessor.java"", ""lines_added"": 49, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""engine/src/main/java/io/camunda/zeebe/engine/processing/message/PendingMessageSubscriptionChecker.java"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""engine/src/main/java/io/camunda/zeebe/engine/processing/message/ProcessMessageSubscriptionCorrelateProcessor.java"", ""lines_added"": 61, ""lines_deleted"": 40, ""file_type"": ""app_code""}, {""filename"": ""engine/src/main/java/io/camunda/zeebe/engine/processing/message/command/SubscriptionCommandSender.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""engine/src/main/java/io/camunda/zeebe/engine/state/message/DbMessageSubscriptionState.java"", ""lines_added"": 6, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/processing/bpmn/subprocess/InterruptingEventSubprocessConcurrencyTest.java"", ""lines_added"": 20, ""lines_deleted"": 12, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/processing/message/MessageCorrelationMultiplePartitionsTest.java"", ""lines_added"": 86, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/processing/message/MessageStreamProcessorTest.java"", ""lines_added"": 38, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/processing/message/command/SubscriptionCommandSenderTest.java"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/util/StreamProcessingComposite.java"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/util/StreamProcessorRule.java"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/util/client/CommandWriter.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""engine/src/test/java/io/camunda/zeebe/engine/util/client/ProcessInstanceClient.java"", ""lines_added"": 15, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test-util/src/main/java/io/camunda/zeebe/test/util/record/CompactRecordLogger.java"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
camunda/camunda,28116,Spring boot starter camunda SDK: Variable with value null causes exception,"**Describe the bug**  There is currently inconsistency in how we handle non-existing variable.  Example: In Client: 8.5.2 -> If a variable does not exist, then `@Variable` uses null In Client: 8.6.7 -> If a variable does not exist, an exception is thrown.  After the investigation here is what happened:  **Version 8.5**: `spring-boot-camunda-sdk` official support was introduced by bringing in the community project codebase as is. This was using the code below to get the variables: ``` final Object variableValue = job.getVariablesAsMap().get(paramName); ```  **Version 8.6**: we brought in the changes to the properties structure that were introduced in the community project. This was using the code below, that does throw the exception: ``` final Object variableValue = getVariable(job); ```  ```  @Override   public Object getVariable(final String name) {     final Map<String, Object> variables = getVariablesAsMap();     if (!variables.containsKey(name)) {       throw new ClientException(String.format(""The variable %s is not available"", name));     }     return getVariablesAsMap().get(name);   } ```  This was fixed in the community project [here](https://github.com/camunda-community-hub/spring-zeebe/issues/752) in order to revert this behavior.  **To Reproduce**  Create a process with a worker which uses the `Variable` annotation, for example:  ``` @Component public class DummyWorker {      @JobWorker(type = ""dummyWorker"")     public void dummyWorker(@Variable(""myVar"")String myVar) {         System.out.println(""Variable: '"" + myVar + ""'"");     } } ```  **Expected behavior**  `null` should be returned instead of throwing a `ClientException`.  **Log/Stacktrace**  Example stacktrace:  ``` io.camunda.zeebe.client.api.command.ClientException: The variable myVar is not available 	at io.camunda.zeebe.client.impl.response.ActivatedJobImpl.getVariable(ActivatedJobImpl.java:186) ~[zeebe-client-java-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.spring.client.jobhandling.parameter.VariableResolver.getVariable(VariableResolver.java:51) ~[spring-boot-starter-camunda-sdk-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.spring.client.jobhandling.parameter.VariableResolver.resolve(VariableResolver.java:36) ~[spring-boot-starter-camunda-sdk-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.spring.client.jobhandling.JobHandlerInvokingSpringBeans.lambda$createParameters$0(JobHandlerInvokingSpringBeans.java:117) ~[spring-boot-starter-camunda-sdk-8.6.8.jar:8.6.8] 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:215) ~[na:na] 	at java.base/java.util.AbstractList$RandomAccessSpliterator.forEachRemaining(AbstractList.java:722) ~[na:na] 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:570) ~[na:na] 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:560) ~[na:na] 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:636) ~[na:na] 	at java.base/java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:291) ~[na:na] 	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:656) ~[na:na] 	at java.base/java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:662) ~[na:na] 	at java.base/java.util.stream.ReferencePipeline.toList(ReferencePipeline.java:667) ~[na:na] 	at io.camunda.zeebe.spring.client.jobhandling.JobHandlerInvokingSpringBeans.createParameters(JobHandlerInvokingSpringBeans.java:117) ~[spring-boot-starter-camunda-sdk-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.spring.client.jobhandling.JobHandlerInvokingSpringBeans.handle(JobHandlerInvokingSpringBeans.java:75) ~[spring-boot-starter-camunda-sdk-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.client.impl.worker.JobRunnableFactoryImpl.executeJob(JobRunnableFactoryImpl.java:45) ~[zeebe-client-java-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.client.impl.worker.JobRunnableFactoryImpl.lambda$create$0(JobRunnableFactoryImpl.java:40) ~[zeebe-client-java-8.6.8.jar:8.6.8] 	at io.camunda.zeebe.client.impl.worker.BlockingExecutor.lambda$execute$0(BlockingExecutor.java:50) ~[zeebe-client-java-8.6.8.jar:8.6.8] 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[na:na] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[na:na] 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[na:na] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[na:na] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[na:na] 	at java.base/java.lang.Thread.run(Thread.java:1575) ~[na:an] ``` ",2025-02-13T11:01:31+00:00,2025-02-17T16:25:03+00:00,0,https://github.com/camunda/camunda/issues/28116,28271.0,2025-02-18T12:09:00+00:00,https://github.com/camunda/camunda/pull/28271,0,2,0,2,64,1,0,65,121.12472222222222,kind/bug;support;component/zeebe;scope/clients-spring;version:8.7.0-alpha5;version:8.8.0-alpha2;version:8.6.10,False,True,normal,security,"[{""filename"": ""clients/spring-boot-starter-camunda-sdk/src/main/java/io/camunda/zeebe/spring/client/jobhandling/parameter/VariableResolver.java"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""clients/spring-boot-starter-camunda-sdk/src/test/java/io/camunda/zeebe/spring/client/jobhandling/parameter/VariableResolverTest.java"", ""lines_added"": 60, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
baidu/braft,119,fix log truncate_suffix bug,"1. truncate_suffix order must be from back to front, otherwise it can break log matching property of raft. For example, considering five servers. (server #: terms of logs) server 1: ...9 10 10 10 server 2: ...9 11 11 server 3: ...9  server 4: ...9  server 5: ...9 10 10 10 Now server 2 is a leader with term 11 and send AppendEntries rpc to server 1. If server 1 crash while truncating from front to back, log can be 9 (empty) (empty) 10. And then server 2 crash. server 5 get server 3 and 4's vote and become a leader with term 12. Afterward server 1 come back to life. Server 5 send AppendEntries rpc to 1 and find the last matching log and add a log with term 12. server 1: ...9 (empty) (empty) 10 12 server 2: ...9 11 11(crashed) server 3: ...9 10 10 10 12 server 4: ...9 10 10 10 12 server 5: ...9 10 10 10 12 Server 5 think the first log with term 12 can be committed. Unfortunately, server 1's log will have the hole forever. The truncate suffix order in original code is that the unique last-need-truncated segment first, and then unlink latter segments from back to front. Although the unit is a segment not a log, it has the same problem. In original code, previous example may lead to server 1 can't revive because of checking validity of segments(e.g. the log sequence number is not continuous). However it's not enough just to check index from file name. Crash may happen before renaming the corresponding file. In this case, server 1 revive but may crash on any call to get its empty log Entry(_offset_and_term vector out of range in Segment::get_meta). l have added code in Segment::load to repair last index in file name and moved check_segments after load_segments.  2. Rename failure in Segment::truncate should return error, otherwise next time rename(add .tmp) in unlink or truncate will fail because _last_index in memory is not consistent with its file name.  3. In SegmentLogStorage::pop_segments_from_back, l think it make no sense to set _first_log_index if all segments need to be deleted because the meaningful interval of last_index_kept should be [_first_log_index -1, _last_log_index](last_index_kept = _first_log_index - 1, so set _first_log_index = last_index_kept + 1 is no use). To say the least, if it does make sense to set _first_log_index, save_meta should called to save right _first_log_index to disk. This confused me, so l didn't change it.  l am sorry for not adding unit test for my code because unit test is hard to check above problems.",2019-04-03T18:59:30+00:00,2019-04-25T05:44:57+00:00,6,https://github.com/baidu/braft/pull/119,119.0,,https://github.com/baidu/braft/pull/119,0,1,1,2,97,55,0,151,514.7575,,False,True,critical,performance,"[{""filename"": ""src/braft/log.cpp"", ""lines_added"": 96, ""lines_deleted"": 55, ""file_type"": ""app_code""}, {""filename"": ""src/braft/log.h"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
camunda/camunda,27502,Incorporate Spring Zeebe bug fixes in Spring SDK,"**Describe the bug**  The following bug fixes have been applied to the community Spring Zeebe project after it was moved to the Spring SDK: * https://github.com/camunda-community-hub/spring-zeebe/pull/813 * https://github.com/camunda-community-hub/spring-zeebe/pull/771  Those bug fixes have not been applied to the Spring SDK and have led to user issues.  **To Reproduce**  * Using the `@Variable` annotation on a Job Worker, previous versions of spring-boot-starter-camunda-sdk (e.g. 8.5.5) would pass NULL if the variable did not exist. In 8.6.7 an exception is thrown and an Incident raised in Operate. * Job streaming is enabled by default for job workers and can lead to 504 issues logged in the application if it's not configured correctly. Since it's not expected to be enabled by default, users are not aware that they need to configure it accordingly. Users can disable job streaming by default via configuration as a workaround.  **Expected behavior**  * Variables are passed as NULL if they don't exist. * Job streaming is disabled by default for job workers.  **Environment:** - OS: <!-- [e.g. Linux] --> - Zeebe Version: 8.6.x - Configuration: <!-- [e.g. exporters etc.] --> ",2025-01-28T17:54:52+00:00,2025-02-18T13:11:07+00:00,2,https://github.com/camunda/camunda/issues/27502,28306.0,2025-02-18T16:50:34+00:00,https://github.com/camunda/camunda/pull/28306,0,1,0,1,1,1,0,2,502.9283333333333,kind/bug;severity/mid;support;BREAKING CHANGE;component/clients;likelihood/mid;component/zeebe;scope/clients-spring;version:8.7.0-alpha5;version:8.8.0-alpha2;version:8.6.10,False,True,normal,configuration,"[{""filename"": ""clients/spring-boot-starter-camunda-sdk/src/main/java/io/camunda/zeebe/spring/client/annotation/JobWorker.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
baidu/braft,26,Bug：在特定条件下重启node失败,"复现步骤： 1. 启动节点1，初始conf为：host1:port1:0，设置节点1为leader 2. 启动节点2，初始conf为：host1:port1:0 3. 节点2加入节点1作为follower 4. 重启节点2，一定初始化失败  原因： 1. 节点2因为初始conf跟自己的地址不匹配，所以voted_for的值是"":0:0"" 2. 重启初始化的时候，PeerId加载"":0:0""的时候出错  https://github.com/brpc/braft/blob/master/src/braft/configuration.h#L61         int parse(const std::string& str) {         reset();         char ip_str[64];         if (2 > sscanf(str.c_str(), ""%[^:]%*[:]%d%*[:]%d"", ip_str, &addr.port, &idx)) {             reset();             return -1;         }         if (0 != butil::str2ip(ip_str, &addr.ip)) {             reset();             return -1;         }         return 0;     }  sscanf对"":0:0""的case处理有问题，unittest里面也缺少相应的case：https://github.com/brpc/braft/blob/master/test/test_configuration.cpp#L30  BTW：需要发PR修复吗？如果需要，流程是什么？",2018-05-10T08:42:24+00:00,2018-11-04T11:16:20+00:00,4,https://github.com/baidu/braft/issues/26,289.0,2021-05-19T06:29:45+00:00,https://github.com/baidu/braft/pull/289,0,3,1,4,40,9,0,45,26517.789166666666,,False,True,normal,configuration,"[{""filename"": ""src/braft/node.cpp"", ""lines_added"": 12, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""src/braft/replicator.cpp"", ""lines_added"": 18, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""src/braft/replicator.h"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""test/test_node.cpp"", ""lines_added"": 6, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
baidu/braft,8,Fix bug in FSMCaller::do_committed.,"call on_configuration_committed when CONFIGURATION entry's old_peers is null, to make sure joint stage not noticeable by end users.",2018-03-18T09:28:02+00:00,2018-03-18T09:53:16+00:00,0,https://github.com/baidu/braft/pull/8,8.0,2018-03-18T09:53:16+00:00,https://github.com/baidu/braft/pull/8,0,2,0,2,13,6,0,19,0.4205555555555555,,False,True,normal,configuration,"[{""filename"": ""src/braft/fsm_caller.cpp"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/test_node.cpp"", ""lines_added"": 12, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
jina-ai/serve,6186,Jina AI install error on Ubuntu 22.04 and Python 3.12 system,"**Describe the bug** The error pops up when i try to install the jina-ai python package on my Ubuntu machine. Building GRPC wheel failure. [jina install error.txt](https://github.com/user-attachments/files/16538089/jina.install.error.txt)  **Describe how you solve it** Not Solved yet  ---  <!-- Optional, but really help us locate the problem faster -->  **Environment** Ubuntu 22.04 and Python 3.12  **Screenshots** The Entire error log is in the above txt file",2024-08-08T05:38:43+00:00,2024-12-31T07:33:54+00:00,15,https://github.com/jina-ai/serve/issues/6186,4701.0,2022-04-27T08:56:14+00:00,https://github.com/jina-ai/serve/pull/4701,0,4,0,4,44,68,0,112,-20012.708055555555,,False,True,normal,ui,"[{""filename"": ""cli/autocomplete.py"", ""lines_added"": 0, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""jina/orchestrate/flow/base.py"", ""lines_added"": 19, ""lines_deleted"": 47, ""file_type"": ""app_code""}, {""filename"": ""jina/parsers/flow.py"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""jina/parsers/orchestrate/base.py"", ""lines_added"": 22, ""lines_deleted"": 13, ""file_type"": ""app_code""}]",,False
intelligent-machine-learning/dlrover,1411,There are too many error logs in the UT.,"```text Traceback (most recent call last):   File ""/usr/local/lib/python3.8/logging/__init__.py"", line 1088, in emit     stream.write(msg + self.terminator) ValueError: I/O operation on closed file. Call stack:   File ""/usr/local/lib/python3.8/threading.py"", line 890, in _bootstrap     self._bootstrap_inner()   File ""/usr/local/lib/python3.8/threading.py"", line 932, in _bootstrap_inner     self.run()   File ""/usr/local/lib/python3.8/threading.py"", line 870, in run     self._target(*self._args, **self._kwargs)   File ""/github/workspace/dlrover/python/elastic_agent/diagnosis/diagnosis_agent.py"", line 251, in _periodically_report     self.send_heartbeat()   File ""/github/workspace/dlrover/python/elastic_agent/diagnosis/diagnosis_agent.py"", line 243, in send_heartbeat     action = self._client.report_heart_beat(ts)   File ""/github/workspace/dlrover/python/elastic_agent/master_client.py"", line 250, in report_heart_beat     response: grpc.HeartbeatResponse = self._get(message)   File ""/github/workspace/dlrover/python/elastic_agent/master_client.py"", line 43, in wrapper     for i in range(retry): ```",2024-12-29T00:18:45+00:00,2025-01-02T02:00:42+00:00,0,https://github.com/intelligent-machine-learning/dlrover/issues/1411,1416.0,2025-01-02T02:00:41+00:00,https://github.com/intelligent-machine-learning/dlrover/pull/1416,0,6,0,6,33,26,0,59,97.69888888888887,report,False,True,normal,functional,"[{""filename"": ""dlrover/python/tests/test_diagnosis_agent.py"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""dlrover/python/tests/test_inference.py"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""dlrover/python/tests/test_utils.py"", ""lines_added"": 6, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""dlrover/trainer/tests/torch/checkpoint_egine_test.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""dlrover/trainer/tests/torch/elastic_run_test.py"", ""lines_added"": 4, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""dlrover/trainer/tests/torch/fsdp_ckpt_test.py"", ""lines_added"": 20, ""lines_deleted"": 14, ""file_type"": ""app_code""}]",,False
jina-ai/serve,6197,"AI-Powered Module Import Enhancements: Auto-Correct, Dynamic Error Handling, and Intelligent Logging","### Summary:   This pull request introduces AI-powered enhancements to the module import process, focusing on improving developer experience through three key features: There are three functions: Auto-Correct Module, Dynamic Error Handling and enhanced Logging and Feedback. These features are designed to enhance problematic areas, which occur during module imports, including real-time conceptions, actions, and logging that can help the developers in the efficient problem detection and solving.     ### Related Issues:   Here is a list of difficulties developers experience when dealing with a lack of, or errors in, Python modules, which are solved by this pull request. Thus, the problems related to module importing, incorrect names of the module, and insufficient logging feedback are solved with the help of the AI solutions described in this update.     ### Discussions:   Some of the topics that dominated the conversation included one’s ability to modify the import experience or the import experience of a module by a first-time developer as well as that of an experienced developer. Everyone agreed that having an intelligent assistant that can minimize the time required to look for solutions to import errors and offer meaningful recommendations will transform the script toward becoming more user-focused.     ### QA Instructions:   - Check the AI-generated recommendations with specific intent to use wrong or non-existing modules.   - Make sure that, for every kind of import error, the dynamic error handling offers suggestions that relate to the context of the mistake.   - Make sure that the Flag that contributing enhanced logging feature will not only log the error but also the probable solutions.   - Carry out a trial to the script using different packages and validate if the recommended other successful module is correct.     ### Merge Plan:   Before merging always confirm that the automated tests are passed or if there is any new feature that was added, make sure that check is run on various environments. Ensure that such optimizations do not cause a break of backward compatibility and that all the current features continue to work as expected.     ### Motivation and Context:   The main reason behind such changes is to overcome the annoyance developers experience while handling the module imports.   ### Types of Changes:   - **Feature addition:** AI driven Auto Correct Module, Dynamic Error management system and Dynamic reporting system.   - **Improvement:** Improved the error handling and feedback to give out more useful info to developers when they are importing.  **Goals:** <!---https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword--->  - resolves #ISSUE-NUMBER - ... - ...  - ... - ... - [ ] check and update documentation. See [guide](https://github.com/jina-ai/jina/blob/master/CONTRIBUTING.md#-contributing-documentation) and ask the team. ",2024-09-13T20:46:27+00:00,2024-11-05T09:00:53+00:00,0,https://github.com/jina-ai/serve/pull/6197,6197.0,,https://github.com/jina-ai/serve/pull/6197,0,1,0,1,194,0,0,194,1260.2405555555556,,False,True,normal,ui,"[{""filename"": ""ai_imports.py"", ""lines_added"": 194, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
intelligent-machine-learning/dlrover,1408,Optimize status flow validation,### What changes were proposed in this pull request?  Add a status flow validation.  ### Why are the changes needed?  To avoid corner case like 'running' -> 'pending' which will cause unexpected fault tolerance.  ### Does this PR introduce any user-facing change?  No  ### How was this patch tested?  UT,2024-12-27T10:11:33+00:00,2024-12-30T02:29:17+00:00,2,https://github.com/intelligent-machine-learning/dlrover/pull/1408,1408.0,2024-12-30T02:29:17+00:00,https://github.com/intelligent-machine-learning/dlrover/pull/1408,0,2,0,2,38,1,0,39,64.29555555555555,bug,False,True,normal,functional,"[{""filename"": ""dlrover/python/master/node/status_flow.py"", ""lines_added"": 29, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""dlrover/python/tests/test_job_manager.py"", ""lines_added"": 9, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
radius-project/radius,8913,Scheduled long running test failed - Run ID: 13943810663,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13943810663).  [AB#15064](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15064)",2025-03-19T10:12:32+00:00,2025-03-19T19:07:46+00:00,0,https://github.com/radius-project/radius/issues/8913,7600.0,,https://github.com/radius-project/radius/pull/7600,0,2,0,2,45,5,0,50,8.920555555555556,test-failure,False,True,major,networking,"[{""filename"": ""pkg/corerp/frontend/controller/applications/updatefilter.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/corerp/frontend/controller/applications/updatefilter_test.go"", ""lines_added"": 41, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",controller,False
jina-ai/serve,5138,feat(telemetry): send telemetry data about exceptions,Add some information about Exceptions raised by Executors and at the Client side,2022-09-06T15:53:27+00:00,2022-09-07T09:55:24+00:00,2,https://github.com/jina-ai/serve/pull/5138,5138.0,,https://github.com/jina-ai/serve/pull/5138,0,5,0,5,32,18,0,50,18.0325,area/core;area/helper;size/S;component/client,False,True,normal,database,"[{""filename"": ""jina/clients/base/grpc.py"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jina/clients/base/http.py"", ""lines_added"": 21, ""lines_deleted"": 16, ""file_type"": ""app_code""}, {""filename"": ""jina/clients/base/websocket.py"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jina/helper.py"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""jina/serve/runtimes/worker/__init__.py"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
radius-project/radius,8911,Scheduled long running test failed - Run ID: 13939945767,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13939945767).  [AB#15062](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/15062)",2025-03-19T06:11:32+00:00,2025-03-19T19:07:45+00:00,0,https://github.com/radius-project/radius/issues/8911,7617.0,2024-05-21T21:06:56+00:00,https://github.com/radius-project/radius/pull/7617,0,3,0,3,54,6,0,60,-7233.076666666667,test-failure,False,True,major,networking,"[{""filename"": ""pkg/cli/cmd/radinit/application.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/radinit/application_test.go"", ""lines_added"": 37, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/prompt/validator.go"", ""lines_added"": 15, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",radinit,False
jina-ai/serve,5578,chore: debug slow process executor k8s test,chore: debug slow process executor k8s test,2023-01-06T12:47:11+00:00,2023-01-10T15:35:13+00:00,1,https://github.com/jina-ai/serve/pull/5578,5578.0,,https://github.com/jina-ai/serve/pull/5578,0,2,0,2,12,0,0,12,98.80055555555556,size/S;area/testing;area/cli,False,True,normal,performance,"[{""filename"": ""jina_cli/autocomplete.py"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/k8s/test_graceful_request_handling.py"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
jina-ai/serve,2402,fix: key exception can be thrown by other lookups,"JinaD assumes any KeyException is about not finding the Flow, but it can be triggered by others",2021-05-12T07:15:15+00:00,2021-05-19T10:18:30+00:00,1,https://github.com/jina-ai/serve/pull/2402,2402.0,,https://github.com/jina-ai/serve/pull/2402,0,1,0,1,2,2,0,4,171.05416666666667,size/XS,False,True,normal,functional,"[{""filename"": ""daemon/api/endpoints/flow.py"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",daemon,False
jina-ai/serve,6084,Topology graph Key Error (After Jina 3.21.0),"**Bug Description** There is a KeyError: 'type' in the File ""/home/lzh/.conda/envs/DIMA/lib/python3.8/site-packages/jina/serve/runtimes/gateway/graph/topology_graph.py"", line 90.  ![image](https://github.com/jina-ai/jina/assets/25542404/70147ad3-b2f9-4cb2-a068-dee6a4f6688c)  **Solution** I have printed the schema_1_properties[property_1], the result is as follows:  ![image](https://github.com/jina-ai/jina/assets/25542404/836bf1a0-c58d-488e-822d-e04c110be96d)  There is no key named 'type' for Env Info Here is the code for Env Info:  ```python class EnvInfo(BaseDoc):     env_memory: ShortTermMemory = ShortTermMemory()  # Interaction information of all agent     history: str = '' ```  ```python class ShortTermMemory(DocList[Info]):     def add(self, info: Info):         if info in self:             return         self.append(info)      def add_batch(self, infos: DocList[Info]):         for info in infos:             self.add(info)      def remember(self, k=0) -> DocList[Info]:         """"""Return the most recent k memories, return all when k=0""""""         return self[-k:]          def remember_news(self, observed: DocList[Info], k=0) -> DocList[Info]:         """"""remember the most recent k memories from observed Messages, return all when k=0""""""         already_observed = self.remember(k)         news = DocList[Info]()         for i in observed:             if i.id in already_observed.id:                 continue             news.append(i)         return news      def remember_by_action(self, action: str) -> DocList[Info]:         """"""Return all messages triggered by a specified Action""""""         storage_index = InMemoryExactNNIndex[Info]()         storage_index.index(self)         query = {'cause_by': {'$eq': action}}         content = storage_index.filter(query)         return content      def remember_by_actions(self, actions: Iterable[str]) -> DocList[Info]:         """"""Return all messages triggered by specified Actions""""""         contents = DocList[Info]()         for action in actions:             storage_index = InMemoryExactNNIndex[Info]()             storage_index.index(self)             query = {'cause_by': {'$eq': action}}             contents = contents + storage_index.filter(query) # become a list after + operation         return DocList[Info](contents) ```  ```python class Info(BaseDoc):     content: List = []     instruction: str = ''     agent_id: str = ''  # the profile of the agent     role: str = 'user'  # system / user / assistant     cause_by: str = ''      @property     def Info_str(self):         # prefix = '-'.join([self.role, str(self.cause_by)])         return f""{self.role}: {self.content}""      def Info_str_repr(self):         return self.Info_str()      def to_dict(self) -> dict:         return {             ""role"": self.role,             ""content"": self.content         } ```  **Environment** Python 3.8 Jina>3.21.0 ",2023-10-15T16:04:03+00:00,2023-10-18T04:46:01+00:00,5,https://github.com/jina-ai/serve/issues/6084,5222.0,2022-09-28T09:48:07+00:00,https://github.com/jina-ai/serve/pull/5222,0,1,0,1,5,3,0,8,-9174.265555555556,,False,True,normal,database,"[{""filename"": ""jina/serve/runtimes/gateway/http/models.py"", ""lines_added"": 5, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
jina-ai/serve,5984,bug: gateway configurations ignored in Flow method- `to_kubernetes_yaml` ,"**Describe the bug** Gateway configurations are ignored while ejecting a `Flow` as kubernetes yaml  ---  **Describe how you solve it**  Try the below code: ```py from jina import Flow  f = (     Flow()     .config_gateway(port=12345, protocol=""http"") \\     .add(uses='jinahub+docker://TransformerTorchEncoder') \\     .add(         uses='jinahub+docker://SimpleIndexer',         uses_metas={'workspace': './mydb'},         uses_with = {'table_name': 'my_custon_table_name'}     ) )  f.to_kubernetes_yaml(""./kbe"") ```  On investigating the generated executors and gateway yaml we can see that both the gateway parameters are ignored.  Generated port is 8080 with GRPC protocol.  ---  <!-- Optional, but really help us locate the problem faster -->  **Environment**  ``` - jina 3.19.1 - docarray 0.21.1 - jcloud 0.2.13 - jina-hubble-sdk 0.39.0 - jina-proto 0.1.27 - protobuf 3.20.3 - proto-backend cpp - grpcio 1.47.5 - pyyaml 6.0 - python 3.10.12 - platform Linux - platform-release 5.15.120+ - platform-version #1 SMP Fri Jul 21 03:39:30 UTC 2023 - architecture x86_64 - processor x86_64 - uid 2485378023938 - session-id 8ee68d36-2caa-11ee-a70b-0242ac130202 - uptime 2023-07-27T18:22:34.368597 - ci-vendor (unset) - internal False * JINA_DEFAULT_HOST (unset) * JINA_DEFAULT_TIMEOUT_CTRL (unset) * JINA_DEPLOYMENT_NAME (unset) * JINA_DISABLE_UVLOOP (unset) * JINA_EARLY_STOP (unset) * JINA_FULL_CLI (unset) * JINA_GATEWAY_IMAGE (unset) * JINA_GRPC_RECV_BYTES (unset) * JINA_GRPC_SEND_BYTES (unset) * JINA_HUB_NO_IMAGE_REBUILD (unset) * JINA_LOG_CONFIG (unset) * JINA_LOG_LEVEL (unset) * JINA_LOG_NO_COLOR (unset) * JINA_MP_START_METHOD (unset) * JINA_OPTOUT_TELEMETRY (unset) * JINA_RANDOM_PORT_MAX (unset) * JINA_RANDOM_PORT_MIN (unset) * JINA_LOCKS_ROOT (unset) * JINA_K8S_ACCESS_MODES (unset) * JINA_K8S_STORAGE_CLASS_NAME (unset) * JINA_K8S_STORAGE_CAPACITY (unset) * JINA_STREAMER_ARGS (unset) ``` ",2023-07-27T18:35:26+00:00,2023-07-28T14:19:08+00:00,6,https://github.com/jina-ai/serve/issues/5984,3754.0,2021-10-28T08:56:48+00:00,https://github.com/jina-ai/serve/pull/3754,0,4,1,5,81,30,0,90,-15297.64388888889,,False,True,normal,configuration,"[{""filename"": ""docs/fundamentals/flow/add-exec-to-flow.md"", ""lines_added"": 13, ""lines_deleted"": 8, ""file_type"": ""other""}, {""filename"": ""jina/executors/__init__.py"", ""lines_added"": 8, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/v2_api/test_override_requests.py"", ""lines_added"": 22, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/v2_api/test_yaml_dump_load.py"", ""lines_added"": 6, ""lines_deleted"": 14, ""file_type"": ""app_code""}, {""filename"": ""tests/unit/executors/test_executor.py"", ""lines_added"": 32, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",integration,False
jina-ai/serve,2601,fix: update logger to debug to decrease resource usage,,2021-06-09T14:40:57+00:00,2021-06-09T14:42:30+00:00,0,https://github.com/jina-ai/serve/pull/2601,2601.0,,https://github.com/jina-ai/serve/pull/2601,0,10,2,12,800,39,0,828,0.0258333333333333,size/L;area/core;area/helper;area/testing;area/network;area/setup;component/proto;area/entrypoint;component/type,False,True,normal,functional,"[{""filename"": ""extra-requirements.txt"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""jina/__init__.py"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jina/proto/jina.proto"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""jina/proto/jina_pb2.py"", ""lines_added"": 90, ""lines_deleted"": 33, ""file_type"": ""app_code""}, {""filename"": ""jina/types/document/__init__.py"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""jina/types/document/graph.py"", ""lines_added"": 431, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jina/types/document/multimodal.py"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""jina/types/score/__init__.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/graph/__init__.py"", ""lines_added"": 0, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/graph/test_graph_executor.py"", ""lines_added"": 83, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/issues/github_2103/test_search_attributes.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/unit/types/document/test_graph_document.py"", ""lines_added"": 179, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
intelligent-machine-learning/dlrover,1227,【WIP】Temp solution for socket conflict.,### What changes were proposed in this pull request?  This is a temp solution: Add a random number to the socket name to avoid socket conflict.  ### Why are the changes needed?  The socket may not be released completely when training process restarted('finally' is not enough).  ### Does this PR introduce any user-facing change?  No.  ### How was this patch tested?  UT. ,2024-08-06T01:57:54+00:00,2024-08-19T08:49:28+00:00,1,https://github.com/intelligent-machine-learning/dlrover/pull/1227,1227.0,2024-08-19T08:49:28+00:00,https://github.com/intelligent-machine-learning/dlrover/pull/1227,0,4,0,4,23,10,0,33,318.8594444444444,bug,False,True,normal,functional,"[{""filename"": ""dlrover/python/common/multi_process.py"", ""lines_added"": 3, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""dlrover/python/elastic_agent/torch/ckpt_saver.py"", ""lines_added"": 11, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""dlrover/trainer/tests/torch/checkpoint_egine_test.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""dlrover/trainer/torch/flash_checkpoint/engine.py"", ""lines_added"": 8, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
jina-ai/serve,5963,Docarray ValidationError when monitoring is enabled,"**Describe the bug** <!-- A clear and concise description of what the bug is. --> When I run a schema'ed executor with custom docarray types as inputs and outputs with monitoring enabled, I get an error. I believe it is trying to cast the return value as the output type.  **Environment** <!-- Run `jina --version-full` and copy paste the output here --> `jina== 3.19.0`   **Screenshots** <!-- If applicable, add screenshots to help explain your problem. --> ``` ERROR  @32875 ValidationError(model='Input', errors=[{'loc': ('messages',), 'msg': 'field required', 'type':                                  'value_error.missing'}, {'loc': ('__root__',), 'msg': 'Only one model config type can be set, found []',                                   'type': 'value_error'}])                                                                                                                    add ""--quiet-error"" to suppress the exception details                                                                                     Traceback (most recent call last):                                                                                                           File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/jina/serve/runtimes/worker/…                            line 956, in process_data                                                                                                                      result = await self.handle(                                                                                                              File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/jina/serve/runtimes/worker/…                            line 670, in handle                                                                                                                            self._record_docs_processed_monitoring(requests)                                                                                         File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/jina/serve/runtimes/worker/…                            line 481, in _record_docs_processed_monitoring                                                                                                 len(requests[0].docs)                                                                                                                    File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/jina/types/request/data.py"",                            line 282, in docs                                                                                                                              return self.data.docs                                                                                                                    File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/jina/types/request/data.py"",                            line 50, in docs                                                                                                                               self._loaded_doc_array = self.document_array_cls.from_protobuf(                                                                          File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/array/doc_list/doc…                            line 296, in from_protobuf                                                                                                                     return super().from_protobuf(pb_msg)                                                                                                     File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/array/doc_list/io.…                            line 119, in from_protobuf                                                                                                                     return cls(cls.doc_type.from_protobuf(doc_proto) for doc_proto in pb_msg.docs)                                                           File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/array/doc_list/doc…                            line 128, in __init__                                                                                                                          super().__init__(docs)                                                                                                                   File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/array/doc_list/doc…                            line 155, in _validate_docs                                                                                                                    for doc in docs:                                                                                                                         File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/array/doc_list/io.…                            line 119, in <genexpr>                                                                                                                         return cls(cls.doc_type.from_protobuf(doc_proto) for doc_proto in pb_msg.docs)                                                           File                                                                                                                                     ""/Users/user/env/lib/python3.9/site-packages/docarray/base_doc/mixins/io…                            line 247, in from_protobuf                                                                                                                     return cls(**fields)                                                                                                                     File ""/Users/user/env/hub/schema.py"", line 28, in __init__                                    super().__init__(**data)                                                                                                                 File ""pydantic/main.py"", line 341, in pydantic.main.BaseModel.__init__                                                                   pydantic.error_wrappers.ValidationError: 2 validation errors for Input                                                                 messages                                                                                                                                     field required (type=value_error.missing)                                                                                                    ```",2023-07-13T14:46:45+00:00,2023-07-13T18:56:08+00:00,2,https://github.com/jina-ai/serve/issues/5963,6177.0,2024-07-10T16:48:30+00:00,https://github.com/jina-ai/serve/pull/6177,0,1,0,1,8,3,0,11,8714.029166666667,,False,True,normal,configuration,"[{""filename"": ""jina/clients/base/http.py"", ""lines_added"": 8, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
intelligent-machine-learning/dlrover,1226,fix unittest error: AttributeError: ElasticLaunchConfig object has no attribute tee,### What changes were proposed in this pull request?  add tee in class ElasticLaunchConfig  ### Why are the changes needed?  the unittest failed  AttributeError: ElasticLaunchConfig object has no attribute tee  ### Does this PR introduce any user-facing change?  no  ### How was this patch tested?  passed,2024-08-03T12:14:03+00:00,2024-08-05T03:39:04+00:00,1,https://github.com/intelligent-machine-learning/dlrover/pull/1226,1226.0,2024-08-05T03:39:04+00:00,https://github.com/intelligent-machine-learning/dlrover/pull/1226,0,1,0,1,1,0,0,1,39.41694444444445,,False,True,normal,configuration,"[{""filename"": ""dlrover/python/elastic_agent/torch/training.py"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
jina-ai/serve,5824,bug: running flow on windows,"On the latest master, jina is hanging when deploying the following flow: ```yml jtype: Flow with:   port: 8080   protocol: http jcloud:   version: 3.14.2.dev18   labels:     creator: microchain   name: gptdeploy executors:   - name: printhelloexecutor4715887     uses: jinaai+docker://auth0-unified-448f11965ce142b6/PrintHelloExecutor4715887:latest     jcloud:       resources:         instance: C2         capacity: spot ```  error: ```txt C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Scripts\\python.exe C:\\Users\\hoenicke\\jina\\gptdeploy\\gptdeploy.py run --path microservice  Run a jina flow locally ⠋ Fetching auth0-unified-448f11965ce142b6/PrintHelloExecutor4715887 from Jina  ⠋ Fetching auth0-unified-448f11965ce142b6/PrintHelloExecutor4715887 from Jina  Hub ...  WARNI… printhelloexecutor4715887/rep-0@18720                [04/24/23 10:36:57]        <jina.orchestrate.pods.container.ContainerPod object                            at 0x000001E8A04D1350> timeout after waiting for                                600000ms, if your executor takes time to load, you                              may increase --timeout-ready                                             🐳  Process terminated, the container fails to start, check the arguments or    entrypoint                                                                      ERROR  Flow@18720 Flow is aborted due to                    [04/24/23 10:36:59]        ['printhelloexecutor4715887'] can not be started.                        WARNI… gateway/rep-0@18720 Pod was forced to close after 1  [04/24/23 10:37:00]        second. Graceful closing is not available on                                    Windows.                                                                 Traceback (most recent call last):   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\gptdeploy.py"", line 6, in <module>     main()   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\click\\core.py"", line 1130, in __call__     return self.main(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\click\\core.py"", line 1055, in main     rv = self.invoke(ctx)          ^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\click\\core.py"", line 1657, in invoke     return _process_result(sub_ctx.command.invoke(sub_ctx))                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\click\\core.py"", line 1404, in invoke     return ctx.invoke(self.callback, **ctx.params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\click\\core.py"", line 760, in invoke     return __callback(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\src\\cli.py"", line 39, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\src\\cli.py"", line 84, in run     Runner().run(path)   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\src\\options\\run\\runner.py"", line 10, in run     run_locally(executor_name, latest_version_path)   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\src\\apis\\jina_cloud.py"", line 204, in run_locally     with flow:   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\jina\\orchestrate\\orchestrator.py"", line 14, in __enter__     return self.start()            ^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\jina\\orchestrate\\flow\\builder.py"", line 33, in arg_wrapper     return func(self, *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\jina\\orchestrate\\flow\\base.py"", line 1832, in start     self._wait_until_all_ready()   File ""C:\\Users\\hoenicke\\jina\\gptdeploy\\venv\\Lib\\site-packages\\jina\\orchestrate\\flow\\base.py"", line 1975, in _wait_until_all_ready     raise RuntimeFailToStart jina.excepts.RuntimeFailToStart ``` ",2023-04-24T08:53:48+00:00,2023-07-10T08:49:45+00:00,4,https://github.com/jina-ai/serve/issues/5824,4794.0,2022-05-20T09:50:27+00:00,https://github.com/jina-ai/serve/pull/4794,0,5,0,5,153,25,0,178,-8135.055833333333,,False,True,normal,networking,"[{""filename"": ""jina/excepts.py"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""jina/serve/networking.py"", ""lines_added"": 28, ""lines_deleted"": 17, ""file_type"": ""app_code""}, {""filename"": ""jina/serve/runtimes/gateway/graph/topology_graph.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""jina/serve/runtimes/gateway/request_handling.py"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""tests/integration/runtimes/test_network_failures.py"", ""lines_added"": 119, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
project-oak/oak,4677,`key_value_lookup tests::test_server` failing in CI in Kokoro,http://fusion2/ci/kokoro/prod:oak%2Fpresubmit_run_tests/activity/9356ad96-675c-473a-92b6-ab1fcf7d0dbf/log  it may just be a case of increasing some timeout.  To run those tests locally:  ``` cargo nextest run --package=key_value_lookup ```  Though at least I cannot reproduce the failure locally.,2024-01-18T18:26:40+00:00,2024-03-07T01:12:09+00:00,4,https://github.com/project-oak/oak/issues/4677,4684.0,,https://github.com/project-oak/oak/pull/4684,0,0,2,2,5,5,0,0,1158.7580555555555,,False,True,normal,networking,"[{""filename"": ""oak_containers_stage1/Makefile"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""xtask/src/launcher.rs"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""other""}]",,False
apache/incubator-kie-kogito-runtimes,3765,Node instance Mvel evaluation error message,"A nitpick, but calling three consecutives times logger.error looks pretty bad ",2024-11-11T12:07:43+00:00,2024-11-14T12:14:47+00:00,0,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3765,3765.0,2024-11-14T12:14:47+00:00,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3765,0,1,0,1,2,3,0,5,72.11777777777777,,False,True,normal,functional,"[{""filename"": ""jbpm/jbpm-flow/src/main/java/org/jbpm/workflow/instance/impl/NodeInstanceImpl.java"", ""lines_added"": 2, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
erda-project/erda,6416,fix(pipeline): task with status NoNeedBySystem cause pipeline status to Failed,"#### What this PR does / why we need it:  Fix the issue that task with status NoNeedBySystem cause pipeline status to Failed.  Commonly seen in scenario that task with if expression.  1. treat task NoNeedBySystem status as success when CalculatePipelineStatusV2 2. set task status to AnalyzedFailed when if-expression parsed error 3. pipeline.task.inspect add metadata field for internal use 4. add more info for if expression exec result   #### Which issue(s) this PR fixes:  - [Erda Cloud Issue Link](https://erda.cloud/erda/dop/projects/387/issues/all?id=617844&iterationID=12783&type=BUG)   #### Specified Reviewers:  /assign @chengjoey    #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that ... in xxx platform （修复了 xxx 平台的 ...） Feature: Support/Optimize ... in xxx platform （实现/优化了 xxx 平台的 ...）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English |   Fix the issue that task with status NoNeedBySystem cause pipeline status to Failed.           | | 🇨🇳 中文    |   修复了无需执行的 action 导致流水线状态为失败的问题          | ",2024-08-02T08:51:27+00:00,2024-08-02T09:46:04+00:00,1,https://github.com/erda-project/erda/pull/6416,6416.0,2024-08-02T09:46:04+00:00,https://github.com/erda-project/erda/pull/6416,0,15,0,15,160,66,0,226,0.9102777777777776,pipeline;bugfix,False,True,normal,database,"[{""filename"": ""internal/tools/pipeline/aop/plugins/pipeline/basic/plugin.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/aop/plugins/task/autotest_cookie_keep_after/plugin.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/aop/plugins/task/unit_test_report/plugin.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/commonutil/statusutil/status.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/commonutil/statusutil/status_test.go"", ""lines_added"": 51, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/dbclient/op_pipeline.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/pexpr/pexpr_params/task_params.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/pipengine/actionexecutor/plugins/apitest/apitest.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/pkg/taskinspect/inspect.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/providers/reconciler/snippet.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/providers/reconciler/taskrun/taskop/prepare.go"", ""lines_added"": 32, ""lines_deleted"": 24, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/providers/reconciler/taskrun/taskop/prepare_test.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/spec/pipeline_task.go"", ""lines_added"": 27, ""lines_deleted"": 25, ""file_type"": ""app_code""}, {""filename"": ""pkg/expression/expression_execute.go"", ""lines_added"": 24, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/expression/expression_execute_test.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",plugins,False
apache/incubator-kie-kogito-runtimes,3698,Downgrade DefaultInstanceEventBatch trace level to debug,Downgrade the trace level of this recurrent trace because it's very noisy   Many thanks for submitting your Pull Request :heart:!   ,2024-10-04T15:18:04+00:00,2024-10-16T18:54:01+00:00,3,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3698,3698.0,,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3698,0,1,0,1,1,1,0,2,291.5991666666667,,False,True,normal,functional,"[{""filename"": ""api/kogito-events-core/src/main/java/org/kie/kogito/event/impl/DefaultInstanceEventBatch.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",api,False
apache/incubator-kie-kogito-runtimes,3579,Fix Springboot IT test reproducible failing when code is modified for build time only modules,"If common-openapi jar is modified, reproducilbe will always fail no matter the nature of the change. The PR makes a dummy modification on common-openapi, which makes reprodubile to fail on CI, and change the pom for  springboot integration test in order to avoid that failure.",2024-07-15T09:51:04+00:00,2024-07-26T15:58:52+00:00,9,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3579,3579.0,2024-07-26T15:58:52+00:00,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3579,1,1,0,2,9,0,4,5,270.13,Do not merge,False,True,normal,ui,"[{""filename"": ""kogito-serverless-workflow/kogito-serverless-workflow-openapi-common/src/main/java/org/kie/kogito/serverless/workflow/operationid/SpecWorkflowOperationIdFactory.java"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""springboot/integration-tests/pom.xml"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""config""}]",kogito-serverless-workflow,False
erda-project/erda,6356,fix(cmp): dbclient sql build error,"#### What this PR does / why we need it: ```shell (/go/src/github.com/erda-project/erda/internal/apps/cmp/dbclient/records.go:112)  [2024-05-27 18:10:57]  Error 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%d second) AND (status in (?,?)) AND (record_type in (?,?,?,?)) ORDER BY created' at line 1  ERRO[2024-05-27 18:10:57.727492874] get create cluster record failed, error: Error 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%d second) AND (status in (?,?)) AND (record_type in (?,?,?,?)) ORDER BY created' at line 1  ```  #### Which issue(s) this PR fixes:  - Fixes #your-issue_number - [Erda Cloud Issue Link](paste your link here)   #### Specified Reviewers:  /assign @sfwn @CeerDecy    #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that ... in xxx platform （修复了 xxx 平台的 ...） Feature: Support/Optimize ... in xxx platform （实现/优化了 xxx 平台的 ...）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English | fix dbclient sql build error             | | 🇨🇳 中文    |   修复 cmp sql build error           |   #### Need cherry-pick to release versions?  Add comment like `/cherry-pick release/1.0` when this PR is merged.  > For details on the cherry pick process, see the [cherry pick requests](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md#how-to-cherry-pick-a-merged-pr) section under [CONTRIBUTING.md](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md). ",2024-05-27T10:11:57+00:00,2024-05-28T01:29:42+00:00,1,https://github.com/erda-project/erda/pull/6356,6356.0,2024-05-28T01:29:42+00:00,https://github.com/erda-project/erda/pull/6356,0,1,0,1,2,2,0,4,15.295833333333333,cmp;bugfix;approved,False,True,normal,database,"[{""filename"": ""internal/apps/cmp/dbclient/records.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",internal,False
apache/incubator-kie-kogito-runtimes,3473,Review ProcessService inject into management addon to avoid UnsatisfiedResolutionException when a project doesn't have a bpmn/serverless workflow file,"              Folks, this PR broke the SonataFlow builder/devmode images:  ``` 2024-04-11 19:38:43,504 3161eb1b5bd6 ERROR [io.quarkus.deployment.dev.IsolatedDevModeMain:156] (main) Failed to start quarkus: java.lang.RuntimeException: io.quarkus.builder.BuildException: Build failure: Build failed due to errors         [error]: Build step io.quarkus.arc.deployment.ArcProcessor#validate threw an exception: jakarta.enterprise.inject.spi.DeploymentException: jakarta.enterprise.inject.UnsatisfiedResolutionException: Unsatisfied dependency for type org.kie.kogito.process.ProcessService and qualifiers [@Default]         - java member: org.kie.kogito.process.management.ProcessInstanceManagementResource#processService         - declared on CLASS bean [types=[java.lang.Object, org.kie.kogito.process.management.ProcessInstanceManagementResource], qualifiers=[@Default, @Any], target=org.kie.kogito.process.management.ProcessInstanceManagementResource] ```  The culprit is this change: [quarkus/addons/process-management/runtime/src/main/java/org/kie/kogito/process/management/ProcessInstanceManagementResource.java](https://github.com/apache/incubator-kie-kogito-runtimes/pull/3403/files#diff-5e23f61fd6ebb5924ae33cb45750608cff7f7f3358918d8fd7248434dc778432)  The `ProcessService` bean is not available. Since these images start with an empty project, I think this injection dependency is not found. I might be wrong, I haven't dug enough.  Can we make this injection optional?  _Originally posted by @ricardozanini in https://github.com/apache/incubator-kie-kogito-runtimes/issues/3403#issuecomment-2050490871_             ",2024-04-12T14:54:29+00:00,2024-04-15T12:43:41+00:00,3,https://github.com/apache/incubator-kie-kogito-runtimes/issues/3473,3474.0,2024-04-15T07:24:03+00:00,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3474,0,3,0,3,8,17,0,25,64.49277777777777,,False,True,normal,ui,"[{""filename"": ""addons/common/process-management/src/main/java/org/kie/kogito/process/management/BaseProcessInstanceManagementResource.java"", ""lines_added"": 4, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""quarkus/addons/process-management/runtime/src/main/java/org/kie/kogito/process/management/ProcessInstanceManagementResource.java"", ""lines_added"": 2, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""springboot/addons/process-management/src/main/java/org/kie/kogito/process/management/ProcessInstanceManagementRestController.java"", ""lines_added"": 2, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
erda-project/erda,6353,"fix(monitor): when report-client failed to serialize metrics, throwing errors rather than ignore","#### What this PR does / why we need it: when use report-client send metrics, i hope to confirm the push, if there is an error, it should be thrown in time, instead of ignoring the error    #### Specified Reviewers:  /assign @sfwn    #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that when report-client failed to serialize metrics, throwing errors rather than ignore（修复了report-client推送metrics时忽略报错的问题）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English | Fix the bug that when report-client failed to serialize metrics, throwing errors rather than ignore             | | 🇨🇳 中文    |   修复了report-client推送metrics时忽略报错的问题           |   #### Need cherry-pick to release versions?  Add comment like `/cherry-pick release/1.0` when this PR is merged.  > For details on the cherry pick process, see the [cherry pick requests](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md#how-to-cherry-pick-a-merged-pr) section under [CONTRIBUTING.md](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md). ",2024-05-24T06:43:01+00:00,2024-05-24T09:29:48+00:00,1,https://github.com/erda-project/erda/pull/6353,6353.0,2024-05-24T09:29:48+00:00,https://github.com/erda-project/erda/pull/6353,0,1,0,1,1,1,0,2,2.7797222222222224,bugfix;approved,False,True,normal,functional,"[{""filename"": ""internal/pkg/metrics/report/collector.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
apache/incubator-kie-kogito-runtimes,1258,Parent process remains in ACTIVE state when an exception occurs in re-usable subprocess ,"Scenario: you have a BPMN with re-usable subprocess being called from parent process.  if the sub process returns an exception (e.g. runtime exception), that sub process gets to state of ERROR (5) but the parent process remains in ACTIVE state (5) and the system responds with success response.  Also if you have listeners implemented (ProcessEventListener), you do not get a handle in any of the life cycle method for parent process since the process is not completed.  May be it would make sense to expose another lifecycle method, afterProcessErrored/Aborted so that application can get handle to decorate domain entities/perform some error handling.  Also parent process should not remain in ACTIVE state. it should get set with ERROR state with errors from subprocess being cascaded.",2021-04-25T15:50:41+00:00,2024-02-20T09:07:40+00:00,1,https://github.com/apache/incubator-kie-kogito-runtimes/issues/1258,3133.0,2023-07-26T10:45:31+00:00,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3133,2,4,0,6,68,4,15,57,19722.91388888889,,False,True,normal,functional,"[{""filename"": ""kogito-serverless-workflow/kogito-serverless-workflow-executor-core/src/main/java/org/kie/kogito/serverless/workflow/executor/StaticWorkflowApplication.java"", ""lines_added"": 11, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""kogito-serverless-workflow/kogito-serverless-workflow-executor-core/src/main/java/org/kie/kogito/serverless/workflow/executor/StaticWorkflowProcess.java"", ""lines_added"": 9, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""kogito-serverless-workflow/kogito-serverless-workflow-executor-kafka/pom.xml"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""kogito-serverless-workflow/kogito-serverless-workflow-executor-kafka/src/test/java/org/kie/kogito/serverless/workflow/executor/WorkflowEventSubscriberTest.java"", ""lines_added"": 26, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""quarkus/addons/process-definitions/runtime/pom.xml"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""config""}, {""filename"": ""quarkus/addons/process-definitions/runtime/src/main/java/org/kie/kogito/process/definitions/ProcessDefinitionsResource.java"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",kogito-serverless-workflow,False
apache/incubator-kie-kogito-runtimes,3320,Flaky timeout tests,"### Describe the bug  org.kie.kogito.quarkus.workflows.CallbackStateTimeoutsIT.callbackStateTimeoutsExceeded org.kie.kogito.quarkus.workflows.CallbackStateWithTimeoutsErrorHandlerIT.callbackStateTimeoutsExceeded org.kie.kogito.quarkus.workflows.EventTimedoutIT.testTimedout  These tests are flaky. Even if I set a longer timeout value (e.g. from 10 sec to 60 sec), the tests may still fail.  Until they get stable, it's better to `@Disable` the tests.  ### Expected behavior  These tests pass stably.  Until then, disabled the tests.  ### Actual behavior  These tests fail occasionally in both CIs and local. (Around 30% chance of failing)  ``` 2023-12-06 12:17:27,667 INFO  [org.kie.kog.ser.wor.dev.DevModeServerlessWorkflowLogger] (executor-thread-1) Triggered node 'TimerNode_14' for process 'callback_state_timeouts' (afb4bfef-8418-49d5-ac62-de009d062af8) 2023-12-06 12:17:27,668 INFO  [org.kie.kog.ser.wor.dev.DevModeServerlessWorkflowLogger] (executor-thread-1) Triggered node 'callbackEvent' for process 'callback_state_timeouts' (afb4bfef-8418-49d5-ac62-de009d062af8) 2023-12-06 12:17:27,669 INFO  [org.kie.kog.ser.wor.dev.DevModeServerlessWorkflowLogger] (executor-thread-1) Workflow 'callback_state_timeouts' (afb4bfef-8418-49d5-ac62-de009d062af8) was started, now 'Active' 2023-12-06 12:17:32,667 INFO  [org.kie.kog.ser.job.imp.InMemoryJobService] (executor-thread-1) Job af217e82-9d62-48ae-a906-aa6dfa07b82c started [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 34.03 s <<< FAILURE! -- in org.kie.kogito.quarkus.workflows.CallbackStateTimeoutsIT [ERROR] org.kie.kogito.quarkus.workflows.CallbackStateTimeoutsIT.callbackStateTimeoutsExceeded -- Time elapsed: 10.43 s <<< ERROR! org.testcontainers.shaded.org.awaitility.core.ConditionTimeoutException:  Assertion condition defined as a org.kie.kogito.test.utils.ProcessInstancesRESTTestUtils 1 expectation failed. Expected status code <404> but was <200>.  within 10 seconds. 	at org.testcontainers.shaded.org.awaitility.core.ConditionAwaiter.await(ConditionAwaiter.java:167) 	at org.testcontainers.shaded.org.awaitility.core.AssertionCondition.await(AssertionCondition.java:119) 	at org.testcontainers.shaded.org.awaitility.core.AssertionCondition.await(AssertionCondition.java:31) 	at org.testcontainers.shaded.org.awaitility.core.ConditionFactory.until(ConditionFactory.java:985) 	at org.testcontainers.shaded.org.awaitility.core.ConditionFactory.untilAsserted(ConditionFactory.java:769) 	at org.kie.kogito.test.utils.ProcessInstancesRESTTestUtils.assertProcessInstanceHasFinished(ProcessInstancesRESTTestUtils.java:161) 	at org.kie.kogito.quarkus.workflows.CallbackStateTimeoutsIT.callbackStateTimeoutsExceeded(CallbackStateTimeoutsIT.java:61) ```  ### How to Reproduce?  Run these tests under `kogito-runtimes/quarkus/extensions/kogito-quarkus-serverless-workflow-extension/kogito-quarkus-serverless-workflow-integration-test`  $ mvn verify  ### Output of `uname -a` or `ver`  Linux tkobayas.nrt.csb 4.18.0-477.27.1.el8_8.x86_64 #1 SMP Thu Aug 31 10:29:22 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux  ### Output of `java -version`  openjdk version ""17.0.9"" 2023-10-17 LTS  ### GraalVM version (if different from Java)  _No response_  ### Kogito version or git rev (or at least Quarkus version if you are using Kogito via Quarkus platform BOM)  `quarkus-3.2LTS` branch  ### Build tool (ie. output of `mvnw --version` or `gradlew --version`)  Apache Maven 3.9.5 (57804ffe001d7215b5e7bcb531cf83df38f93546)  ### Additional information  _No response_",2023-12-06T03:21:34+00:00,2024-02-20T01:24:08+00:00,1,https://github.com/apache/incubator-kie-kogito-runtimes/issues/3320,3649.0,2024-12-05T15:11:42+00:00,https://github.com/apache/incubator-kie-kogito-runtimes/pull/3649,0,2,0,2,46,48,0,94,8771.835555555555,bug,False,True,normal,networking,"[{""filename"": ""jbpm/jbpm-tests/src/test/java/org/jbpm/bpmn2/ErrorEventTest.java"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""jbpm/jbpm-tests/src/test/java/org/jbpm/bpmn2/FlowTest.java"", ""lines_added"": 31, ""lines_deleted"": 48, ""file_type"": ""app_code""}]",,False
erda-project/erda,6250,fix: incorrect app pipeline filtering,"#### What this PR does / why we need it: fix issue of displaying pipelines from other applications while filtering in application's pipeline  #### Which issue(s) this PR fixes:  - Fixes #your-issue_number - [Erda Cloud Issue Link](https://erda.cloud/erda/dop/projects/387/issues/all?id=553755&iterationID=12783&tab=ALL&type=BUG)   #### Specified Reviewers:  /assign @sfwn @chengjoey    #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that ... in xxx platform （修复了 xxx 平台的 ...） Feature: Support/Optimize ... in xxx platform （实现/优化了 xxx 平台的 ...）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English |   fix: incorrect app pipeline filtering           | | 🇨🇳 中文    |    修复了在应用的流水线页面筛选时获得其他应用流水线的问题          |   #### Need cherry-pick to release versions?  Add comment like `/cherry-pick release/1.0` when this PR is merged.  > For details on the cherry pick process, see the [cherry pick requests](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md#how-to-cherry-pick-a-merged-pr) section under [CONTRIBUTING.md](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md). ",2024-01-22T03:46:38+00:00,2024-02-29T03:29:34+00:00,3,https://github.com/erda-project/erda/pull/6250,6250.0,2024-02-29T03:29:34+00:00,https://github.com/erda-project/erda/pull/6250,0,3,0,3,56,0,0,56,911.7155555555556,bugfix,False,True,normal,ui,"[{""filename"": ""internal/apps/dop/component-protocol/components/project-pipeline/myPage/tabsTable/pipelineTable/inparams.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/apps/dop/component-protocol/components/project-pipeline/myPage/tabsTable/pipelineTable/inparams_test.go"", ""lines_added"": 38, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/apps/dop/component-protocol/components/project-pipeline/myPage/tabsTable/pipelineTable/provider.go"", ""lines_added"": 11, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",internal,False
erda-project/erda,6242,fix: constant appending of errors causing oom,"#### What this PR does / why we need it: task reconciler reties and appends errors continuously, leading to memory growth if the error not resolved(will cause oom). existing mechanism of merging errors is to compare with the last err, but fails with cycling difference err will ignore this mechanism, so it needs adjustment  #### Which issue(s) this PR fixes:  - Fixes #your-issue_number - [Erda Cloud Issue Link](https://erda.cloud/erda/dop/projects/387/issues/all?id=550206&iterationID=12783&type=TASK)   #### Specified Reviewers:  /assign @sfwn   #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that ... in xxx platform （修复了 xxx 平台的 ...） Feature: Support/Optimize ... in xxx platform （实现/优化了 xxx 平台的 ...）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English |   fix constant appending of errors when task reconciler causing oom          | | 🇨🇳 中文    |    修复了task调度时错误不断追加导致OOM的问题           |   #### Need cherry-pick to release versions?  Add comment like `/cherry-pick release/1.0` when this PR is merged.  > For details on the cherry pick process, see the [cherry pick requests](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md#how-to-cherry-pick-a-merged-pr) section under [CONTRIBUTING.md](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md). ",2024-01-18T03:33:21+00:00,2024-01-18T08:54:48+00:00,3,https://github.com/erda-project/erda/pull/6242,6242.0,2024-01-18T08:54:48+00:00,https://github.com/erda-project/erda/pull/6242,0,4,0,4,130,7,0,137,5.3575,bugfix;approved,False,True,normal,performance,"[{""filename"": ""internal/tools/pipeline/pkg/taskerror/ordered.go"", ""lines_added"": 42, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/pkg/taskerror/ordered_test.go"", ""lines_added"": 80, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/providers/reconciler/provider.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/tools/pipeline/providers/reconciler/task_reconciler.go"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",ordered.go;ordered_test.go,True
erda-project/erda,6202,fix(orchestrator): edas list application multi response error,"#### What this PR does / why we need it: The EDAS list application utilizes the fuzzy matching technique, which distinguishes it from the anticipated approach. Currently EDAS does not support getting by the get `application name`.  e.g. `go-demo` will get multi response.  ![image](https://github.com/erda-project/erda/assets/31346321/f855c779-8964-448e-8885-fab282a7317e)   #### Which issue(s) this PR fixes:  - Fixes #your-issue_number - [Erda Cloud Issue Link](paste your link here)   #### Specified Reviewers:  /assign @sfwn    #### ChangeLog <!-- Describe the specific changes from the user's perspective, as well as possible Breaking Change and other risks. Common Format： Bugfix： Fix the bug that ... in xxx platform （修复了 xxx 平台的 ...） Feature: Support/Optimize ... in xxx platform （实现/优化了 xxx 平台的 ...）  `xxx` is one of DevOps/Micro Service/Cloud Management -->  | Language | Changelog | | --------- | ------------ | | 🇺🇸 English | fix edas list application multi response error              | | 🇨🇳 中文    |     修复 edas 获取应用多返回错误        |   #### Need cherry-pick to release versions?  Add comment like `/cherry-pick release/1.0` when this PR is merged.  > For details on the cherry pick process, see the [cherry pick requests](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md#how-to-cherry-pick-a-merged-pr) section under [CONTRIBUTING.md](https://github.com/erda-project/erda/blob/master/CONTRIBUTING.md). ",2023-12-28T03:51:41+00:00,2023-12-28T08:29:32+00:00,3,https://github.com/erda-project/erda/pull/6202,6202.0,2023-12-28T08:29:32+00:00,https://github.com/erda-project/erda/pull/6202,0,1,0,1,7,10,0,17,4.630833333333333,orchestrator;bugfix;approved,False,True,normal,ui,"[{""filename"": ""internal/tools/orchestrator/scheduler/executor/plugins/edas/wrapclient/edas/app.client.go"", ""lines_added"": 7, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",edas,False
openclarity/openclarity,897,fix(scanner): panic when nil result,## Description  https://github.com/openclarity/openclarity/issues/885  ## Type of Change  - [x] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [x] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [ ] Existing issues have been referenced (where applicable) - [x] I have verified this change is not present in other open pull requests - [ ] Functionality is documented - [x] All code style checks pass - [ ] New code contribution is covered by automated tests - [x] All new and existing tests pass ,2024-10-23T16:10:28+00:00,2024-10-24T07:48:43+00:00,0,https://github.com/openclarity/openclarity/pull/897,897.0,2024-10-24T07:48:43+00:00,https://github.com/openclarity/openclarity/pull/897,0,1,0,1,1,1,0,2,15.6375,bug;size/XS;area/scanner,False,True,normal,ui,"[{""filename"": ""scanner/families/sbom/types/result.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
openclarity/openclarity,885,Scanner crashes (nil deref),"## Description  Scanner crashes.  ## Expected Behavior  Scanner does not crash.  ## Actual Behavior  ``` time=""2024-10-14T16:52:18Z"" level=info msg=Running... app=openclarity time=""2024-10-14T16:52:18Z"" level=info msg=""Initializing configuration..."" app=openclarity time=""2024-10-14T16:52:18Z"" level=info msg=""Using config file (/etc/openclarity/config.yaml):\\nexploits:\\n  enabled: true\\n  input_from_vuln: true\\n  inputs:\\n  - input: /mnt/snapshot/image.tar\\n    input_type: oci-archive\\n    strip_path_from_result: true\\n  scanners_config:\\n    exploit_db:\\n      base_url: http://openclarity-exploit-db-server:1326\\n  scanners_list:\\n  - exploitdb\\ninfofinder:\\n  enabled: true\\n  inputs:\\n  - input: /mnt/snapshot/image.tar\\n    input_type: oci-archive\\n    strip_path_from_result: true\\n  scanners_config:\\n    ssh_topology: {}\\n  scanners_list:\\n  - sshTopology\\n  strip_input_paths: false\\nmalware:\\n  enabled: false\\n  inputs: ]\\n  scanners_config:\\n    clam:\\n      alternative_freshclam_mirror_url: \\""\\""\\n      clam_daemon_binary_path: \\""\\""\\n      clam_daemon_client_binary_path: \\""\\""\\n      clam_daemon_config_path: \\""\\""\\n      clamscan_binary_path: \\""\\""\\n      clamscan_exclude_dirs: ]\\n      clamscan_exclude_files: ]\\n      freshclam_binary_path: \\""\\""\\n      freshclam_config_path: \\""\\""\\n      use_native_clamscan: false\\n    yara:\\n      cache_dir: \\""\\""\\n      compiled_rule_url: \\""\\""\\n      directories_to_scan: ]\\n      rule_sources: ]\\n      yara_binary_path: \\""\\""\\n      yarac_binary_path: \\""\\""\\n  scanners_list: ]\\n  strip_input_paths: false\\nmisconfiguration:\\n  enabled: true\\n  inputs:\\n  - input: /mnt/snapshot/image.tar\\n    input_type: oci-archive\\n    strip_path_from_result: true\\n  scanners_config:\\n    cisdocker:\\n      registry: null\\n      timeout: 0\\n    lynis:\\n      binary_path: \\""\\""\\n  scanners_list:\\n  - lynis\\n  - cisdocker\\n  strip_input_paths: false\\nplugins:\\n  binary_artifacts_clean: false\\n  binary_artifacts_path: \\""\\""\\n  binary_mode: false\\n  enabled: false\\n  inputs: ]\\n  scanners_config: null\\n  scanners_list: ]\\nrootkits:\\n  enabled: false\\n  inputs: ]\\n  scanners_config:\\n    chkrootkit:\\n      binary_path: \\""\\""\\n  scanners_list: ]\\n  strip_input_paths: false\\nsbom:\\n  analyzers_config:\\n    syft:\\n      exclude_paths: ]\\n      local_image_scan: false\\n      registry: null\\n      scope: squashed\\n    trivy:\\n      cache_dir: \\""\\""\\n      local_image_scan: false\\n      registry: null\\n      temp_dir: \\""\\""\\n      timeout: 300\\n  analyzers_list:\\n  - syft\\n  - trivy\\n  - windows\\n  - gomod\\n  enabled: true\\n  inputs:\\n  - input: /mnt/snapshot/image.tar\\n    input_type: oci-archive\\n    strip_path_from_result: null\\n  local_image_scan: false\\n  merge_with: ]\\n  output_format: cyclonedx-json\\n  registry:\\n    auths: ]\\n    skip-verify-tls: false\\n    use-http: false\\nsecrets:\\n  enabled: true\\n  inputs:\\n  - input: /mnt/snapshot/image.tar\\n    input_type: oci-archive\\n    strip_path_from_result: true\\n  scanners_config:\\n    gitleaks:\\n      binary_path: \\""\\""\\n  scanners_list:\\n  - gitleaks\\n  strip_input_paths: false\\nvulnerabilities:\\n  enabled: true\\n  input_from_sbom: true\\n  inputs: ]\\n  local_image_scan: false\\n  registry: null\\n  scanners_config:\\n    grype:\\n      local_grype_config:\\n        db_root_dir: \\""\\""\\n        listing_file_timeout: 0\\n        listing_url: \\""\\""\\n        local_image_scan: false\\n        max_allowed_built_age: 0\\n        registry: null\\n        scope: \\""\\""\\n        update_db: false\\n        update_timeout: 0\\n      mode: REMOTE\\n      remote_grype_config:\\n        grype_server_address: openclarity-grype-server:9991\\n        grype_server_schemes: ]\\n        grype_server_timeout: 120000000000\\n    trivy:\\n      cache_dir: \\""\\""\\n      registry: null\\n      server_addr: http://openclarity-trivy-server:9992\\n      server_token: \\""\\""\\n      temp_dir: \\""\\""\\n      timeout: 300\\n  scanners_list:\\n  - grype\\n  - trivy\\n"" app=openclarity time=""2024-10-14T16:52:33Z"" level=info msg=""Running scanners..."" app=openclarity time=""2024-10-14T16:52:33Z"" level=info msg=""Running family \\""misconfiguration\\"" in progress..."" app=openclarity family=misconfiguration time=""2024-10-14T16:52:33Z"" level=info msg=""Scanning inputs in progress..."" inputs=""[oci-archive:/mnt/snapshot/image.tar]"" app=openclarity family=misconfiguration time=""2024-10-14T16:52:33Z"" level=info msg=""Scan job \\""lynis\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" scanner=lynis input=""oci-archive:/mnt/snapshot/image.tar"" app=openclarity family=misconfiguration time=""2024-10-14T16:52:33Z"" level=info msg=""Scan job \\""cisdocker\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" app=openclarity family=misconfiguration scanner=cisdocker input=""oci-archive:/mnt/snapshot/image.tar"" time=""2024-10-14T16:52:33Z"" level=warning msg=""Scan job \\""cisdocker\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" app=openclarity family=misconfiguration scanner=cisdocker input=""oci-archive:/mnt/snapshot/image.tar"" error=""unsupported source type=oci-archive"" time=""2024-10-14T16:52:33Z"" level=warning msg=""Scan job \\""lynis\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" error=""failed to convert input to filesystem: failed to expand container to rootfs directory: unable to get image: unable to parse image from src oci-archive:/mnt/snapshot/image.tar: unable to detect input for '/mnt/snapshot/image.tar', errs: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json"" family=misconfiguration scanner=lynis input=""oci-archive:/mnt/snapshot/image.tar"" app=openclarity time=""2024-10-14T16:52:33Z"" level=error msg=""Scanning inputs failed with 1 errors"" error=""scan job \\""cisdocker\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: unsupported source type=oci-archive\\nscan job \\""lynis\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to convert input to filesystem: failed to expand container to rootfs directory: unable to get image: unable to parse image from src oci-archive:/mnt/snapshot/image.tar: unable to detect input for '/mnt/snapshot/image.tar', errs: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json"" app=openclarity family=misconfiguration time=""2024-10-14T16:52:33Z"" level=error msg=""Family \\""misconfiguration\\"" finished with error"" app=openclarity family=misconfiguration error=""failed to process inputs for misconfigurations: scan job \\""cisdocker\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: unsupported source type=oci-archive\\nscan job \\""lynis\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to convert input to filesystem: failed to expand container to rootfs directory: unable to get image: unable to parse image from src oci-archive:/mnt/snapshot/image.tar: unable to detect input for '/mnt/snapshot/image.tar', errs: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json"" time=""2024-10-14T16:52:33Z"" level=info msg=""Running family \\""sbom\\"" in progress..."" family=sbom app=openclarity time=""2024-10-14T16:52:34Z"" level=info msg=""Scanning inputs in progress..."" family=sbom app=openclarity inputs=""[oci-archive:/mnt/snapshot/image.tar]"" time=""2024-10-14T16:52:34Z"" level=info msg=""Generating hash for input /mnt/snapshot/image.tar"" family=sbom app=openclarity time=""2024-10-14T16:52:34Z"" level=info msg=""Skip generating hash in the case of image"" time=""2024-10-14T16:52:34Z"" level=info msg=""Scan job \\""syft\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" app=openclarity family=sbom scanner=syft input=""oci-archive:/mnt/snapshot/image.tar"" time=""2024-10-14T16:52:34Z"" level=info msg=""Scan job \\""trivy\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" family=sbom input=""oci-archive:/mnt/snapshot/image.tar"" scanner=trivy app=openclarity time=""2024-10-14T16:52:34Z"" level=info msg=""Scan job \\""windows\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" app=openclarity family=sbom input=""oci-archive:/mnt/snapshot/image.tar"" scanner=windows time=""2024-10-14T16:52:34Z"" level=info msg=""Scan job \\""gomod\\"" for input oci-archive:/mnt/snapshot/image.tar in progress..."" input=""oci-archive:/mnt/snapshot/image.tar"" app=openclarity family=sbom scanner=gomod time=""2024-10-14T16:52:34Z"" level=warning msg=""Scan job \\""windows\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" family=sbom input=""oci-archive:/mnt/snapshot/image.tar"" error=""skipping analyzing unsupported source type: oci-archive"" scanner=windows app=openclarity time=""2024-10-14T16:52:34Z"" level=warning msg=""Scan job \\""gomod\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" input=""oci-archive:/mnt/snapshot/image.tar"" app=openclarity error=""unsupported input type=oci-archive"" family=sbom scanner=gomod time=""2024-10-14T16:52:34Z"" level=warning msg=""Scan job \\""syft\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" input=""oci-archive:/mnt/snapshot/image.tar"" error=""failed to create source analyzer=syft: an error occurred attempting to resolve '/mnt/snapshot/image.tar': oci-archive: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json"" app=openclarity family=sbom scanner=syft 2024/10/14 16:52:35 INFO Java DB Repository repository=ghcr.io/aquasecurity/trivy-db:2 2024/10/14 16:52:35 INFO Downloading the Java DB... time=""2024-10-14T16:52:35Z"" level=warning msg=""Scan job \\""trivy\\"" for input oci-archive:/mnt/snapshot/image.tar failed"" family=sbom input=""oci-archive:/mnt/snapshot/image.tar"" error=""failed to generate SBOM: image scan error: scan error: scan failed: failed analysis: analyze error: pipeline error: failed to analyze layer (sha256:bff6683ea42c9216ad9427b6487fd00feadce5219a3a26e2923d563f6c4c865e): post analysis error: post analysis error: Unable to initialize the Java DB: Java DB update failed: DB download error: unacceptable media type: application/vnd.aquasec.trivy.db.layer.v1.tar+gzip"" scanner=trivy app=openclarity time=""2024-10-14T16:52:35Z"" level=error msg=""Scanning inputs failed with 1 errors"" error=""scan job \\""windows\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: skipping analyzing unsupported source type: oci-archive\\nscan job \\""gomod\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: unsupported input type=oci-archive\\nscan job \\""syft\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to create source analyzer=syft: an error occurred attempting to resolve '/mnt/snapshot/image.tar': oci-archive: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json\\nscan job \\""trivy\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to generate SBOM: image scan error: scan error: scan failed: failed analysis: analyze error: pipeline error: failed to analyze layer (sha256:bff6683ea42c9216ad9427b6487fd00feadce5219a3a26e2923d563f6c4c865e): post analysis error: post analysis error: Unable to initialize the Java DB: Java DB update failed: DB download error: unacceptable media type: application/vnd.aquasec.trivy.db.layer.v1.tar+gzip"" family=sbom app=openclarity time=""2024-10-14T16:52:35Z"" level=error msg=""Family \\""sbom\\"" finished with error"" app=openclarity error=""failed to process inputs for sbom: scan job \\""windows\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: skipping analyzing unsupported source type: oci-archive\\nscan job \\""gomod\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: unsupported input type=oci-archive\\nscan job \\""syft\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to create source analyzer=syft: an error occurred attempting to resolve '/mnt/snapshot/image.tar': oci-archive: unable to parse OCI directory as an image: unexpected media type for sha256:beee3efa712609c45c5a5822bc633fbf36cb7917cd43108d4edcad8a9f7c1d97: application/vnd.oci.image.index.v1+json\\nscan job \\""trivy\\"" failed for input oci-archive:/mnt/snapshot/image.tar, reason: failed to generate SBOM: image scan error: scan error: scan failed: failed analysis: analyze error: pipeline error: failed to analyze layer (sha256:bff6683ea42c9216ad9427b6487fd00feadce5219a3a26e2923d563f6c4c865e): post analysis error: post analysis error: Unable to initialize the Java DB: Java DB update failed: DB download error: unacceptable media type: application/vnd.aquasec.trivy.db.layer.v1.tar+gzip"" family=sbom panic: runtime error: invalid memory address or nil pointer dereference [signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0xe7115d]  goroutine 103 [running]: github.com/openclarity/openclarity/scanner/families/sbom/types.(*Result).EncodeToBytes(0x0, {0xc00469a980?, 0x0?})     /build/scanner/families/sbom/types/result.go:45 +0x7d github.com/openclarity/openclarity/cli/presenter.(*DefaultPresenter).ExportSbomResult(0xc0061a2618, {0xc004984410?, 0xc003ccb080?}, {{0x65c8c31, 0x4}, {0x5bc2a20, 0x0}, {0x7a94000, 0xc0001e9860}})     /build/cli/presenter/default.go:76 +0x8a github.com/openclarity/openclarity/cli/presenter.(*DefaultPresenter).ExportFamilyResult(0xc0061a2600?, {0x7aee930?, 0xc00650ff50?}, {{0x65c8c31, 0x4}, {0x5bc2a20, 0x0}, {0x7a94000, 0xc0001e9860}})     /build/cli/presenter/default.go:47 +0x8f github.com/openclarity/openclarity/cli/presenter.(*MultiPresenter).ExportFamilyResult(0xc0001e9880?, {0x7aee930, 0xc00650ff50}, {{0x65c8c31, 0x4}, {0x5bc2a20, 0x0}, {0x7a94000, 0xc0001e9860}})     /build/cli/presenter/multi.go:31 +0xd2 github.com/openclarity/openclarity/cli.(*CLI).FamilyFinished(0x0?, {0x7aee930?, 0xc00650ff50?}, {{0x65c8c31, 0x4}, {0x5bc2a20, 0x0}, {0x7a94000, 0xc0001e9860}})     /build/cli/cli.go:65 +0x72 github.com/openclarity/openclarity/scanner/internal/family_runner.(*Runner[...]).Run(0x7a92660, {0x7aee968, 0xc005183b30}, {0x7aac308, 0xc0064ee300}, {0x7ad8998, 0xc007413760})     /build/scanner/internal/family_runner/runner.go:79 +0x79a github.com/openclarity/openclarity/scanner.New.newFamilyTask[...].func1({{0x7aac308, 0xc0064ee300}, {0x7ad8998, 0xc007413760}, 0xc00506aa80})     /build/scanner/scanner.go:211 +0x12a github.com/openclarity/openclarity/workflow/types.Task[...].Run(...)     /build/workflow/types/task.go:36 github.com/openclarity/openclarity/workflow/internal/dispatcher.(*Dispatcher[...]).Dispatch.func1()     /build/workflow/internal/dispatcher/dispatcher.go:121 +0xe8 created by github.com/openclarity/openclarity/workflow/internal/dispatcher.(*Dispatcher[...]).Dispatch in goroutine 100     /build/workflow/internal/dispatcher/dispatcher.go:120 +0x48d Stream closed EOF for openclarity/openclarity-scan-8ba61151-a678-46c2-88c4-21345e319c4e-kp7tv (openclarity-scan-8ba61151-a678-46c2-88c4-21345e319c4e)  ```  ## Affected Version  1.0.0  ## Steps to Reproduce  1. Initiate a scan  ## Checklist  <!-- TODO: Update the link below to point to your project's contributing guidelines --> - [x] I have read the [contributing guidelines](/CONTRIBUTING.md) - [x] I have verified this does not duplicate an existing issue ",2024-10-14T17:10:54+00:00,2024-10-21T09:17:29+00:00,2,https://github.com/openclarity/openclarity/issues/885,897.0,2024-10-24T07:48:43+00:00,https://github.com/openclarity/openclarity/pull/897,0,1,0,1,1,1,0,2,230.6302777777778,kind/bug;area/cli,False,True,critical,configuration,"[{""filename"": ""scanner/families/sbom/types/result.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
openclarity/openclarity,889,fix(cli): check SBOM result before writing,"## Description  <!-- Please provide a meaningful description of what this change will do, or is for. Bonus points for including links to related issues, other PRs, or technical references and some before/after screenshots if the change affects the UI.  | Before | After | | :---: | :---: | | 🖼️ | 🖼️ |  Note that by _not_ including a description, you are asking reviewers to do extra work to understand the context of this change, which may lead to your PR taking much longer to review, or result in it not being reviewed at all. -->  Fixes #885   We can now return an empty byte slice from the parser if there are no SBOM results.  ## Type of Change  - [X] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [X] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [X] Existing issues have been referenced (where applicable) - [X] I have verified this change is not present in other open pull requests - [ ] Functionality is documented - [ ] All code style checks pass - [ ] New code contribution is covered by automated tests - [ ] All new and existing tests pass ",2024-10-17T15:53:34+00:00,2024-10-21T09:17:28+00:00,0,https://github.com/openclarity/openclarity/pull/889,889.0,2024-10-21T09:17:28+00:00,https://github.com/openclarity/openclarity/pull/889,0,2,0,2,106,3,0,109,89.39833333333333,bug;size/M;kind/bug,False,True,normal,ui,"[{""filename"": ""cli/presenter/writer_test.go"", ""lines_added"": 100, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""scanner/families/sbom/types/result.go"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
dotnet/orleans,8766,Serialization: Stack<T> is deserialized in reversed order,Consider following example: ```cs public class MyGrain : IMyGrain {     public Task<Stack<int>> GetStack()     {         var stack = new Stack<int>();         stack.Push(1);         stack.Push(2);         stack.Push(3);         stack.Push(4);         return Task.FromResult(stack);     } } ``` And the calling code: ```cs var stack = await myGrain.GetStack(); var value = stack.Pop(); // returns 1 ```  Here in `StackCodec` during deserealization process values from original stack are just pushed to the new stack what makes the order to be reversed. https://github.com/dotnet/orleans/blob/cd09126368aeb9e5ede05ba6618a5b538e987722/src/Orleans.Serialization/Codecs/StackCodec.cs#L97,2023-12-12T13:12:27+00:00,2023-12-12T18:27:59+00:00,2,https://github.com/dotnet/orleans/issues/8766,8768.0,2023-12-12T18:27:58+00:00,https://github.com/dotnet/orleans/pull/8768,0,2,0,2,164,121,0,285,5.258611111111111,bug;Status: Fixed;area-serialization,False,True,normal,functional,"[{""filename"": ""src/Orleans.Serialization/Codecs/StackCodec.cs"", ""lines_added"": 122, ""lines_deleted"": 121, ""file_type"": ""app_code""}, {""filename"": ""test/Orleans.Serialization.UnitTests/BuiltInCodecTests.cs"", ""lines_added"": 42, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
kanisterio/kanister,3386,[BUG] KanX buffer overflow test produces an enormous amount of log output ...,"**Describe the bug**  KanX buffer overflow test produces an enormous amount of log output ...  KanX buffer overflow test uses the `kando process client output` command to check for buffer overflow in the KanX server.  The `output` sub-command produces lots of log output for the buffer overflow unit test.  **To Reproduce** Steps to reproduce the behavior: 1. Go to '...' 2. Click on '....' 3. Scroll down to '....' 4. See error  **Expected behavior** A clear and concise description of what you expected to happen.  **Screenshots** If applicable, add screenshots to help explain your problem.  **Environment** Kubernetes Version/Provider: ... Storage Provider: ... Cluster Size (#nodes): ... Data Size: ...  **Additional context** Add any other context about the problem here. ",2025-02-26T18:00:13+00:00,2025-02-27T14:20:07+00:00,1,https://github.com/kanisterio/kanister/issues/3386,3387.0,2025-02-27T14:20:06+00:00,https://github.com/kanisterio/kanister/pull/3387,0,1,0,1,1,0,0,1,20.33138888888889,bug;triage,False,True,normal,database,"[{""filename"": ""pkg/kanx/kanx_test.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
openclarity/openclarity,776,fix(api): empty arch failure when vm discovered,"## Description  Currently when the orchestrator finds a VM, it fails to create a new `VMInfo` because the architecture field is empty. So, we need to provide the option of `Unknown` architecture when parsing it.  Cheers to @akijakya for finding this issue!  ## Type of Change  - [x] Bug Fix - [ ] New Feature - [ ] Breaking Change - [ ] Refactor - [ ] Documentation - [ ] Other (please describe)  ## Checklist  - [x] I have read the [contributing guidelines](https://github.com/openclarity/openclarity/blob/main/CONTRIBUTING.md) - [ ] Existing issues have been referenced (where applicable) - [x] I have verified this change is not present in other open pull requests - [ ] Functionality is documented - [x] All code style checks pass - [ ] New code contribution is covered by automated tests - [x] All new and existing tests pass ",2024-08-23T08:30:36+00:00,2024-08-23T09:46:28+00:00,0,https://github.com/openclarity/openclarity/pull/776,776.0,2024-08-23T09:46:28+00:00,https://github.com/openclarity/openclarity/pull/776,0,2,0,2,4,9,0,13,1.2644444444444445,bug,False,True,normal,ui,"[{""filename"": ""api/types/asset.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""provider/aws/discoverer/discoverer.go"", ""lines_added"": 1, ""lines_deleted"": 8, ""file_type"": ""app_code""}]",api,False
openclarity/openclarity,200,chore: debug level log in the case of empty sbom,,2022-06-27T08:46:49+00:00,2022-06-27T11:03:30+00:00,0,https://github.com/openclarity/openclarity/pull/200,200.0,2022-06-27T11:03:30+00:00,https://github.com/openclarity/openclarity/pull/200,0,1,0,1,1,1,0,2,2.2780555555555555,archive/kubeclarity,False,True,normal,functional,"[{""filename"": ""shared/pkg/analyzer/merge.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
kanisterio/kanister,3342,[BUG] The Kopia SnapshotStatsFromSnapshotCreate function fails on ignored/fatal error counts,"**Describe the bug** The `pkg/kopia/command.SnapshotStatsFromSnapshotCreate` function extracts statistics from the Kopia snapshot create progress output. The parser handles lines that look like this: ``` * 0 hashing, 1 hashed (2 B), 3 cached (40 KB), uploaded 6.7 GB, estimated 1092.3 MB (100.0%) 0s left ``` However, Kopia could also emit variants of the above line that break the parser: ``` * 0 hashing, 1 hashed (2 B), 3 cached (40 KB), uploaded 6.7 GB (3 errors ignored), estimated 1092.3 MB (100.0%) 0s left * 0 hashing, 1 hashed (2 B), 3 cached (40 KB), uploaded 6.7 GB (1 fatal error), estimated 1092.3 MB (100.0%) 0s left * 0 hashing, 1 hashed (2 B), 3 cached (40 KB), uploaded 6.7 GB (1 fatal error) (3 errors ignored), estimated 1092.3 MB (100.0%) 0s left ```  **To Reproduce** Ignored errors are issued by Kopia when it encounters a special file: e.g. character or block device files or FIFOs. Kopia ignores these without failing, but tracks their existence and reports the count in the progress message.  **Expected behavior** The `SnapshotStatsFromSnapshotCreate` should ignore these counts.  **Screenshots** If applicable, add screenshots to help explain your problem.  **Environment** Kubernetes Version/Provider: ... Storage Provider: ... Cluster Size (#nodes): ... Data Size: ...  **Additional context** Add any other context about the problem here. ",2025-01-23T00:54:08+00:00,2025-01-23T20:46:56+00:00,1,https://github.com/kanisterio/kanister/issues/3342,3341.0,2025-01-23T20:46:55+00:00,https://github.com/kanisterio/kanister/pull/3341,0,2,0,2,35,21,0,56,19.879722222222224,bug;triage,False,True,normal,database,"[{""filename"": ""pkg/kopia/command/parse_command_output.go"", ""lines_added"": 5, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/kopia/command/parse_command_output_test.go"", ""lines_added"": 30, ""lines_deleted"": 19, ""file_type"": ""app_code""}]",,False
kanisterio/kanister,3232,kanx.Get and bug fixes,"## Change Overview  This PR introduces a new request to kanx: `Get()`.  Much like kanx's `List()` get returns a single process instead of multiple.  Some ""bugs"" have been fixed in `kanx` by improving concurrency (thread safety).  ## Pull request type  Please check the type of change your PR introduces: - [X] :bug: Bugfix - [X] :sunflower: Feature - [X] :robot: Test  ## Issues <!-- to auto-close the issue, add the ""fixes"" keyword -->  - fixes #issue-number  ## Test Plan  - [X] :zap: Unit test ",2024-11-16T01:13:25+00:00,2025-01-14T02:12:18+00:00,0,https://github.com/kanisterio/kanister/pull/3232,3232.0,,https://github.com/kanisterio/kanister/pull/3232,0,7,1,8,398,276,0,669,1416.981388888889,,False,True,normal,functional,"[{""filename"": ""pkg/kando/process_client.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kando/process_client_get.go"", ""lines_added"": 64, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kanx/client.go"", ""lines_added"": 16, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kanx/kanx.pb.go"", ""lines_added"": 138, ""lines_deleted"": 156, ""file_type"": ""app_code""}, {""filename"": ""pkg/kanx/kanx.proto"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""pkg/kanx/kanx_grpc.pb.go"", ""lines_added"": 103, ""lines_deleted"": 115, ""file_type"": ""app_code""}, {""filename"": ""pkg/kanx/kanx_test.go"", ""lines_added"": 32, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/kanx/server.go"", ""lines_added"": 39, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",kanx,False
caoyingjunz/pixiu,525,【Bug】Pixiu 若以 Debug 模式启动前端会出现 get nil user 问题,,2024-08-31T12:03:12+00:00,2024-08-31T16:21:45+00:00,0,https://github.com/caoyingjunz/pixiu/issues/525,526.0,2024-08-31T16:21:44+00:00,https://github.com/caoyingjunz/pixiu/pull/526,0,2,0,2,26,1,0,27,4.308888888888889,,False,True,normal,functional,"[{""filename"": ""api/server/middleware/authentication.go"", ""lines_added"": 13, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/db/user.go"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",user.go;api,True
kanisterio/kanister,3205,Fix incorrect conversion between int types,"## Change Overview  Fixes https://github.com/kanisterio/kanister/issues/3202  In the older version of the logic where we were reading replicas of workload in `ScaleWorkload` function; if val (string type) has an int value that is beyond the range of int32, even though call `strconv.Atoi(val)` would succeed, the next of `replicas = int32(v)` would result into unexpected result because the val is beyond what can be accommodated into int32 type. This change fixes that.  ## Pull request type  Please check the type of change your PR introduces: - [ ] :construction: Work in Progress - [ ] :rainbow: Refactoring (no functional changes, no api changes) - [x] :hamster: Trivial/Minor - [ ] :bug: Bugfix - [ ] :sunflower: Feature - [ ] :world_map: Documentation - [ ] :robot: Test - [ ] :building_construction: Build  ## Issues <!-- to auto-close the issue, add the ""fixes"" keyword -->  - fixes https://github.com/kanisterio/kanister/issues/3202  ## Test Plan  - [ ] :muscle: Manual - [x] :zap: Unit test - [ ] :green_heart: E2E  ``` /kanister/pkg/function (fix-codescanning-incorrect-intconv*) » go test -check.f ""ScaleWorkloadSuite"" OK: 1 passed PASS ok      github.com/kanisterio/kanister/pkg/function     0.894s ``` ",2024-10-25T15:45:00+00:00,2024-10-28T11:04:28+00:00,0,https://github.com/kanisterio/kanister/pull/3205,3205.0,2024-10-28T11:04:28+00:00,https://github.com/kanisterio/kanister/pull/3205,0,2,0,2,48,2,0,50,67.32444444444444,kueue,False,True,normal,ui,"[{""filename"": ""pkg/function/scale_workload.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/scale_workload_test.go"", ""lines_added"": 46, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
radius-project/radius,8709,`rad resource expose containers` fails with Error: only Applications.Core/containers is supported,"### Steps to reproduce  Deploy a Radius app with a container: ```shell ❯ rad deploy /Users/willsmith/dev/radius/samples/samples/demo/app.bicep --application demo -p image=ghcr.io/radius-project/samples/demo:latest  Building /Users/willsmith/dev/radius/samples/samples/demo/app.bicep...  Deployment In Progress...   Completed            db              Applications.Datastores/redisCaches ...                  demo            Applications.Core/containers  Deployment Complete  Resources:     demo            Applications.Core/containers     db              Applications.Datastores/redisCache ```  ### Observed behavior  If you try to port-forward the container, you get an error:  ```shell ❯ rad resource expose containers demo --port 3000 --application demo Error: only Applications.Core/containers is supported ```  ### Desired behavior  The port-forwarding should be successful. We document this in our docs: https://docs.radapp.io/reference/cli/rad_resource_expose/  ### Workaround  if you use `Applications.Core/containers` instead of `containers` it works.  ```shell ❯ rad resource expose Applications.Core/containers demo --port 3000 --application demo Exposing replica demo-f9f65bf66-w4s9v Forwarding from 127.0.0.1:3000 -> 3000 Forwarding from [::1]:3000 -> 3000 ```  ### rad Version  ```shell ❯ rad version RELEASE   VERSION   BICEP     COMMIT edge      bbb1d24   0.33.93   bbb1d240b6a152fcec3c7df12b17b40beff95e24 ```  ### Operating system  macOS M1  ### Additional context  _No response_  ### Would you like to support us?  - [x] Yes, I would like to support you  [AB#14732](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/14732)",2025-02-28T23:56:25+00:00,2025-03-01T00:06:26+00:00,2,https://github.com/radius-project/radius/issues/8709,8710.0,,https://github.com/radius-project/radius/pull/8710,0,1,0,1,1,1,0,2,0.1669444444444444,bug,False,True,normal,database,"[{""filename"": ""cmd/rad/cmd/resourceExpose.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
lakesoul-io/LakeSoul,502,[Flink] Fix flink select only partition column,,2024-06-27T09:51:03+00:00,2024-06-27T11:44:18+00:00,0,https://github.com/lakesoul-io/LakeSoul/pull/502,502.0,2024-06-27T11:44:18+00:00,https://github.com/lakesoul-io/LakeSoul/pull/502,0,4,0,4,93,23,0,116,1.8875,bug;flink,False,True,normal,functional,"[{""filename"": ""lakesoul-flink/src/main/java/org/apache/flink/lakesoul/source/LakeSoulOneSplitRecordsReader.java"", ""lines_added"": 3, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-flink/src/test/java/org/apache/flink/lakesoul/test/AbstractTestBase.java"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-flink/src/test/java/org/apache/flink/lakesoul/test/flinkSource/DMLSuite.java"", ""lines_added"": 84, ""lines_deleted"": 15, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-flink/src/test/java/org/apache/flink/lakesoul/test/flinkSource/TestUtils.java"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
kanisterio/kanister,3097,Fix a bug in the code where we merge actionset and bp labels/annotations,"## Change Overview  In the code where we are merging the labels/annotations passed via actionset with the labels/annotations configured inthe blueprint we checked if tp.Annotations is nil and then merged both the annotations. While doing this we introduced a bug, where if the actionset labels and annotations are nil, and even though blueprint annotations are proivided the kanister function created pod would not have the correct labels/annotations.  Even though this might not be a problem if the actionset is created by the kanctl command it would be a problem if the pod labels and annotations set to nil by creating the actionset resource yaml manually.  This PR fixes that.  ## Pull request type  Please check the type of change your PR introduces: - [ ] :construction: Work in Progress - [ ] :rainbow: Refactoring (no functional changes, no api changes) - [x] :hamster: Trivial/Minor - [ ] :bug: Bugfix - [ ] :sunflower: Feature - [ ] :world_map: Documentation - [ ] :robot: Test - [ ] :building_construction: Build  ## Issues <!-- to auto-close the issue, add the ""fixes"" keyword -->  - fixes #issue-number  ## Test Plan  <!-- Will run prior to merging.--> <!-- Include example how to run.-->  - [ ] :muscle: Manual - [ ] :zap: Unit test - [x] :green_heart: E2E  ``` make integration-test ... 2024/09/09 11:21:05 Completed E2E TestPodLabelsAndAnnotations OK: 3 passed --- PASS: Test (140.91s) PASS ok      github.com/kanisterio/kanister/pkg/testing      141.056s /go/src/github.com/kanisterio/kanister real 144.24 user 0.01 sys 0.01 ```",2024-09-09T11:22:01+00:00,2024-09-09T19:03:49+00:00,0,https://github.com/kanisterio/kanister/pull/3097,3097.0,2024-09-09T19:03:49+00:00,https://github.com/kanisterio/kanister/pull/3097,0,14,0,14,27,14,0,41,7.696666666666666,kueue,False,True,normal,configuration,"[{""filename"": ""pkg/function/backup_data_stats.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/checkRepository.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/copy_volume_data.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/delete_data.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/delete_data_all.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/delete_data_using_kopia_server.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/export_rds_snapshot_location.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/kube_task.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/prepare_data.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/restore_data.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/restore_data_all.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/restore_data_using_kopia_server.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/restore_rds_snapshot.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/testing/e2e_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",function,False
kanisterio/kanister,2925,fix: Make sure all storage providers return similar error if snapshot doesn't exist,"## Change Overview  `DeleteVolumeSnapshot` function matches error returned from SnapshotGet with a 'does not exist' string.  This PR makes it so all providers wrap their errors with ""Snapshot does not exist"" message if they detect that snapshot does not exist in SnapshotGet  **NOTE:** technically SnapshotDelete should be idempotent, but it seems like `DeleteVolumeSnapshot` adds an extra check for logging purposes. Also we could have done some better error matching then just comparing strings. Although long term goal is to remove storage providers and move to CSI interfaces, hence it doesn't make sense to spend too much time on providers.  ## Pull request type  Please check the type of change your PR introduces: - [ ] :construction: Work in Progress - [ ] :rainbow: Refactoring (no functional changes, no api changes) - [ ] :hamster: Trivial/Minor - [x] :bug: Bugfix - [ ] :sunflower: Feature - [ ] :world_map: Documentation - [ ] :robot: Test - [ ] :building_construction: Build  ## Issues <!-- to auto-close the issue, add the ""fixes"" keyword -->  - fixes #issue-number  ## Test Plan  - We need to make sure we run `blockstorage_test.go` for azure, aws and gcp. - @mdsoysa can you please provide a test scenario to reproduce the original issue?  - [ ] :muscle: Manual - [x] :zap: Unit test - [x] :green_heart: E2E ",2024-06-05T20:06:07+00:00,2024-06-06T19:52:12+00:00,0,https://github.com/kanisterio/kanister/pull/2925,2925.0,2024-06-06T19:52:12+00:00,https://github.com/kanisterio/kanister/pull/2925,0,6,0,6,24,4,0,28,23.76805555555556,kueue,False,True,normal,ui,"[{""filename"": ""pkg/blockstorage/awsebs/awsebs.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/blockstorage/azure/azuredisk.go"", ""lines_added"": 13, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/blockstorage/blockstorage.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/blockstorage/blockstorage_test.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/blockstorage/gcepd/gcepd.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/function/delete_volume_snapshot.go"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
lakesoul-io/LakeSoul,284,[NativeIO] Filters should be ignored when file does not contain the field,"For multi-stream upsert, one upserted file may not contain all fields of the table. When applying filters we should ignore those having field not in the file",2023-07-25T03:20:42+00:00,2023-09-27T05:21:51+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/284,341.0,2023-09-27T05:21:50+00:00,https://github.com/lakesoul-io/LakeSoul/pull/341,0,4,14,18,861,249,0,16,1538.0188888888888,bug;native-io,False,True,normal,functional,"[{""filename"": ""lakesoul-common/src/main/java/com/dmetasoul/lakesoul/meta/dao/PartitionInfoDao.java"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-flink/src/main/java/org/apache/flink/lakesoul/source/ParquetFilters.java"", ""lines_added"": 1, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-flink/src/main/java/org/apache/flink/lakesoul/table/LakeSoulTableSource.java"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-spark/src/main/java/org/apache/spark/sql/execution/datasources/parquet/NativeVectorizedReader.java"", ""lines_added"": 2, ""lines_deleted"": 7, ""file_type"": ""app_code""}, {""filename"": ""lakesoul-spark/src/main/scala/org/apache/spark/sql/execution/datasources/v2/merge/parquet/Native/NativeMergeParquetPartitionReaderFactory.scala"", ""lines_added"": 31, ""lines_deleted"": 30, ""file_type"": ""other""}, {""filename"": ""lakesoul-spark/src/main/scala/org/apache/spark/sql/execution/datasources/v2/merge/parquet/batch/merge_operator/MergeOperator.scala"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""lakesoul-spark/src/main/scala/org/apache/spark/sql/lakesoul/catalog/LakeSoulScanBuilder.scala"", ""lines_added"": 20, ""lines_deleted"": 1, ""file_type"": ""other""}, {""filename"": ""lakesoul-spark/src/test/scala/org/apache/spark/sql/lakesoul/test/TestUtils.scala"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""rust/Cargo.lock"", ""lines_added"": 46, ""lines_deleted"": 28, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/datasource/mod.rs"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/datasource/parquet_source.rs"", ""lines_added"": 636, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/filter/mod.rs"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/filter/parser.rs"", ""lines_added"": 18, ""lines_deleted"": 17, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/lakesoul_reader.rs"", ""lines_added"": 11, ""lines_deleted"": 148, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/lib.rs"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/projection/mod.rs"", ""lines_added"": 67, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/sorted_merge/merge_operator.rs"", ""lines_added"": 14, ""lines_deleted"": 6, ""file_type"": ""other""}, {""filename"": ""rust/lakesoul-io/src/sorted_merge/sorted_stream_merger.rs"", ""lines_added"": 0, ""lines_deleted"": 1, ""file_type"": ""other""}]",catalog,False
lakesoul-io/LakeSoul,209,[NativeIO] fix memory leak in native reader,Close #208 ,2023-05-09T10:05:31+00:00,2023-05-09T10:22:39+00:00,0,https://github.com/lakesoul-io/LakeSoul/pull/209,209.0,2023-05-09T10:22:39+00:00,https://github.com/lakesoul-io/LakeSoul/pull/209,0,1,3,4,25,19,0,1,0.2855555555555555,bug;native-io,False,True,normal,performance,"[{""filename"": ""native-io/lakesoul-io-c/src/lib.rs"", ""lines_added"": 14, ""lines_deleted"": 12, ""file_type"": ""other""}, {""filename"": ""native-io/lakesoul-io-java/src/main/java/com/dmetasoul/lakesoul/lakesoul/io/NativeIOReader.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""native-io/lakesoul-io-java/src/main/scala/com/dmetasoul/lakesoul/LakeSoulArrowReader.scala"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""native-io/lakesoul-io/src/lakesoul_reader.rs"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,208,[NativeIO] Memory leak in reader,,2023-05-09T09:53:21+00:00,2023-05-09T10:22:40+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/208,209.0,2023-05-09T10:22:39+00:00,https://github.com/lakesoul-io/LakeSoul/pull/209,0,1,3,4,25,19,0,1,0.4883333333333333,bug;native-io,False,True,normal,performance,"[{""filename"": ""native-io/lakesoul-io-c/src/lib.rs"", ""lines_added"": 14, ""lines_deleted"": 12, ""file_type"": ""other""}, {""filename"": ""native-io/lakesoul-io-java/src/main/java/com/dmetasoul/lakesoul/lakesoul/io/NativeIOReader.java"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""native-io/lakesoul-io-java/src/main/scala/com/dmetasoul/lakesoul/LakeSoulArrowReader.scala"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""native-io/lakesoul-io/src/lakesoul_reader.rs"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,176,fix native_io_timestamp_conversion for default case,,2023-03-20T11:24:33+00:00,2023-03-20T11:27:28+00:00,0,https://github.com/lakesoul-io/LakeSoul/pull/176,176.0,2023-03-20T11:27:28+00:00,https://github.com/lakesoul-io/LakeSoul/pull/176,0,0,1,1,3,0,0,0,0.0486111111111111,bug;native-io,False,True,normal,functional,"[{""filename"": ""native-io/lakesoul-io/src/default_column_stream/default_column_stream.rs"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""other""}]",,False
lakesoul-io/LakeSoul,153,[Bug][Flink][NativeIO] RecordBatch may be in use in rust but its buffer is overridden during write ,,2023-02-08T09:03:02+00:00,2023-02-13T03:48:40+00:00,0,https://github.com/lakesoul-io/LakeSoul/issues/153,157.0,2023-02-13T03:48:39+00:00,https://github.com/lakesoul-io/LakeSoul/pull/157,0,1,0,1,3,1,0,4,114.76027777777776,bug;flink;native-io,False,True,normal,functional,"[{""filename"": ""lakesoul-flink/src/main/java/org/apache/flink/lakesoul/sink/writer/NativeParquetWriter.java"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
lakesoul-io/LakeSoul,81,fix MultiPartitionMergeBucketScan bug,"fix bug: when bucketId's isSingleFile is false firstly then true, the value MergeFilePartition's isSingleFile should be false",2022-09-04T07:12:44+00:00,2022-09-04T07:13:57+00:00,0,https://github.com/lakesoul-io/LakeSoul/pull/81,81.0,2022-09-04T07:13:57+00:00,https://github.com/lakesoul-io/LakeSoul/pull/81,0,0,1,1,3,1,0,0,0.0202777777777777,,False,True,normal,functional,"[{""filename"": ""lakesoul-spark/src/main/scala/org/apache/spark/sql/execution/datasources/v2/merge/MergeParquetScan.scala"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
SwissLife-OSS/squadron,114,Library doesn't work with docker desktop v4.7.0,"When updated docker desktop to v4.7.0 he library stopped working and throws exceptions: ` Squadron.ContainerException : Error in ImageExists: mongo:latest ---- Docker.DotNet.DockerApiException : Docker API responded with status code=BadRequest, response=400 Bad Request `  Steps to reproduce the behavior: 1. Update docker desktop to v4.7.0 2. Run tests using squadrom's mongo instance  **Desktop (please complete the following information):**  - OS: Windows 10  - Version 21H1 (19043.1466) ",2022-04-15T07:31:16+00:00,2022-04-19T09:39:44+00:00,2,https://github.com/SwissLife-OSS/squadron/issues/114,115.0,2022-04-15T20:41:35+00:00,https://github.com/SwissLife-OSS/squadron/pull/115,0,1,1,2,13,5,0,14,13.171944444444444,bug,False,True,normal,functional,"[{""filename"": ""src/Core/Core.csproj"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""other""}, {""filename"": ""src/Core/DockerContainerManager.cs"", ""lines_added"": 11, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
apache/incubator-kie-kogito-apps,1775,KOGITO-9386: [SWF Dev UI] Custom dashboards list has invalid date and broken cards,"Many thanks for submitting your Pull Request :heart:!   Please make sure that your PR meets the following requirements:  - [ ] You have read the [contributors guide](https://github.com/kiegroup/kogito-runtimes#contributing-to-kogito) - [ ] Pull Request title is properly formatted: `KOGITO-XYZ Subject` - [ ] Pull Request title contains the target branch if not targeting main: `[0.9.x] KOGITO-XYZ Subject` - [ ] Pull Request contains link to the JIRA issue - [ ] Pull Request contains link to any dependent or related Pull Request - [ ] Pull Request contains description of the issue - [ ] Pull Request does not include fixes for issues other than the main ticket  <details> <summary> How to replicate CI configuration locally? </summary>  Build Chain tool does ""simple"" maven build(s), the builds are just Maven commands, but because the repositories relates and depends on each other and any change in API or class method could affect several of those repositories there is a need to use [build-chain tool](https://github.com/kiegroup/github-action-build-chain) to handle cross repository builds and be sure that we always use latest version of the code for each repository.   [build-chain tool](https://github.com/kiegroup/github-action-build-chain) is a build tool which can be used on command line locally or in Github Actions workflow(s), in case you need to change multiple repositories and send multiple dependent pull requests related with a change you can easily reproduce the same build by executing it on Github hosted environment or locally in your development environment. See [local execution](https://github.com/kiegroup/github-action-build-chain#local-execution) details to get more information about it. </details>  <details> <summary> How to retest this PR or trigger a specific build: </summary>  - for <b>pull request checks</b>     Please add comment: <b>Jenkins retest this</b>  - for a <b>specific pull request check</b>     Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] tests</b>  - for <b>quarkus branch checks</b>     Run checks against Quarkus current used branch     Please add comment: <b>Jenkins run quarkus-branch</b>  - for a <b>quarkus branch specific check</b>     Run checks against Quarkus current used branch     Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] quarkus-branch</b>  - for <b>quarkus main checks</b>     Run checks against Quarkus main branch     Please add comment: <b>Jenkins run quarkus-main</b>  - for a <b>specific quarkus main check</b>     Run checks against Quarkus main branch     Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] quarkus-main</b>  - for <b>quarkus lts checks</b>     Run checks against Quarkus lts branch     Please add comment: <b>Jenkins run quarkus-lts</b>  - for a <b>specific quarkus lts check</b>     Run checks against Quarkus lts branch     Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] quarkus-lts</b>   - for <b>native checks</b>     Run native checks     Please add comment: <b>Jenkins run native</b>  - for a <b>specific native check</b>     Run native checks    Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] native</b>  - for <b>native lts checks</b>     Run native checks against quarkus lts branch   Please add comment: <b>Jenkins run native-lts</b>  - for a <b>specific native lts check</b>     Run native checks against quarkus lts branch   Please add comment: <b>Jenkins (re)run [kogito-apps|kogito-examples] native-lts</b>   </details>  <details> <summary> How to backport a pull request to a different branch? </summary>  In order to automatically create a **backporting pull request** please add one or more labels having the following format `backport-<branch-name>`, where `<branch-name>` is the name of the branch where the pull request must be backported to (e.g., `backport-7.67.x` to backport the original PR to the `7.67.x` branch).  > **NOTE**: **backporting** is an action aiming to move a change (usually a commit) from a branch (usually the main one) to another one, which is generally referring to a still maintained release branch. Keeping it simple: it is about to move a specific change or a set of them from one branch to another.  Once the original pull request is successfully merged, the automated action will create one backporting pull request per each label (with the previous format) that has been added.  If something goes wrong, the author will be notified and at this point a manual backporting is needed.  > **NOTE**: this automated backporting is triggered whenever a pull request on `main` branch is labeled or closed, but both conditions must be satisfied to get the new PR created. </details>  <details> <summary> Quarkus-3 PR check is failing ... what to do ? </summary> The Quarkus 3 check is applying patches from the `.ci/environments/quarkus-3/patches`.  The first patch, called `0001_before_sh.patch`, is generated from Openrewrite `.ci/environments/quarkus-3/quarkus3.yml` recipe. The patch is created to speed up the check. But it may be that some changes in the PR broke this patch.   No panic, there is an easy way to regenerate it. You just need to comment on the PR: ``` jenkins rewrite quarkus-3 ``` and it should, after some minutes (~20/30min) apply a commit on the PR with the patch regenerated.  Other patches were generated manually. If any of it fails, you will need to manually update it... and push your changes. </details>",2023-06-14T18:24:28+00:00,2023-06-14T22:06:06+00:00,2,https://github.com/apache/incubator-kie-kogito-apps/pull/1775,1775.0,2023-06-14T22:06:06+00:00,https://github.com/apache/incubator-kie-kogito-apps/pull/1775,0,1,4,5,26,1391,0,20,3.693888888888889,,False,True,critical,configuration,"[{""filename"": ""kogito-quarkus-serverless-workflow-devui-parent/kogito-quarkus-serverless-workflow-devui/src/main/java/org/kie/kogito/swf/tools/custom/dashboard/model/CustomDashboardInfo.java"", ""lines_added"": 10, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""ui-packages/packages/custom-dashboard-list/src/envelope/components/CustomDashboardList/CustomDashboardList.tsx"", ""lines_added"": 10, ""lines_deleted"": 12, ""file_type"": ""other""}, {""filename"": ""ui-packages/packages/custom-dashboard-list/src/envelope/components/CustomDashboardList/tests/CustomDashboardList.test.tsx"", ""lines_added"": 6, ""lines_deleted"": 5, ""file_type"": ""other""}, {""filename"": ""ui-packages/packages/custom-dashboard-list/src/envelope/components/CustomDashboardList/tests/__snapshots__/CustomDashboardList.test.tsx.snap"", ""lines_added"": 0, ""lines_deleted"": 1219, ""file_type"": ""other""}, {""filename"": ""ui-packages/packages/custom-dashboard-list/src/envelope/tests/__snapshots__/CustomDashboardListEnvelopeView.test.tsx.snap"", ""lines_added"": 0, ""lines_deleted"": 145, ""file_type"": ""other""}]",kogito-quarkus-serverless-workflow-devui-parent,False
SwissLife-OSS/squadron,80,Fix initialize sequence logging bug,Fixes bug where the container logs are not being properly inserted in exception on initialize error.,2021-01-15T15:43:26+00:00,2021-01-22T14:36:33+00:00,2,https://github.com/SwissLife-OSS/squadron/pull/80,80.0,2021-01-22T14:36:33+00:00,https://github.com/SwissLife-OSS/squadron/pull/80,0,1,0,1,6,4,0,10,166.8852777777778,,False,True,normal,functional,"[{""filename"": ""src/Core/ContainerInitializer.cs"", ""lines_added"": 6, ""lines_deleted"": 4, ""file_type"": ""app_code""}]",,False
radius-project/radius,8400,Scheduled functional test failed - Run ID: 13122328583,"## Bug information   This issue is automatically generated if the scheduled functional test fails. The Radius functional test operates on a schedule of every 4 hours during weekdays and every 12 hours over the weekend. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13122328583).  [AB#14347](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/14347)",2025-02-03T20:50:40+00:00,2025-02-18T21:07:10+00:00,2,https://github.com/radius-project/radius/issues/8400,5476.0,,https://github.com/radius-project/radius/pull/5476,0,9,0,9,746,703,0,1449,360.275,bug;test-failure,False,True,major,networking,"[{""filename"": ""pkg/cli/cmd/env/create/create.go"", ""lines_added"": 0, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/env/create/create_test.go"", ""lines_added"": 0, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/radinit/init.go"", ""lines_added"": 11, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/radinit/init_test.go"", ""lines_added"": 207, ""lines_deleted"": 183, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/radinit/recipe_util.go"", ""lines_added"": 130, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/cli/cmd/radinit/recipe_util_test.go"", ""lines_added"": 80, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/corerp/frontend/controller/environments/createorupdateenvironment.go"", ""lines_added"": 27, ""lines_deleted"": 131, ""file_type"": ""app_code""}, {""filename"": ""pkg/corerp/frontend/controller/environments/createorupdateenvironment_test.go"", ""lines_added"": 290, ""lines_deleted"": 359, ""file_type"": ""app_code""}, {""filename"": ""pkg/corerp/frontend/controller/environments/types.go"", ""lines_added"": 1, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",,False
vmware-tanzu/kubeapps,8153,"An error occurred while fetching the catalog: Unable to get the available package detail for the package ""bitnami/mysql"" using the plugin ""helm.packages"": internal: Unable to retrieve chart files: sql: no rows in result set. ","### Name and Version  bitnami/mysql  ### What architecture are you using?  amd64  ### What steps will reproduce the bug?  When I searched for mysql, tomcat, and redis using kubeapps, I was able to find these applications, but when I deployed them, it prompted: ""An error occurred while fetching the catalog: Unable to get the available package detail for the package ""bitnami/mysql"" using the plugin ""helm.packages"": internal: Unable to retrieve chart files: sql: no rows in result set."" But helm search was able to search and install them. I found the mysql record in the database files, and indeed did not find the record of 12.1.0, but I found the record of 12.0.0, and then I changed 12.0.0 to 12.1.0 and it succeeded. There are many similar situations, such as redis-cluster. The root cause is that the records in the files table in PostgreSQL have not been updated in a timely manner. This problem is easy to reproduce. For any installed or newly installed kubeapps, the same error will appear when searching for mysql, because after Syncing Package Repositories is completed, there is no latest sql record in postgres.    ### Are you using any custom parameters or values?  no  ### What is the expected behavior?  _No response_  ### What do you see instead?   ![image](https://github.com/user-attachments/assets/0119cb4b-fe11-4999-927e-bcf694727da2) ![image](https://github.com/user-attachments/assets/137a8cd4-ebef-4e50-9fbe-c922650acc32) ![image](https://github.com/user-attachments/assets/5747a51b-72b0-460c-b997-904ba43d94a8) ![image](https://github.com/user-attachments/assets/68b5a3de-e593-4806-a848-e357ad0ce940) ![image](https://github.com/user-attachments/assets/3956d556-0e5c-4638-b348-e27dbc9aaa48) ![image](https://github.com/user-attachments/assets/bfa3b214-4417-4712-be60-94908aa5ea98) ![image](https://github.com/user-attachments/assets/5f28c137-00b0-4c60-ac77-f49df1c078a9) ![image](https://github.com/user-attachments/assets/a92bd93d-7635-4d93-8869-2f61f0d37973)  The following is the page that can be accessed normally after modification ![image](https://github.com/user-attachments/assets/dbd4ff74-77be-4362-8b67-f5c266f96d62)  Kubeapps accesses lastest by default, but there is no latest record in postgres, so this error is reported. I have confirmed that Syncing Package Repositories is completed The problem is that the kubeapps scheduled task apprepo-kubeapps-sync cannot pull the latest data when pulling data from https://charts.bitnami.com/bitnami and storing it in postgresql.  The same problem also exists in nginx. The last version 18.2.6 is not recorded in the database, but 18.2.5 is, so it is possible to directly modify the URL to 18.2.5. This is obviously a BUG.  For example: http:kubeapps:80/proxy/#/c/default/ns/test/packages/helm.packages/v1alpha1/default/kubeapps/bitnami%2Fnginx/versions/18.2.6 Modified to: http:kubeapps:80/proxy/#/c/default/ns/test/packages/helm.packages/v1alpha1/default/kubeapps/bitnami%2Fnginx/versions/18.2.5  ![image](https://github.com/user-attachments/assets/d6bdc460-ba44-4905-895b-bd5e21d8c891)  ![image](https://github.com/user-attachments/assets/e76c60c3-e88c-4b14-b341-424a1da1e232)  ### Additional information  _No response_",2024-11-29T10:04:38+00:00,2024-12-24T08:06:54+00:00,6,https://github.com/vmware-tanzu/kubeapps/issues/8153,8184.0,2024-12-24T08:06:53+00:00,https://github.com/vmware-tanzu/kubeapps/pull/8184,0,5,0,5,45,25,0,70,598.0375,kind/bug,False,True,normal,database,"[{""filename"": ""cmd/asset-syncer/server/utils.go"", ""lines_added"": 27, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""cmd/kubeapps-apis/plugins/helm/packages/v1alpha1/server.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""cmd/kubeapps-apis/plugins/helm/packages/v1alpha1/utils/chart-client.go"", ""lines_added"": 12, ""lines_deleted"": 14, ""file_type"": ""app_code""}, {""filename"": ""cmd/kubeapps-apis/plugins/helm/packages/v1alpha1/utils/chart-client_test.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""cmd/kubeapps-apis/plugins/helm/packages/v1alpha1/utils/fake/chart.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",cmd;asset-syncer,True
radius-project/radius,8561,Scheduled long running test failed - Run ID: 13397123864,"## Bug information   This issue is automatically generated if the scheduled long running test fails. The Radius long running test operates on a schedule of every 2 hours everyday. It's important to understand that the test may fail due to workflow infrastructure issues, like network problems, rather than the flakiness of the test itself. For the further investigation, please visit [here](https://github.com/radius-project/radius/actions/runs/13397123864).  [AB#14547](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/14547)",2025-02-18T18:07:18+00:00,2025-02-18T18:42:32+00:00,2,https://github.com/radius-project/radius/issues/8561,6100.0,2023-08-23T00:01:06+00:00,https://github.com/radius-project/radius/pull/6100,0,13,0,13,253,31,0,284,-13098.103333333333,bug;test-failure,False,True,major,networking,"[{""filename"": ""pkg/armrpc/asyncoperation/controller/result.go"", ""lines_added"": 1, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/linkrp/backend/controller/createorupdateresource.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/linkrp/backend/controller/deleteresource.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/driver/bicep.go"", ""lines_added"": 9, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/driver/bicep_test.go"", ""lines_added"": 8, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/driver/terraform.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/driver/terraform_test.go"", ""lines_added"": 22, ""lines_deleted"": 6, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/error.go"", ""lines_added"": 58, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/error_test.go"", ""lines_added"": 104, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/errorcodes.go"", ""lines_added"": 31, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/recipes/terraform/execute.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/rp/util/registry.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""test/functional/shared/resources/recipe_bicep_test.go"", ""lines_added"": 5, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1626,"[BUG] CRR is blocking, if recreate a pod which is already during recreate","<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: Apply two CRRs to recreate the same pod, there is probability to happen: there is only one CRR reaches ```Completed```, but the other one stay in ```Pending``` unless event ( label the CRR) to trigger reconcile  **What you expected to happen**:   **How to reproduce it (as minimally and precisely as possible)**: 0. prepare pod ``` $ k get pod --selector=""app""=sample NAME       READY   STATUS    RESTARTS   AGE sample-0   1/1     Running   0          3s sample-1   1/1     Running   0          2s ``` 1. apply two CRRs to recreate pod same named ```sample-0``` ``` $ k apply -f crrs.yaml ``` ```yaml $ cat crrs.yaml  --- apiVersion: apps.kruise.io/v1alpha1 kind: ContainerRecreateRequest metadata:   namespace: default   name: crr # crr to recreate sample-0 spec:   podName: sample-0   containers:         - name: nginx  --- apiVersion: apps.kruise.io/v1alpha1 kind: ContainerRecreateRequest metadata:   namespace: default   name: crr2 # crr2 also to recreate sample-0 spec:   podName: sample-0   containers:          - name: nginx ```  2. wait for seconds, crr is completed, but crr2 is in ```Pending``` permanently ``` $ k get crr NAME   PHASE       POD        NODE           AGE crr    Pending     sample-0   kind-worker2   36s.  # stay in Pending unless label to trigger reconcile crr2   Completed   sample-0   kind-worker2   38s  $ k get pod --selector=""app""=sample NAME       READY   STATUS    RESTARTS      AGE sample-0   1/1     Running   1 (37s ago)   85s sample-1   1/1     Running   0             84s ``` **Anything else we need to know?**:  **Environment**: - Kruise version: ```1.6.3``` - Kubernetes version (use `kubectl version`): ```1.27``` - Install details (e.g. helm install args): ```helm install kruise openkruise/kruise``` - Others: ",2024-05-21T08:06:16+00:00,2025-01-04T14:19:50+00:00,5,https://github.com/openkruise/kruise/issues/1626,1653.0,,https://github.com/openkruise/kruise/pull/1653,0,1,0,1,1,1,0,2,5478.226111111111,kind/bug;wontfix,False,True,normal,configuration,"[{""filename"": ""pkg/daemon/containerrecreate/crr_daemon_controller.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1869,bugfix: pub webhook no longer requires pod ownerRef to exist,"<!--  Please make sure you have read and understood the contributing guidelines; https://github.com/openkruise/kruise/blob/master/CONTRIBUTING.md -->  ### Ⅰ. Describe what this PR does   ### Ⅱ. Does this pull request fix one issue? <!--If so, add ""fixes #xxxx"" below in the next line, for example, fixes #15. Otherwise, add ""NONE"" -->  ### Ⅲ. Describe how to verify it   ### Ⅳ. Special notes for reviews  ",2024-12-27T08:09:19+00:00,2025-01-03T01:42:35+00:00,1,https://github.com/openkruise/kruise/pull/1869,1869.0,2025-01-03T01:42:35+00:00,https://github.com/openkruise/kruise/pull/1869,0,2,0,2,286,10,0,296,161.55444444444444,,False,True,normal,ui,"[{""filename"": ""pkg/controller/podunavailablebudget/pub_pod_event_handler.go"", ""lines_added"": 9, ""lines_deleted"": 10, ""file_type"": ""app_code""}, {""filename"": ""pkg/webhook/pod/mutating/pod_unavailable_budget_test.go"", ""lines_added"": 277, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1814,ImageListPullJob with imagePullPolicy=Always can't pull latest tag image when the image has changed,"## what's wrong I created a ImageListPullJob to pull images in some nodes,and set imagePullPolicy: Always to pull the latest tag whatever image exists in these nodes. but unexpectedly,it's not working,when i build a new image and tag it  harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:latest,i push it to my harbor registry,repoDigests update to ""sha256:d9d70b3d"".the next day imagepulljob run again,imageid in all nodes don't update,then i clean all images in node01,it can pull the real latest iamges.  ### image inspect in node04 ""status"": {     ""id"": ""sha256:d5b08854cde36b2531131773a36b6920ee650b3575dbe91982451a39760dca5e"",     ""repoTags"": [       ""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:latest""     ],     ""repoDigests"": [       ""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:41f571b072b5f0a5ee85b1393dbd95de6bc3cc13a5bc826a412f9a0f066e6c6e""     ],     ""size"": ""30127273"",     ""uid"": null,     ""username"": """",     ""spec"": null,     ""pinned"": false   } ### image inspect in node01 ""status"": {     ""id"": ""sha256:91849a5248a24bf524879ddb34192ddd291dc547c229abc6364d3d25c0db7788"",     ""repoTags"": [       ""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:latest""     ],     ""repoDigests"": [       ""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:d9d70b3d97587d027ce12d773cca1ed66e6d16dd8a563a5b9c7c216d84d7dbda""     ],     ""size"": ""30127842"",     ""uid"": null,     ""username"": """",     ""spec"": null,     ""pinned"": false   }  ## version k8s:v1.26.15 containerd://1.7.13 kruise:v1.7.2  ## ImageListPullJob: ``` apiVersion: apps.kruise.io/v1alpha1 kind: ImageListPullJob metadata:   name: job-with-always spec:   images:   - 'harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:latest'   imagePullPolicy: Always   parallelism: 2      # [optional] the maximal number of Nodes that pull this image at the same time, defaults to 1     #  pullSecrets:     #   - idc-harbor   selector:            # [optional] the names or label selector to assign Nodes (only one of them can be set)     names:     - gz-dr-node01     - gz-dr-node02     - gz-dr-node03     - gz-dr-node04     - gz-dr-node05     - gz-dr-node06     - gz-dr-node07     - gz-dr-node08     # matchLabels:     #   env: prod   completionPolicy:     type: Never                  # [optional] defaults to Always       #activeDeadlineSeconds: 1200   # [optional] no default, only work for Always type       #ttlSecondsAfterFinished: 300  # [optional] no default, only work for Always type   pullPolicy:                     # [optional] defaults to backoffLimit=3, timeoutSeconds=600     backoffLimit: 3     timeoutSeconds: 300   pullSecrets:   - idc-harbor ```  ## ImagePullJob: ``` Name:         job-with-always-drgcb Namespace:    default Labels:       controller-revision-hash=56f9cc997 Annotations:  <none> API Version:  apps.kruise.io/v1alpha1 Kind:         ImagePullJob Metadata:   Creation Timestamp:  2024-11-04T01:30:17Z   Finalizers:     apps.kruise.io/deletion-protection   Generate Name:  job-with-always-   Generation:     1   Owner References:     API Version:           apps.kruise.io/v1alpha1     Block Owner Deletion:  true     Controller:            true     Kind:                  ImageListPullJob     Name:                  job-with-always     UID:                   f7eca1db-1d82-4e3d-8067-d26a1b221b7d   Resource Version:        119767682   UID:                     9d07f677-8993-4c6e-bb64-f63bb1d90be1 Spec:   Completion Policy:     Type:             Never   Image:              harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:latest   Image Pull Policy:  Always   Parallelism:        2   Pull Policy:     Backoff Limit:    3     Timeout Seconds:  300   Pull Secrets:     idc-harbor   Selector:     Names:       gz-dr-node01       gz-dr-node02       gz-dr-node03       gz-dr-node04       gz-dr-node05       gz-dr-node06       gz-dr-node07       gz-dr-node08 Status:   Active:      0   Desired:     8   Failed:      0   Message:     job is running, progress 100.0%   Start Time:  2024-11-04T01:39:58Z   Succeeded:   8 Events:        <none> ```   ## nodeimage:gz-dr-node01  ``` harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:   pullSecrets:     - name: idc-harbor-p5lzl       namespace: kruise-daemon-config   tags:     - createdAt: '2024-11-04T01:39:58Z'       ownerReferences:         - apiVersion: apps.kruise.io/v1alpha1           kind: ImagePullJob           name: job-with-always-drgcb           namespace: default           uid: 9d07f677-8993-4c6e-bb64-f63bb1d90be1       pullPolicy:         activeDeadlineSeconds: 1800         backoffLimit: 3         timeoutSeconds: 300         ttlSecondsAfterFinished: 86017       tag: latest status: harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:   Tags:     Completion Time:  2024-11-04T01:40:01Z     Image ID:         harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:91849a5248a24bf524879ddb34192ddd291dc547c229abc6364d3d25c0db7788     Phase:            Succeeded     Progress:         100     Start Time:       2024-11-04T01:40:01Z     Tag:              latest ```  ## nodeimage:gz-dr-node04 ``` harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:   pullSecrets:     - name: idc-harbor-p5lzl       namespace: kruise-daemon-config   tags:     - createdAt: '2024-11-04T03:12:30Z'       ownerReferences:         - apiVersion: apps.kruise.io/v1alpha1           kind: ImagePullJob           name: job-with-always-drgcb           namespace: default           uid: 9d07f677-8993-4c6e-bb64-f63bb1d90be1       pullPolicy:         activeDeadlineSeconds: 1800         backoffLimit: 3         timeoutSeconds: 300         ttlSecondsAfterFinished: 86319       tag: latest       version: 1 status: harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe:   Tags:     Completion Time:  2024-11-04T03:12:35Z     Image ID:         harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:d5b08854cde36b2531131773a36b6920ee650b3575dbe91982451a39760dca5e     Phase:            Succeeded     Progress:         100     Start Time:       2024-11-04T03:12:35Z     Tag:              latest     Version:          1 ```   ## logs: ### gz-dr-node04 ``` \\""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe\\"":{\\""tags\\"":[{\\""tag\\"":\\""latest\\"",\\""phase\\"":\\""Succeeded\\"",\\""progress\\"":100,\\""startTime\\"":\\""2024-11-04T03:12:35Z\\"",\\""completionTime\\"":\\""2024-11-04T03:12:35Z\\"",\\""version\\"":1,\\""imageID\\"":\\""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:d5b08854cde36b2531131773a36b6920ee650b3575dbe91982451a39760dca5e\\""}]} ``` ### gz-dr-node01 ``` \\""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe\\"":{\\""tags\\"":[{\\""tag\\"":\\""latest\\"",\\""phase\\"":\\""Succeeded\\"",\\""progress\\"":100,\\""startTime\\"":\\""2024-11-04T01:40:01Z\\"",\\""completionTime\\"":\\""2024-11-04T01:40:01Z\\"",\\""imageID\\"":\\""harbor.jusda.int/juslink-ms/juslink-4pl-spm-fe@sha256:91849a5248a24bf524879ddb34192ddd291dc547c229abc6364d3d25c0db7788\\""}]} ```  ",2024-11-04T07:52:23+00:00,2025-01-02T03:03:31+00:00,6,https://github.com/openkruise/kruise/issues/1814,1830.0,2024-11-19T09:57:48+00:00,https://github.com/openkruise/kruise/pull/1830,0,1,0,1,1,0,0,1,362.09027777777777,kind/bug,False,True,normal,configuration,"[{""filename"": ""pkg/controller/imagepulljob/imagepulljob_controller.go"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
GoogleCloudPlatform/kubernetes-engine-samples,1307,gke-scheduled-autoscaler uses deprecated package,"The autoscaler sample uses this package ""google.golang.org/api/monitoring/v3"" in main.go, but that package indicates the following:  This package is DEPRECATED. Use package cloud.google.com/go/monitoring/apiv3 instead.  Will this sample be updated to use the new package?",2024-05-30T19:00:04+00:00,2024-06-07T19:13:02+00:00,0,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/issues/1307,1309.0,2024-06-07T19:13:01+00:00,https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/pull/1309,0,2,4,6,88,237,0,80,192.2158333333333,type: bug;priority: p2,False,True,normal,functional,"[{""filename"": ""cost-optimization/gke-scheduled-autoscaler/go.mod"", ""lines_added"": 9, ""lines_deleted"": 7, ""file_type"": ""other""}, {""filename"": ""cost-optimization/gke-scheduled-autoscaler/go.sum"", ""lines_added"": 14, ""lines_deleted"": 72, ""file_type"": ""other""}, {""filename"": ""cost-optimization/gke-scheduled-autoscaler/main.go"", ""lines_added"": 20, ""lines_deleted"": 17, ""file_type"": ""app_code""}, {""filename"": ""observability/custom-metrics-autoscaling/direct-to-sd/go.mod"", ""lines_added"": 9, ""lines_deleted"": 8, ""file_type"": ""other""}, {""filename"": ""observability/custom-metrics-autoscaling/direct-to-sd/go.sum"", ""lines_added"": 14, ""lines_deleted"": 112, ""file_type"": ""other""}, {""filename"": ""observability/custom-metrics-autoscaling/direct-to-sd/sd_dummy_exporter.go"", ""lines_added"": 22, ""lines_deleted"": 21, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1832,[BUG] A smaller MaxUnavailable will block the sidecarSet from updating Pods.,<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: ``` apiVersion: apps.kruise.io/v1alpha1 kind: SidecarSet metadata:   name: test-sidecarset spec:   containers:   - command:     - sleep     - 999d     image: mirrors-ssl.aliyuncs.com/centos:6.8     imagePullPolicy: IfNotPresent     name: sidecar1     podInjectPolicy: BeforeAppContainer     resources: {}     shareVolumePolicy:       type: disabled     terminationMessagePath: /dev/termination-log     terminationMessagePolicy: File     upgradeStrategy:       upgradeType: ColdUpgrade   injectionStrategy: {}   revisionHistoryLimit: 10   selector:     matchLabels:       app: nginx   updateStrategy:     maxUnavailable: 5%     partition: 0     type: RollingUpdate status:   collisionCount: 0   latestRevision: test-sidecarset-7658c89f8d   matchedPods: 5   observedGeneration: 3   readyPods: 5   updatedPods: 0 ``` **What you expected to happen**:  **How to reproduce it (as minimally and precisely as possible)**:  **Anything else we need to know?**:  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ,2024-11-19T13:07:58+00:00,2024-11-25T05:38:43+00:00,1,https://github.com/openkruise/kruise/issues/1832,1834.0,2024-11-21T05:12:55+00:00,https://github.com/openkruise/kruise/pull/1834,0,2,0,2,62,2,0,64,40.0825,kind/bug,False,True,normal,database,"[{""filename"": ""pkg/controller/sidecarset/sidecarset_strategy.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""pkg/controller/sidecarset/sidecarset_strategy_test.go"", ""lines_added"": 59, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1771,bugfix(cloneset-update): correct the wrong unavailableCount logic.,"In my opinion, the correct approach should be` !IsPodAvailable()`, rather than `IsPodAvailable`.",2024-10-02T02:53:49+00:00,2024-10-02T03:11:40+00:00,1,https://github.com/openkruise/kruise/pull/1771,1771.0,,https://github.com/openkruise/kruise/pull/1771,0,1,0,1,1,1,0,2,0.2975,,False,True,normal,functional,"[{""filename"": ""pkg/controller/cloneset/sync/cloneset_update.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
envoyproxy/gateway,5470,Make file provider errors useful,"**What type of PR is this?**  Enhances error message with file provider.  **What this PR does / why we need it**:  Previously, the file provider doesn't log any information about which file is wrong, so this fixes it and adds the missing test coverage.",2025-03-11T22:05:17+00:00,2025-03-12T12:23:33+00:00,1,https://github.com/envoyproxy/gateway/pull/5470,5470.0,2025-03-12T12:23:33+00:00,https://github.com/envoyproxy/gateway/pull/5470,0,2,0,2,91,4,0,95,14.304444444444444,,False,True,normal,functional,"[{""filename"": ""internal/provider/file/resources.go"", ""lines_added"": 4, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""internal/provider/file/resources_test.go"", ""lines_added"": 87, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
envoyproxy/gateway,5467,fix: do not log error when HTTP server is properly closed,"**What type of PR is this?**  fix: incorrect handling of err returned by ListenAndServe.  **What this PR does / why we need it**:  Previously, the error returned by `ListenAndServe` was unconditionally treated as an error hence a log saying like ""failed to start server etc"" was emitted when the envoy-gateway is properly shutdown, which is misleading as it is not an error in practice. `ErrServerClosed` is always returned by ListenAndServe and its variant so this makes the change to ignore that error.  > 2025-03-11T09:22:53.397-0700    ERROR   provider        file/file.go:205        failed to start health probe server     {""runner"": ""provider"", ""error"": ""http: Server closed""} > 2025-03-11T09:22:53.397-0700    ERROR   wasm-cache      wasm/httpserver.go:130  Failed to start Wasm HTTP server        {""error"": ""http: Server closed""}   Release Notes:No ",2025-03-11T16:30:56+00:00,2025-03-11T20:57:41+00:00,1,https://github.com/envoyproxy/gateway/pull/5467,5467.0,2025-03-11T20:57:41+00:00,https://github.com/envoyproxy/gateway/pull/5467,0,2,0,2,4,2,0,6,4.445833333333334,,False,True,normal,functional,"[{""filename"": ""internal/provider/file/file.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/wasm/httpserver.go"", ""lines_added"": 2, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",wasm,False
openkruise/kruise,1688,fix kind install bug,"<!--  Please make sure you have read and understood the contributing guidelines; https://github.com/openkruise/kruise/blob/master/CONTRIBUTING.md -->  ### Ⅰ. Describe what this PR does auto install kind when `make create-cluster`  ### Ⅱ. Does this pull request fix one issue? <!--If so, add ""fixes #xxxx"" below in the next line, for example, fixes #15. Otherwise, add ""NONE"" --> fixes #1686 ### Ⅲ. Describe how to verify it   ### Ⅳ. Special notes for reviews  ",2024-08-02T06:48:36+00:00,2024-09-05T02:31:00+00:00,3,https://github.com/openkruise/kruise/pull/1688,1688.0,2024-09-05T02:31:00+00:00,https://github.com/openkruise/kruise/pull/1688,0,1,1,2,3,1,0,2,811.7066666666667,lgtm;approved;size/XS,False,True,normal,ui,"[{""filename"": ""tools/src/kind/pin.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""tools/tools.mk"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""other""}]",,False
ray-project/kuberay,3140,[Bug] kubectl plugin e2e test is flaky,### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ci  ### What happened + What you expected to happen  https://buildkite.com/ray-project/ray-ecosystem-ci-kuberay-ci/builds/7280#01955c7f-fd4e-4752-a4c7-c1e0862f7a29  ### Reproduction script  https://buildkite.com/ray-project/ray-ecosystem-ci-kuberay-ci/builds/7280#01955c7f-fd4e-4752-a4c7-c1e0862f7a29  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [ ] Yes I am willing to submit a PR!,2025-03-03T17:31:23+00:00,2025-03-03T19:31:25+00:00,4,https://github.com/ray-project/kuberay/issues/3140,3147.0,2025-03-03T19:31:24+00:00,https://github.com/ray-project/kuberay/pull/3147,0,1,0,1,3,2,0,5,2.000277777777778,bug;cli;P0,False,True,critical,ui,"[{""filename"": ""kubectl-plugin/test/e2e/kubectl_ray_session_test.go"", ""lines_added"": 3, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,3135,[ci] remove the test for modin because its certificate is broken,"## Why are these changes needed?  Closes https://github.com/ray-project/kuberay/issues/3134.  The certificate of `modin-datasets.intel.com` is broken. Therefore, remove the test until https://github.com/modin-project/modin/issues/7451 is resolved.  Screenshot: <img width=""1309"" alt=""image"" src=""https://github.com/user-attachments/assets/76b0379f-5973-496d-9955-9300d41705c1"" /> ![image](https://github.com/user-attachments/assets/f4497ab2-b520-4aa5-a3cf-ab77f0722081) ![image](https://github.com/user-attachments/assets/9c3b08f9-4830-4470-add4-d754549697f1)    <!-- Please give a short summary of the change and the problem this solves. -->  ## Related issue number  Closes https://github.com/ray-project/kuberay/issues/3134.   ## Checks  - [ ] I've made sure the tests are passing. - Testing Strategy   - [ ] Unit tests   - [ ] Manual tests   - [ ] This PR is not tested :( ",2025-03-01T06:46:55+00:00,2025-03-01T09:15:06+00:00,1,https://github.com/ray-project/kuberay/pull/3135,3135.0,2025-03-01T09:15:06+00:00,https://github.com/ray-project/kuberay/pull/3135,0,1,0,1,0,3,0,3,2.4697222222222224,,False,True,critical,database,"[{""filename"": ""ray-operator/test/sampleyaml/rayjob_test.go"", ""lines_added"": 0, ""lines_deleted"": 3, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,3124,[Bug] Re-enable flaky kubectl plugin e2e test in kubectl_ray_job_submit_test.go,"<!-- Thank you for your contribution! -->  <!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->  ## Why are these changes needed?  There are 2 reason make the test fail. - There are some kubectl process occupied the 8265 port that is required by the kubectl job submit. But for the independence of tests, we should not expect the test cause this problem to kill the process properly every time, so added the pkill kubectl before running kubectl job submit test. - The ray is not installed on build kite env, so modified the setup-env.sh to install.   <!-- Please give a short summary of the change and the problem this solves. -->  ## Related issue number Closes #2801  <!-- For example: ""Closes #1234"" -->  ## Checks  - [ ] I've made sure the tests are passing. - Testing Strategy   - [ ] Unit tests   - [ ] Manual tests   - [ ] This PR is not tested :( ",2025-02-27T09:38:33+00:00,2025-02-28T22:02:50+00:00,3,https://github.com/ray-project/kuberay/pull/3124,3124.0,2025-02-28T22:02:50+00:00,https://github.com/ray-project/kuberay/pull/3124,0,1,1,2,11,2,0,6,36.40472222222222,,False,True,normal,security,"[{""filename"": "".buildkite/setup-env.sh"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""kubectl-plugin/test/e2e/kubectl_ray_job_submit_test.go"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1636,[BUG] SidecarSet meaningless status update when restart,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**:  Tons of status update happened when you restart kruise-manager even though no changes about Pod and SidecarSet resource.  **What you expected to happen**:  No meaningless status update when kruise-manager restart.  **How to reproduce it (as minimally and precisely as possible)**:  Restart kruise-manager, watch rest-client QPS metric, you will find PUT QPS will increase for a while.  **Anything else we need to know?**:  The reason why meaningless update happened is we compared Pointer field in status inconsistence check, here is related code https://github.com/openkruise/kruise/blob/0e69ed4bec56d7956457802225a45b962e7869be/pkg/controller/sidecarset/sidecarset_processor.go#L629  https://github.com/openkruise/kruise/blob/0e69ed4bec56d7956457802225a45b962e7869be/apis/apps/v1alpha1/sidecarset_types.go#L305  **Environment**: - Kruise version: - Kubernetes version (use `kubectl version`): - Install details (e.g. helm install args): - Others: ",2024-06-03T07:36:29+00:00,2024-06-12T03:23:44+00:00,0,https://github.com/openkruise/kruise/issues/1636,1719.0,2024-08-30T09:28:55+00:00,https://github.com/openkruise/kruise/pull/1719,0,2,0,2,19,9,0,28,2113.873888888889,kind/bug;kind/good-first-issue,False,True,normal,ui,"[{""filename"": ""pkg/control/sidecarcontrol/util.go"", ""lines_added"": 15, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/webhook/pod/mutating/sidecarset.go"", ""lines_added"": 4, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,3126,[Bug] InteractiveMode RayJob rejected for missing entrypoint,"### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  When submitting an InteractiveMode ray job, kube-apiserver rejects the manifest because it lacks a spec.entrypoint. I expected an interactive mode ray job to be created fine without an entry point.  Reproduced with kuberay 1.2.1.  ### Reproduction script  ``` $ kubectl apply -f ray-operator/config/samples/ray-job.interactive-mode.yaml The RayJob ""rayjob-interactive-mode"" is invalid: spec.entrypoint: Required value ```  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [x] Yes I am willing to submit a PR!",2025-02-27T19:28:57+00:00,2025-02-28T22:21:43+00:00,2,https://github.com/ray-project/kuberay/issues/3126,3186.0,2025-03-11T21:41:48+00:00,https://github.com/ray-project/kuberay/pull/3186,0,2,0,2,7,0,0,7,290.21416666666664,bug;triage,False,True,normal,configuration,"[{""filename"": ""kubectl-plugin/pkg/cmd/job/job_submit.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""kubectl-plugin/pkg/util/generation/generation.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,2801,[Bug] Re-enable flaky kubectl plugin e2e test in `kubectl_ray_job_submit_test.go`,### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  https://github.com/ray-project/kuberay/pull/2800 temporarily disabled this flaky test. Try to fix it and re-enable it.  https://github.com/ray-project/kuberay/blob/4bb1226ce442f6a03241de91e496adb3f5adef37/kubectl-plugin/test/e2e/kubectl_ray_job_submit_test.go#L34  https://github.com/ray-project/kuberay/blob/4bb1226ce442f6a03241de91e496adb3f5adef37/kubectl-plugin/test/e2e/kubectl_ray_job_submit_test.go#L70  Possibly related to https://github.com/ray-project/kuberay/issues/2758  ### Reproduction script  N/A  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [x] Yes I am willing to submit a PR!,2025-01-22T07:05:49+00:00,2025-02-28T22:02:51+00:00,1,https://github.com/ray-project/kuberay/issues/2801,3124.0,2025-02-28T22:02:50+00:00,https://github.com/ray-project/kuberay/pull/3124,0,1,1,2,11,2,0,6,902.9502777777776,bug;cli;P0,False,True,critical,functional,"[{""filename"": "".buildkite/setup-env.sh"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""kubectl-plugin/test/e2e/kubectl_ray_job_submit_test.go"", ""lines_added"": 4, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,3116,"[Bug] Re-enable flaky kubectl plugin e2e test ""should reconnect after pod connection is lost""","<!-- Thank you for your contribution! -->  <!-- Please add a reviewer to the assignee section when you create a PR. If you don't have the access to it, we will shortly find a reviewer and assign them to your PR. -->  ## Why are these changes needed?  <!-- Please give a short summary of the change and the problem this solves. -->  ## Related issue number Closes #2752 <!-- For example: ""Closes #1234"" -->  ## Checks  - [ ] I've made sure the tests are passing. - Testing Strategy   - [ ] Unit tests   - [ ] Manual tests   - [ ] This PR is not tested :( ",2025-02-26T18:49:05+00:00,2025-02-27T01:20:37+00:00,1,https://github.com/ray-project/kuberay/pull/3116,3116.0,2025-02-27T01:20:37+00:00,https://github.com/ray-project/kuberay/pull/3116,0,1,0,1,1,2,0,3,6.525555555555556,,False,True,normal,networking,"[{""filename"": ""kubectl-plugin/test/e2e/kubectl_ray_session_test.go"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1583,[BUG] tag v1.6.2 unit-test TestMatchRegistryAuths case4(test4) failed,"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!-->  **What happened**: I use kruise tag v1.6.2 to  run unit-test with go version 1.19 & 1.20 **What you expected to happen**: All unit-test should pass but case TestMatchRegistryAuths(https://github.com/openkruise/kruise/blob/v1.6.2/pkg/daemon/criruntime/imageruntime/helpers_test.go#L89) failed, **How to reproduce it (as minimally and precisely as possible)**: Run test case TestMatchRegistryAuths **Anything else we need to know?**:  **Environment**: - Kruise version: tag v1.6.2 - Kubernetes version (use `kubectl version`): unit-test - Install details (e.g. helm install args): - Others: ",2024-04-17T03:44:48+00:00,2024-05-27T07:55:13+00:00,9,https://github.com/openkruise/kruise/issues/1583,1640.0,2024-06-05T04:10:36+00:00,https://github.com/openkruise/kruise/pull/1640,0,1,0,1,8,7,0,15,1176.43,kind/bug,False,True,normal,security,"[{""filename"": ""pkg/daemon/criruntime/imageruntime/helpers_test.go"", ""lines_added"": 8, ""lines_deleted"": 7, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,2752,"[Bug] Re-enable flaky kubectl plugin e2e test ""should reconnect after pod connection is lost""",### Search before asking  - [x] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  https://github.com/ray-project/kuberay/pull/2745 temporarily disabled this flaky test. Try to fix it and re-enable it.  https://github.com/ray-project/kuberay/blob/f687794d0c2711c9318c2a2566ca951d1a8e4e3f/kubectl-plugin/test/e2e/kubectl_ray_session_test.go#L67  Possibly related to https://github.com/ray-project/kuberay/issues/2758  ### Reproduction script  N/A  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [x] Yes I am willing to submit a PR!,2025-01-15T03:51:14+00:00,2025-02-27T01:20:38+00:00,1,https://github.com/ray-project/kuberay/issues/2752,3116.0,2025-02-27T01:20:37+00:00,https://github.com/ray-project/kuberay/pull/3116,0,1,0,1,1,2,0,3,1029.4897222222223,bug;cli,False,True,normal,networking,"[{""filename"": ""kubectl-plugin/test/e2e/kubectl_ray_session_test.go"", ""lines_added"": 1, ""lines_deleted"": 2, ""file_type"": ""app_code""}]",,False
openkruise/kruise,1628,fix: fix miss makezero bug,"<!--  Please make sure you have read and understood the contributing guidelines; https://github.com/openkruise/kruise/blob/master/CONTRIBUTING.md -->  ### Ⅰ. Describe what this PR does  I was running github actions to run linter [makezero](https://github.com/ashanbrown/makezero) for top github golang repos.  see issues https://github.com/alingse/go-linter-runner/issues/1  and the github actions output https://github.com/alingse/go-linter-runner/actions/runs/9242658768/job/25425757534   ``` ==================================================================================================== append to slice `names` with non-zero initialized length at https://github.com/openkruise/kruise/blob/master/pkg/util/imagejob/imagejob_reader.go#L60:11 ==================================================================================================== ```    ### Ⅱ. Does this pull request fix one issue? <!--If so, add ""fixes #xxxx"" below in the next line, for example, fixes #15. Otherwise, add ""NONE"" -->  ### Ⅲ. Describe how to verify it  the names no need empty string value   ### Ⅳ. Special notes for reviews  ",2024-05-26T15:39:46+00:00,2024-05-27T02:29:28+00:00,4,https://github.com/openkruise/kruise/pull/1628,1628.0,2024-05-27T02:29:28+00:00,https://github.com/openkruise/kruise/pull/1628,0,1,0,1,1,1,0,2,10.828333333333331,lgtm;approved;size/XS,False,True,normal,ui,"[{""filename"": ""pkg/util/imagejob/imagejob_reader.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
envoyproxy/gateway,5200,How to inject credentials into a proxied request,"*Description*: >I am trying to inject the credentials into the proxied request through EnvoyPatchPolicy resource Followed the reference for adding typed_config for credentials  https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/credential_injector/v3/credential_injector.proto.html  My template is  ``` apiVersion: gateway.envoyproxy.io/v1alpha1 kind: EnvoyPatchPolicy metadata:   name: credential-injection-patch   namespace: envoy-gateway spec:   targetRef:     group: gateway.networking.k8s.io     kind: Gateway     name: eg   type: JSONPatch   jsonPatches:     - type: ""type.googleapis.com/envoy.config.listener.v3.Listener""       name: envoy-gateway/eg/http       operation:         op: add         path: ""/default_filter_chain/filters/0/typed_config/http_filters/-""         value:           name: envoy.filters.http.credential_injector           typed_config:             ""@type"": type.googleapis.com/envoy.extensions.filters.http.credential_injector.v3.CredentialInjector             allow_request_without_credential: false             overwrite: true             credential:               name: envoy.http.injected_credentials.generic               typed_config:                 ""@type"": type.googleapis.com/envoy.extensions.http.injected_credentials.generic.v3.Generic                 header: Authorization                 credential:                   name: credential   ```  Expected Behavior: When allow_request_without_credential: false, the request should be blocked with a 401 Unauthorized response. Envoy Gateway should apply the patch properly.  Actual Behavior: The patch is applied successfully (kubectl get envoypatchpolicy shows it as Accepted). But the request always succeeds with 200 OK, even when no credentials are passed.  Additionally, I want to inject a static credential using a Secret: ``` resources: - ""@type"": ""type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.Secret""   name: credential   generic_secret:     secret:       inline_string: ""Bearer myToken"" ```  Could you provide an example on how to properly add this Secret and inject credentials in EnvoyPatchPolicy?  BTW, i have tested using envoyproxy to inject credentials. It works as expected  [optional *Relevant Links*:] >Any extra documentation required to understand the issue. ",2025-02-04T14:02:11+00:00,2025-02-14T22:07:06+00:00,6,https://github.com/envoyproxy/gateway/issues/5200,5268.0,2025-02-14T22:07:05+00:00,https://github.com/envoyproxy/gateway/pull/5268,0,1,2,3,4,0,0,2,248.08166666666668,kind/bug;area/api,False,True,normal,configuration,"[{""filename"": ""api/v1alpha1/envoypatchpolicy_types.go"", ""lines_added"": 2, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""site/content/en/latest/api/extension_types.md"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}, {""filename"": ""site/content/zh/latest/api/extension_types.md"", ""lines_added"": 1, ""lines_deleted"": 0, ""file_type"": ""other""}]",latest;api,True
radius-project/radius,7670,"`interface conversion: tea.Model is nil, not *radinit.summaryModel` error in unit test","### Steps to reproduce  See this action run: https://github.com/radius-project/radius/actions/runs/9409236142/job/25918756343?pr=7669.  Unit test **Test_summaryModel/Result:_Cancel** failed with the following error:  ``` panic: interface conversion: tea.Model is nil, not *radinit.summaryModel [recovered] 	panic: interface conversion: tea.Model is nil, not *radinit.summaryModel ```  It is not all the time but I have seen it happen a few times.  ### Observed behavior  Flaky unit test.  ### Desired behavior  Unit test should pass all the time.  ### Workaround  Rerunning works.  ### rad Version  edge  ### Operating system  _No response_  ### Additional context  ![image](https://github.com/radius-project/radius/assets/5220939/5bf37fe2-9fc0-470e-8cc3-e2b1032ae079)  ### Would you like to support us?  - [ ] Yes, I would like to support you  [AB#12482](https://dev.azure.com/azure-octo/e61041b4-555f-47ae-95b2-4f8ab480ea57/_workitems/edit/12482)",2024-06-07T07:40:55+00:00,2025-02-12T18:39:37+00:00,7,https://github.com/radius-project/radius/issues/7670,7985.0,2024-10-10T17:30:26+00:00,https://github.com/radius-project/radius/pull/7985,0,1,0,1,7,6,0,13,3009.825277777778,triaged;bug;skipped-test,False,True,normal,ui,"[{""filename"": ""pkg/cli/prompt/text/text_test.go"", ""lines_added"": 7, ""lines_deleted"": 6, ""file_type"": ""app_code""}]",,False
envoyproxy/gateway,4981,Unexpected port number increase in Standalone mode,"*Description*: I'm trying to run `envoy-gateway` in **standalone mode** inside docker container with host network.  I've created `Gateway` resource with port `80`:  ```yaml spec:   listeners:   - name: http-80     protocol: HTTP     port: 80 ```  But `envoy-gateway` logs say it runs `envoy` with port `10080`:  ```json {   ""http"":[     {       ""name"":""envoy-gateway-system/compose/http-80"",       ""address"":""0.0.0.0"",       ""port"":10080     }   ] } ```  I found this in [internal/gatewayapi/helpers.go](https://github.com/envoyproxy/gateway/blob/24a50b49fae835d440b1c013140f07dc4180be5f/internal/gatewayapi/helpers.go#L261):  ```go // If the service port is a privileged port (1-1023) // add a constant to the value converting it into an ephemeral port. // This allows the container to bind to the port without needing a // CAP_NET_BIND_SERVICE capability. if servicePort < minEphemeralPort { 	return servicePort + wellKnownPortShift } return servicePort ```  Both `minEphemeralPort` and `wellKnownPortShift` are `const` so I can't change this behavior via settings.  The idea to run less privileged container is nice and I see how in `kubernetes` cluster it will be working as expected in almost all situations but in my case (host network) it means I unable to use `envoy-gateway` with the ports I need and I was not expecting ports will be changed to something else.  Should it be available as some sort of a setting?",2024-12-30T17:36:39+00:00,2025-01-10T06:15:01+00:00,5,https://github.com/envoyproxy/gateway/issues/4981,5027.0,2025-01-10T06:15:00+00:00,https://github.com/envoyproxy/gateway/pull/5027,0,5,0,5,50,33,0,83,252.63916666666665,kind/bug,False,True,normal,configuration,"[{""filename"": ""internal/gatewayapi/helpers.go"", ""lines_added"": 0, ""lines_deleted"": 19, ""file_type"": ""app_code""}, {""filename"": ""internal/gatewayapi/listener.go"", ""lines_added"": 25, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""internal/gatewayapi/runner/runner.go"", ""lines_added"": 9, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""internal/gatewayapi/translator.go"", ""lines_added"": 5, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""internal/gatewayapi/translator_test.go"", ""lines_added"": 11, ""lines_deleted"": 5, ""file_type"": ""app_code""}]",internal,False
ray-project/kuberay,2694,[GCS FT] GCS FT misconfiguration,"### Search before asking  - [X] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  See [this Slack thread](https://ray.slack.com/archives/C02GFQ82JPM/p1733940403153719) for more details.  The user creates a RayService without setting the annotation `ray.io/ft-enabled: ""true""`, but they do set `RAY_REDIS_ADDRESS` and `REDIS_PASSWORD`. As a result, the RayCluster enables GCS FT and writes data to the external Redis, but KubeRay is unaware of this. KubeRay doesn't configure `RAY_external_storage_namespace` so that the RayCluster writes data to the key `default` in the Redis.  When the user triggers a zero-downtime upgrade, the new RayCluster also attempts to read metadata from the `default` key in Redis. Therefore, the new RayCluster will see some information from the old RayCluster.  Solution: * Explicitly disable GCS FT if KubeRay determines that the RayCluster has not enabled GCS FT.  ### Reproduction script  TODO  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [ ] Yes I am willing to submit a PR!",2024-12-27T22:56:08+00:00,2025-01-13T18:00:30+00:00,4,https://github.com/ray-project/kuberay/issues/2694,2738.0,2025-01-13T18:00:28+00:00,https://github.com/ray-project/kuberay/pull/2738,0,2,0,2,81,2,0,83,403.0722222222222,bug;gcs-ft;1.3.0,False,True,normal,configuration,"[{""filename"": ""ray-operator/controllers/ray/raycluster_controller.go"", ""lines_added"": 2, ""lines_deleted"": 2, ""file_type"": ""app_code""}, {""filename"": ""ray-operator/controllers/ray/raycluster_controller_unit_test.go"", ""lines_added"": 79, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
envoyproxy/gateway,4677,Trivy Scans are sometimes failing,"*Description*: >Describe the issue.  Seen in https://github.com/envoyproxy/gateway/actions/runs/11734591724/job/32690874144  ``` 2024-11-08T01:51:50Z	ERROR	[vulndb] Failed to download artifact	repo=""ghcr.io/aquasecurity/trivy-db:2"" err=""oci download error: failed to fetch the layer: GET https://ghcr.io/v2/aquasecurity/trivy-db/blobs/sha256:f6b05109846a0037211bb931e7cf50edf5a554005a390cc19e0cabf163ee9cf8: TOOMANYREQUESTS: retry-after: 414.967µs, allowed: 44000/minute"" 2024-11-08T01:51:50Z	FATAL	Fatal error	init error: DB error: failed to download vulnerability DB: OCI artifact error: failed to download vulnerability DB: failed to download artifact from any source Error: Process completed with exit code 1. ```  Relates to https://github.com/aquasecurity/trivy/discussions/7668  [optional *Relevant Links*:] >Any extra documentation required to understand the issue. ",2024-11-08T03:20:47+00:00,2025-01-06T17:50:59+00:00,2,https://github.com/envoyproxy/gateway/issues/4677,2134.0,2023-10-31T10:23:53+00:00,https://github.com/envoyproxy/gateway/pull/2134,0,1,0,1,7,0,0,7,-8968.948333333334,help wanted;area/ci,False,True,normal,security,"[{""filename"": ""internal/status/status.go"", ""lines_added"": 7, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
ray-project/kuberay,2210,[Bug] no feedback about failure to create submitter pod due to invalid spec,"### Search before asking  - [X] I searched the [issues](https://github.com/ray-project/kuberay/issues) and found no similar issues.   ### KubeRay Component  ray-operator  ### What happened + What you expected to happen  created a RayJob with a `submitterPodTemplate` but no `restartPolicy`  had to search the logs of the ray-operator to find:  ``` {""level"":""error"",""ts"":""2024-06-28T18:09:14.679Z"",""logger"":""controllers.RayJob"",""msg"":""failed to create k8s Job"",""RayJob"":{""name"":""mick-gxccf"",""namespace"":""launch""},""reconcileID"":""3b03831c-d14d-497f-9c8c-4ac790e1ff35"",""error"":""Job.batch \\""mick-gxccf\\"" is invalid: spec.template.spec.restartPolicy: Required value: valid values: \\""OnFailure\\"", \\""Never\\"""",""stacktrace"":""github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayJobReconciler).createNewK8sJob\\n\\t/home/runner/work/kuberay/kuberay/ray-operator/controllers/ray/rayjob_controller.go:440\\ngithub.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayJobReconciler).createK8sJobIfNeed\\n\\t/home/runner/work/kuberay/kuberay/ray-operator/controllers/ray/rayjob_controller.go:350\\ngithub.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayJobReconciler).Reconcile\\n\\t/home/runner/work/kuberay/kuberay/ray-operator/controllers/ray/rayjob_controller.go:168\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2\\n\\t/home/runner/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227""} ```  I thought the RayJob spec is supposed to be validated on submission to the API? Is the validation not the same?   ### Reproduction script  ``` ""submitterPodTemplate"": {     ""spec"": {         // ""restartPolicy"": ""Never"", <- OFFENDER         // ... as usual     } } ```  ### Anything else  _No response_  ### Are you willing to submit a PR?  - [ ] Yes I am willing to submit a PR!",2024-06-28T18:23:34+00:00,2025-01-09T01:17:20+00:00,12,https://github.com/ray-project/kuberay/issues/2210,2306.0,2024-09-18T12:12:30+00:00,https://github.com/ray-project/kuberay/pull/2306,0,2,0,2,62,1,0,63,1961.815555555556,bug;rayjob;crd-observability;1.3.0,False,True,normal,ui,"[{""filename"": ""ray-operator/controllers/ray/rayjob_controller.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""ray-operator/controllers/ray/rayjob_controller_unit_test.go"", ""lines_added"": 59, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",,False
knative/eventing,8511,SQS IntegrationSink queueNameOrArn missing,"**Describe the bug** The SQS IntegrationSink can not be successfully created. Once running, I am presented this error within the associated pod: ` ... Caused by: java.lang.IllegalArgumentException: Route template aws-sqs-sink the following mandatory parameters must be provided: queueNameOrArn 	at org.apache.camel.impl.DefaultModel.addRouteFromTemplate(DefaultModel.java:478) 	at org.apache.camel.impl.DefaultModel.addRouteFromTemplate(DefaultModel.java:416) 	at org.apache.camel.impl.DefaultCamelContext.addRouteFromTemplate(DefaultCamelContext.java:362) 	at org.apache.camel.component.kamelet.KameletComponent$LifecycleHandler.createRouteForEndpoint(KameletComponent.java:457) 	... 28 more`       **Expected behavior** The IntegrationSink is created and runs correctly with no errors.   **To Reproduce**  ```apiVersion: sinks.knative.dev/v1alpha1 kind: IntegrationSink metadata:   name: sqsIntegrationSinkTest   namespace: test spec:   aws:     auth:       secret:         ref:           name: sqsIntegrationSinkTest     sqs:       arn: ""arn:aws:sqs:us-east-2:************:sqsIntegrationSinkTest       region: us-east-2 ```   **Knative release version** 1.17.0   **Additional context** This is the first time attempting to use this custom resource type. I noticed on the describe call against the associated pod this set of registered environment variables:  ```    Environment:       CAMEL_KAMELET_AWS_SQS_SINK_REGION:              us-east-2       CAMEL_KAMELET_AWS_SQS_SINK_OVERRIDEENDPOINT:    false       CAMEL_KAMELET_AWS_SQS_SOURCE_QUEUENAMEORARN:    arn:aws:sqs:us-east-2:************:sqsIntegrationSinkTest       CAMEL_KAMELET_AWS_SQS_SINK_DELETEAFTERREAD:     true       CAMEL_KAMELET_AWS_SQS_SINK_AUTOCREATEQUEUE:     false       CAMEL_KAMELET_AWS_SQS_SOURCE_AMAZONAWSHOST:     amazonaws.com       CAMEL_KAMELET_AWS_SQS_SINK_PROTOCOL:            https       CAMEL_KAMELET_AWS_SQS_SINK_GREEDY:              false       CAMEL_KAMELET_AWS_SQS_SINK_DELAY:               500       CAMEL_KAMELET_AWS_SQS_SINK_MAXMESSAGESPERPOLL:  1       CAMEL_KAMELET_AWS_SQS_SINK_WAITTIMESECONDS:     0       CAMEL_KAMELET_AWS_SQS_SINK_VISIBILITYTIMEOUT:   0       CAMEL_KAMELET_AWS_SQS_SINK_ACCESSKEY:           <set to the key 'aws.accessKey' in secret '*******'>  Optional: false       CAMEL_KAMELET_AWS_SQS_SINK_SECRETKEY:           <set to the key 'aws.secretKey' in secret '*******'>  Optional: false ```   I believe that the environment variable `CAMEL_KAMELET_AWS_SQS_SOURCE_QUEUENAMEORARN""`should be `CAMEL_KAMELET_AWS_SQS_SINK_QUEUENAMEORARN` in order for this code  ```	if sink.Spec.Log != nil { 		envVars = append(envVars, integration.GenerateEnvVarsFromStruct(""CAMEL_KAMELET_LOG_SINK"", *sink.Spec.Log)...) 		return envVars 	} ```  [here](https://github.com/knative/eventing/blob/3153b4735d1c5dbfc9e3c0bfa98e655219fe0708/pkg/reconciler/integration/sink/resources/container_image.go#L166) to properly read the environment variables.",2025-02-26T16:59:13+00:00,2025-03-18T09:17:15+00:00,5,https://github.com/knative/eventing/issues/8511,8528.0,2025-03-18T09:17:14+00:00,https://github.com/knative/eventing/pull/8528,0,3,0,3,93,39,0,132,472.30027777777775,kind/bug;triage/accepted;area/sinks;area/integrations,False,True,normal,networking,"[{""filename"": ""pkg/apis/common/integration/v1alpha1/aws.go"", ""lines_added"": 25, ""lines_deleted"": 25, ""file_type"": ""app_code""}, {""filename"": ""pkg/reconciler/integration/helper.go"", ""lines_added"": 3, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/reconciler/integration/helper_test.go"", ""lines_added"": 65, ""lines_deleted"": 13, ""file_type"": ""app_code""}]",pkg,False
knative/eventing,8401,Don't swallow errors in Lineage package,"<!--  Are you using Knative? If you do, we would love to know! https://github.com/knative/community/issues/new?template=ADOPTERS.yaml&title=%5BADOPTERS%5D%3A+%24%7BCOMPANY+NAME+HERE%7D -->  Fixes #  <!-- Please include the 'why' behind your changes if no issue exists -->  ## Proposed Changes  <!-- Please categorize your changes: - :gift: Add new feature - :bug: Fix bug - :broom: Update or clean up current behavior - :wastebasket: Remove feature or internal logic -->  - Don't swallow errors in Lineage package - I was having hard time understanding why the Backstage backend was returning 200 with corrupt auth - Introduced a field called `Lenient:bool` (wanted to have the default to `false`, thus, a positive field name versus `FailFast`). If this field is set to false, there will be no behavior changes, except one place (noted below).  ### Pre-review Checklist  <!-- If these boxes are not checked, you will be asked to complete these requirements or explain why they do not apply to your PR. -->  - [ ] **At least 80% unit test coverage** - [ ] **E2E tests** for any new behavior - [ ] **Docs PR** for any user-facing impact - [ ] **Spec PR** for any new API feature - [ ] **Conformance test** for any change to the spec  **Release Note**  <!-- :page_facing_up: If this change has user-visible impact, write a release note in the block below. Include the string ""action required"" if additional action is required of users switching to the new release, for example in case of a breaking change.  Write as if you are speaking to users, not other Knative contributors. If this change has no user-visible impact, no release note is needed. -->  ```release-note :page_facing_up: The event lineage package does not swallow the 400 and 401 errors anymore. Callers should handle those errors themselves. ```   **Docs**  <!-- :book: If this change has user-visible impact, link to an issue or PR in https://github.com/knative/docs. -->  ",2025-01-08T09:23:04+00:00,2025-01-08T10:57:58+00:00,3,https://github.com/knative/eventing/pull/8401,8401.0,2025-01-08T10:57:57+00:00,https://github.com/knative/eventing/pull/8401,0,1,0,1,117,56,0,173,1.581388888888889,approved;size/L;lgtm,False,True,normal,configuration,"[{""filename"": ""pkg/graph/constructor.go"", ""lines_added"": 117, ""lines_deleted"": 56, ""file_type"": ""app_code""}]",,False
knative/eventing,8335,fix: rename `job-sink` to `job_sink`,"<!--  Are you using Knative? If you do, we would love to know! https://github.com/knative/community/issues/new?template=ADOPTERS.yaml&title=%5BADOPTERS%5D%3A+%24%7BCOMPANY+NAME+HERE%7D -->  Fixes #8296   <!-- Please include the 'why' behind your changes if no issue exists -->   ## Proposed Changes  Rename the JobSink component from 'job-sink' to `job_sink` to make the generated metrics meet the  Prometheus naming requirement.  ref: https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels  <!-- Please categorize your changes: - :gift: Add new feature - :bug:  - :broom: Update or clean up current behavior - :wastebasket: Remove feature or internal logic -->  - :bug: Fix bug  ### Pre-review Checklist  <!-- If these boxes are not checked, you will be asked to complete these requirements or explain why they do not apply to your PR. -->  - [ ] **At least 80% unit test coverage** - [ ] **E2E tests** for any new behavior - [ ] **Docs PR** for any user-facing impact - [ ] **Spec PR** for any new API feature - [ ] **Conformance test** for any change to the spec  **Release Note**  <!-- :page_facing_up: If this change has user-visible impact, write a release note in the block below. Include the string ""action required"" if additional action is required of users switching to the new release, for example in case of a breaking change.  Write as if you are speaking to users, not other Knative contributors. If this change has no user-visible impact, no release note is needed. -->  ```release-note JobSink: all related metrics will start with prefix `job_sink` instead of `job-sink` ```   **Docs**  <!-- :book: If this change has user-visible impact, link to an issue or PR in https://github.com/knative/docs. -->  ",2024-11-20T08:54:46+00:00,2024-11-20T17:23:01+00:00,10,https://github.com/knative/eventing/pull/8335,8335.0,2024-11-20T17:23:01+00:00,https://github.com/knative/eventing/pull/8335,0,1,0,1,1,1,0,2,8.470833333333333,kind/bug;approved;size/XS;lgtm;ok-to-test,False,True,normal,configuration,"[{""filename"": ""cmd/jobsink/main.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",,False
carvel-dev/kapp,1019,Rephrasing the error message in case of continuous failure,"Rephrasing the error message in case of continuous failure <!--  Thanks for sending a pull request!  Here are some tips for you:  If this is your first time, please read our contributor guidelines: https://github.com/carvel-dev/kapp/blob/develop/CONTRIBUTING.md and developer guide https://github.com/carvel-dev/kapp/blob/develop/docs/dev.md -->  #### What this PR does / why we need it:  #### Which issue(s) this PR fixes: <!-- If no issue exists for this change, please create an issue and link it here. --> Fixes #  #### Does this PR introduce a user-facing change? <!-- If no, just write ""NONE"" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below.   --> ```release-note  ```  #### Additional Notes for your reviewer:  ##### Review Checklist:  - [ ] Follows the [developer guidelines](https://carvel.dev/shared/docs/latest/development_guidelines/) - [ ] Relevant tests are added or updated - [ ] Relevant docs in this repo added or updated - [ ] Relevant carvel.dev docs added or updated in a separate PR and there's   a link to that PR - [ ] Code is at least as readable and maintainable as it was before this   change  #### Additional documentation e.g., Proposal, usage docs, etc.:  ```docs  ``` ",2024-09-20T08:37:20+00:00,2024-09-26T04:37:30+00:00,0,https://github.com/carvel-dev/kapp/pull/1019,1019.0,,https://github.com/carvel-dev/kapp/pull/1019,0,3,0,3,13,4,0,17,140.00277777777777,,False,True,normal,ui,"[{""filename"": ""pkg/kapp/resourcesmisc/custom_waiting_resource.go"", ""lines_added"": 9, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/resourcesmisc/wait_rule_contract_v1.go"", ""lines_added"": 3, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/wait_timeout_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,929,kapp flip flops betweek gk scoping and non scoping while using `--dangerous-allow-empty-list-of-resources`,"**What steps did you take:** I have a Gitops system where we deploy resources from files using kapp. We just use `kapp deploy` to deploy and delete resources, and hence we have to use the `--dangerous-allow-empty-list-of-resources` flag when all files are removed. ```bash $ kapp deploy -a test -f examples/simple-app/config-1.yml . . . // Deployment and Service Created $ kapp deploy -a test -f empty-dir --dangerous-allow-empty-list-of-resources . . . // Deployment and Service deleted.  ```  **What happened:** After the above two commands are run, kapp removes the `usedGKs` list from the meta ConfigMap. Due to this, the next time kapp deploy is run, kapp doesn't use GK scoping because it thinks that this is an old app which didn't have gk scoping enabled.  **What did you expect:** I expected kapp to continue scope the listing to the GKs from the resources I am trying to deploy.   **Anything else you would like to add:**   **Environment:**  - kapp version (use `kapp --version`): - OS (e.g. from `/etc/os-release`): - Kubernetes version (use `kubectl version`)  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2024-04-16T12:44:18+00:00,2024-04-17T10:19:33+00:00,0,https://github.com/carvel-dev/kapp/issues/929,928.0,2024-04-17T10:19:32+00:00,https://github.com/carvel-dev/kapp/pull/928,0,2,0,2,25,5,0,30,21.587222222222223,bug;carvel accepted,False,True,major,configuration,"[{""filename"": ""pkg/kapp/app/recorded_app.go"", ""lines_added"": 6, ""lines_deleted"": 5, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/app_metadata_output_test.go"", ""lines_added"": 19, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",e2e;pkg,True
carvel-dev/kapp,451,#446: updated kapp timeout error. Will list resource's description fo…,"Updated kapp timeout error. Will list resource's description for which timeout is happening in error log.  Error will look like: ``` $ ./kapp delete -a app1                                             Target cluster 'https://127.0.0.1:56202' (nodes: minikube)  Changes  Namespace  Name                         Kind           Age  Op      Op st.  Wait to  Rs  Ri   default    simple-app                   Deployment     3s   delete  -       delete   ok  -   ^          simple-app                   Endpoints      3s   -       -       delete   ok  -   ^          simple-app                   Service        3s   delete  -       delete   ok  -   ^          simple-app-4prws             EndpointSlice  3s   -       -       delete   ok  -   ^          simple-app-64f85bbb96        ReplicaSet     3s   -       -       delete   ok  -   ^          simple-app-64f85bbb96-8g8q7  Pod            3s   -       -       delete   ok  -    Op:      0 create, 2 delete, 0 update, 4 noop, 0 exists Wait to: 0 reconcile, 6 delete, 0 noop  Continue? [yN]: y  9:48:40AM: ---- applying 6 changes [0/6 done] ---- 9:48:40AM: noop replicaset/simple-app-1-684f6ff85c (apps/v1) namespace: default 9:48:40AM: noop endpointslice/simple-app-1-qpp9h (discovery.k8s.io/v1) namespace: default 9:48:40AM: noop endpoints/simple-app-1 (v1) namespace: default 9:48:40AM: noop pod/simple-app-1-684f6ff85c-gq9vv (v1) namespace: default 9:48:40AM: delete deployment/simple-app-1 (apps/v1) namespace: default 9:48:40AM: delete service/simple-app-1 (v1) namespace: default 9:48:40AM: ---- waiting on 6 changes [0/6 done] ---- 9:48:40AM: ok: delete service/simple-app-1 (v1) namespace: default 9:48:40AM: ongoing: delete endpointslice/simple-app-1-qpp9h (discovery.k8s.io/v1) namespace: default 9:48:40AM: ongoing: delete pod/simple-app-1-684f6ff85c-gq9vv (v1) namespace: default 9:48:40AM: ok: delete endpoints/simple-app-1 (v1) namespace: default 9:48:40AM: ongoing: delete replicaset/simple-app-1-684f6ff85c (apps/v1) namespace: default 9:48:40AM: ok: delete deployment/simple-app-1 (apps/v1) namespace: default 9:48:40AM: ---- waiting on 3 changes [3/6 done] ----  kapp: Error: Timed out waiting after 1µs for resources:     - replicaset/simple-app-1-684f6ff85c (apps/v1) namespace: default    - endpointslice/simple-app-1-qpp9h (discovery.k8s.io/v1) namespace: default    - pod/simple-app-1-684f6ff85c-gq9vv (v1) namespace: default ```",2022-03-04T09:09:46+00:00,2022-03-10T21:20:54+00:00,1,https://github.com/carvel-dev/kapp/pull/451,451.0,2022-03-10T21:20:54+00:00,https://github.com/carvel-dev/kapp/pull/451,0,1,0,1,7,1,0,8,156.18555555555557,cla-not-required,False,True,normal,networking,"[{""filename"": ""pkg/kapp/clusterapply/waiting_changes.go"", ""lines_added"": 7, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",pkg,False
knative/eventing,8133,Default EventPolicy `.spec.from[].namespace` to EventPolicies namespace,Defaulting the `.spec.from[].namespace` to the event policy namespace as written in the docs  https://github.com/knative/eventing/blob/32f849105ca5c37454079607b91b11ba61e91662/pkg/apis/eventing/v1alpha1/eventpolicy_types.go#L122-L126,2024-08-07T11:58:54+00:00,2024-08-07T14:18:31+00:00,3,https://github.com/knative/eventing/pull/8133,8133.0,2024-08-07T14:18:31+00:00,https://github.com/knative/eventing/pull/8133,0,2,0,2,42,0,0,42,2.3269444444444445,kind/bug;size/M;approved;lgtm,False,True,normal,functional,"[{""filename"": ""pkg/apis/eventing/v1alpha1/eventpolicy_defaults.go"", ""lines_added"": 10, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""pkg/apis/eventing/v1alpha1/eventpolicy_defaults_test.go"", ""lines_added"": 32, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,755,`kapp app-group deploy` should order based on folder names,"**What steps did you take:** Deploy any app using `kapp app-group deploy`  **What happened:** App deployment order is randomized on every execution.  **What did you expect:** App deployment order should be alphabetically based on folder names.   **Anything else you would like to add:** When reading directory using `ioutil.ReadDir` the ordering is correct, but when assigning to the result map here: https://github.com/carvel-dev/kapp/blob/df3cfc8d74bf79d52e96f8d1c1296d6f553f048b/pkg/kapp/cmd/appgroup/deploy.go#L137 ordering gets randomized.   **Environment:**  - kapp version (use `kapp --version`): 0.56.0 - OS (e.g. from `/etc/os-release`): macOS Ventura - Kubernetes version (use `kubectl version`) 1.25.7  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2023-05-16T14:53:50+00:00,2023-11-04T10:52:56+00:00,4,https://github.com/carvel-dev/kapp/issues/755,829.0,,https://github.com/carvel-dev/kapp/pull/829,0,1,0,1,12,10,0,22,4123.985,bug;good first issue;carvel accepted,False,True,major,functional,"[{""filename"": ""pkg/kapp/cmd/appgroup/deploy.go"", ""lines_added"": 12, ""lines_deleted"": 10, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,791,`kapp deploy --diff-run` should not require updating the existing ConfigMap,"**What steps did you take:** - Executed `kapp deploy --app ""svc-kapp"" --diff-run -f kapp.yaml` for an _existing_ app `svc-kapp`.  **What happened:** - Error: ``` kapp: Error: Updating app: configmaps ""svc-kapp"" is forbidden:    User ""xxxxx"" cannot update resource ""configmaps"" in API group """" in the namespace ""svc"" ```  **What did you expect:** - A diff of the changes to be displayed with no errors and no further action.   **Anything else you would like to add:** - My primary use case for this feature is displaying configuration changes raised in a pull request or on a branch during which CI should have read-only permissions in the cluster.   **Environment:**  - kapp version (use `kapp --version`): v0.58.0 - OS (e.g. from `/etc/os-release`): Ubuntu 22.04 LTS (GitHub Actions) - Kubernetes version (use `kubectl version`): v1.24.15  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2023-07-27T16:21:25+00:00,2023-08-08T13:25:35+00:00,0,https://github.com/carvel-dev/kapp/issues/791,793.0,2023-08-08T13:25:34+00:00,https://github.com/carvel-dev/kapp/pull/793,0,2,0,2,99,3,0,102,285.06916666666666,bug;carvel accepted;priority/important-longterm,False,True,major,configuration,"[{""filename"": ""pkg/kapp/app/recorded_app.go"", ""lines_added"": 15, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/diff_test.go"", ""lines_added"": 84, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
kubeflow/spark-operator,2199,[BUG] Pod creation creation fails on submission with invalid resource quantities,"## Description  I've been scratching my head on this one for the past few days - without any resolution.  I am in the process of testing migrating the spark operator from `spark-operator-chart-1.4.6` to `v2.0.1` and have come across the following issues. It seems that submission fails at the point it tries to create a driver pod - with the following error around resource quantities:  ``` Failure executing: POST at: https://127.0.01:443/api/v1/namespaces/spark-operator/pods.       Message: Pod in version \\""v1\\"" cannot be handled as a Pod: quantities must match       the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$'. ```  Below is the full error log.  ``` status:   applicationState:     errorMessage: ""failed to run spark-submit: failed to run spark-submit: 24/09/27       14:55:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your       platform... using builtin-java classes where applicable\\n24/09/27 14:55:49 INFO       SparkKubernetesClientFactory: Auto-configuring K8S client using current context       from users K8S config file\\n24/09/27 14:55:50 INFO KerberosConfDriverFeatureStep:       You have not specified a krb5.conf file locally or via a ConfigMap. Make sure       that you have the krb5.conf locally on the driver image.\\n24/09/27 14:55:50       ERROR Client: Please check \\""kubectl auth can-i create pod\\"" first. It should       be yes.\\nException in thread \\""main\\"" io.fabric8.kubernetes.client.KubernetesClientException:       Failure executing: POST at: https://127.0.01:443/api/v1/namespaces/spark-operator/pods.       Message: Pod in version \\""v1\\"" cannot be handled as a Pod: quantities must match       the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$'. Received       status: Status(apiVersion=v1, code=400, details=null, kind=Status, message=Pod       in version \\""v1\\"" cannot be handled as a Pod: quantities must match the regular       expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', metadata=ListMeta(_continue=null,       remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}),       reason=BadRequest, status=Failure, additionalProperties={}).\\n\\tat io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:518)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:535)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:340)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:703)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:92)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1108)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:92)\\n\\tat       org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:153)\\n\\tat       org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6(KubernetesClientApplication.scala:256)\\n\\tat       org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6$adapted(KubernetesClientApplication.scala:250)\\n\\tat       org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:48)\\n\\tat       org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\\n\\tat       org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\\n\\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:250)\\n\\tat       org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:223)\\n\\tat       org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)\\n\\tat       org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\\n\\tat       org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\\n\\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\\n\\tat       org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\\n\\tat       org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\\n\\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\\nCaused       by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing:       POST at: https://127.0.0.1:443/api/v1/namespaces/spark-operator/pods.       Message: Pod in version \\""v1\\"" cannot be handled as a Pod: quantities must match       the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$'. Received       status: Status(apiVersion=v1, code=400, details=null, kind=Status, message=Pod       in version \\""v1\\"" cannot be handled as a Pod: quantities must match the regular       expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', metadata=ListMeta(_continue=null,       remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}),       reason=BadRequest, status=Failure, additionalProperties={}).\\n\\tat io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:671)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.requestFailure(OperationSupport.java:651)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.assertResponseCode(OperationSupport.java:600)\\n\\tat       io.fabric8.kubernetes.client.dsl.internal.OperationSupport.lambda$handleResponse$0(OperationSupport.java:560)\\n\\tat       java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(Unknown Source)\\n\\tat       java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown Source)\\n\\tat       java.base/java.util.concurrent.CompletableFuture.complete(Unknown Source)\\n\\tat       io.fabric8.kubernetes.client.http.StandardHttpClient.lambda$completeOrCancel$10(StandardHttpClient.java:140)\\n\\tat       java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)\\n\\tat       java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown       Source)\\n\\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown       Source)\\n\\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown       Source)\\n\\tat io.fabric8.kubernetes.client.http.ByteArrayBodyHandler.onBodyDone(ByteArrayBodyHandler.java:52)\\n\\tat       java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)\\n\\tat       java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(Unknown       Source)\\n\\tat java.base/java.util.concurrent.CompletableFuture.postComplete(Unknown       Source)\\n\\tat java.base/java.util.concurrent.CompletableFuture.complete(Unknown       Source)\\n\\tat io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$OkHttpAsyncBody.doConsume(OkHttpClientImpl.java:137)\\n\\tat       java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n\\tat       java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n\\tat       java.base/java.lang.Thread.run(Unknown Source)\\n24/09/27 14:55:50 INFO ShutdownHookManager:       Shutdown hook called\\n24/09/27 14:55:50 INFO ShutdownHookManager: Deleting directory       /tmp/spark-2fe1d114-2f30-44b5-9a62-89db1478492f\\n""     state: FAILED ```  First thing to note on this log line: `ERROR Client: Please check \\""kubectl auth can-i create pod\\"" first. It should be yes.` - the CR is using a serviceAccount that _does_ have the appropriate permissions to perform full CRUD operations to the `pods` resource - just to rule that out before anyone asks.  There is no change I made to the resource values compared to `spark-operator-chart-1.4.6` and `v2.0.1`. My driver & executor resource asks essentially look like this:  ```yaml driver:     cores: 2     coreLimit: 8124m     memory: 6123m  executor:     cores: 2     coreLimit: 8124m     memory: 4123m     instances: 2 ```  After enabling debug logs on the operator-controller, I can see that these values are correctly passed in and submitted as `--conf` arguments, but it fails directly after that.  This smells to me that it is an issue with `spark:3.5.1`.. But I am not entirely sure. I will post the full `SparkApplication` below for reference.  - [x] ✋ I have searched the open/closed issues and my issue is not listed.  ## Reproduction Code [Required]  ```yaml apiVersion: sparkoperator.k8s.io/v1beta2 kind: SparkApplication metadata:   name: cian-test   namespace: spark-operator spec:   driver:     annotations:       ad.datadoghq.com/spark-kubernetes-driver.check_names: '[""prometheus""]'       ad.datadoghq.com/spark-kubernetes-driver.init_configs: '[{}]'       ad.datadoghq.com/spark-kubernetes-driver.instances: ""\\n[\\n  {\\n    \\""prometheus_url\\"":         \\""http://%%host%%:8090/metrics\\"",\\n    \\""namespace\\"": \\""spark-operator\\"",\\n         \\   \\""metrics\\"": [\\""*\\""],\\n    \\""tags\\"": []\\n  }\\n]\\n        ""      cores: 2     coreLimit: 8124m     memory: 6123m     javaOptions: -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -Dlog4j.configuration=file:/opt/log4j.properties     nodeSelector:       node-type: node-ssd     podSecurityContext:       fsGroup: 185     serviceAccount: spark-operator     tolerations:     - effect: NoSchedule       key: compute/nodegroup       operator: Equal       value: node-ssd     volumeMounts:     - mountPath: /data/spark/temp       name: spark-data     - mountPath: /var/lib/containerd/spark       name: spark-local-dir-nvme   executor:     annotations:       ad.datadoghq.com/spark-kubernetes-executor.check_names: '[""prometheus""]'       ad.datadoghq.com/spark-kubernetes-executor.init_configs: '[{}]'       ad.datadoghq.com/spark-kubernetes-executor.instances: ""\\n[\\n  {\\n    \\""prometheus_url\\"":         \\""http://%%host%%:8090/metrics\\"",\\n    \\""namespace\\"": \\""spark-operator\\"",\\n         \\   \\""metrics\\"": [\\""*\\""],\\n    \\""tags\\"": []\\n  }\\n]\\n        ""     cores: 2     coreLimit: 8124m     memory: 4123m     instances: 2     javaOptions: -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -Dlog4j.configuration=file:/opt/log4j.properties     nodeSelector:       node-type: node-ssd     podSecurityContext:       fsGroup: 185     serviceAccount: spark-operator     tolerations:     - effect: NoSchedule       key: compute/nodegroup       operator: Equal       value: node-ssd     volumeMounts:     - mountPath: /data/spark/temp       name: spark-data     - mountPath: /var/lib/containerd/spark       name: spark-local-dir-nvme   image: my-custom-image:v1   mainApplicationFile: s3a://my-bucket/my-jar.jar   mainClass: com.myClass.Cian.Application   mode: cluster   monitoring:     exposeDriverMetrics: true     exposeExecutorMetrics: true     prometheus:       jmxExporterJar: /opt/spark/jars/jmx_prometheus_javaagent-0.11.0.jar       port: 8090   restartPolicy:     type: Never   sparkConf:     spark.decommission.enabled: ""true""     spark.dynamicAllocation.shuffleTracking.enabled: ""true""     spark.eventLog.dir: s3a://my-s3-bucket/logs     spark.eventLog.enabled: ""true""     # spark.kubernetes.memoryOverheadFactor: ""0.1""     spark.storage.decommission.enabled: ""true""     spark.storage.decommission.rddBlocks.enabled: ""true""     spark.storage.decommission.shuffleBlocks.enabled: ""true""   sparkUIOptions:     servicePort: 4040     servicePortName: spark-driver-ui-port     serviceType: """"   sparkVersion: 3.4.1   timeToLiveSeconds: 3600   type: Scala   volumes:   - name: api-token     projected:       defaultMode: 420       sources:       - serviceAccountToken:           expirationSeconds: 31536000           path: token   - name: spark-data     persistentVolumeClaim:       claimName: spark-operator-efs-pvc   - emptyDir: {}     name: spark-local-dir-nvme ```   ## Expected behavior  Driver & Executor pods should spin up and job should start.  ## Actual behavior  Job submission fails.  ### Terminal Output Screenshot(s)  <!-- Optional but helpful -->   ## Environment & Versions  - Spark Operator App version: `v2.0.1` - Helm Chart Version:  `v2.0.1` - Kubernetes Version: `v1.29.3` - Apache Spark version: `3.4.1`  ## Additional context  <!-- Add any other context about the problem here -->  cc: @ChenYi015 @jacobsalway  ",2024-09-27T15:23:40+00:00,2024-11-04T11:17:17+00:00,6,https://github.com/kubeflow/spark-operator/issues/2199,2305.0,2024-11-04T11:17:16+00:00,https://github.com/kubeflow/spark-operator/pull/2305,0,1,0,1,14,12,0,26,907.8933333333332,,False,True,normal,configuration,"[{""filename"": ""internal/controller/sparkapplication/submission.go"", ""lines_added"": 14, ""lines_deleted"": 12, ""file_type"": ""app_code""}]",controller,False
carvel-dev/kapp,646,App name cannot have more than 63 characters,"**What steps did you take:** Created an app with the app name having more than 63 characters.  **What happened:** App creation failed because the app-change ConfigMaps uses the app name as a label. ``` kapp: Error: Creating app change:   ConfigMap ""byoi-2213-photon-ns-antrea-11ns1-c1-guest-cluster-auth-serzfmpw"" is invalid: metadata.labels:     Invalid value: ""byoi-2213-photon-ns-antrea-11ns1-c1-guest-cluster-auth-service-ctrl"": must be no more than 63 characters ```  **What did you expect:** App to be created successfully.  **Anything else you would like to add:** This causes issues for App CRs and PackageInstall CRs from kapp-controller because a kapp app is created using the name of app/package install.   **Environment:** - kapp version (use `kapp --version`): kapp controller version is `v0.41.2` - OS (e.g. from `/etc/os-release`): `Photon` - Kubernetes version (use `kubectl version`): `1.22.13`  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2022-11-29T04:29:57+00:00,2023-03-06T05:23:03+00:00,1,https://github.com/carvel-dev/kapp/issues/646,710.0,2023-03-06T05:23:02+00:00,https://github.com/carvel-dev/kapp/pull/710,0,2,0,2,55,26,0,81,2328.884722222222,bug;carvel accepted;priority/important-soon,False,True,major,configuration,"[{""filename"": ""pkg/kapp/app/recorded_app.go"", ""lines_added"": 18, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/app/recorded_app_changes.go"", ""lines_added"": 37, ""lines_deleted"": 18, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,644,Failure in `kapp delete` results in creation of app-change ConfigMaps with no limit on the number of app-changes,"**What steps did you take:** - Deploy an app - Delete the app (with no permission to delete ConfigMap, to simulate a failure)  Notice that the resources are deleted but the app metadata ConfigMap still exists and there is an additional app-change ConfigMap for the last change.  - Try to delete the app again   Notice that there is another app-change CM present now.  **What happened:** `kapp delete` creates app change ConfigMaps which are then deleted, but in case the app deletion fails for some reason, these ConfigMaps remain on the cluster. If there is a controller or a script which tries to delete the app at certain intervals then the number of CMs goes onto increase without any limit.  **What did you expect:** - App change ConfigMaps should not be created for delete command in the first place (this is debatable) - In case we are creating the CMs for delete command, there should be garbage collection happening in delete just like deploy command.  **Environment:** - kapp version (use `kapp --version`): 0.54.0 - OS (e.g. from `/etc/os-release`): darwin - Kubernetes version (use `kubectl version`) 1.25.0  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2022-11-25T08:59:22+00:00,2023-02-28T05:21:12+00:00,1,https://github.com/carvel-dev/kapp/issues/644,704.0,2023-02-28T05:21:11+00:00,https://github.com/carvel-dev/kapp/pull/704,0,1,0,1,15,0,0,15,2276.363611111111,bug;carvel accepted;priority/important-soon,False,True,major,configuration,"[{""filename"": ""pkg/kapp/cmd/app/delete.go"", ""lines_added"": 15, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,486,Add recalculated diff to conflicting update error,"Add masked recalculated diff to conflicting update error message when there's a mismatch in the original diff and new diff.  Sample error message ```bash kapp: Error: Applying update secret/mysecret (v1) namespace: default:   Failed to update due to resource conflict (approved diff no longer matches):     Updating resource secret/mysecret (v1) namespace: default:       API server says:         Operation cannot be fulfilled on secrets ""mysecret"": the object has been modified; please apply your changes to the latest version and try again (reason: Conflict):           Recalculated diff:   3,  3 -   password2: <-- value not shown (#1)   4,  3 +   user: <-- value not shown (#2) ```",2022-04-14T12:57:57+00:00,2022-04-21T15:41:34+00:00,1,https://github.com/carvel-dev/kapp/pull/486,486.0,2022-04-21T15:41:33+00:00,https://github.com/carvel-dev/kapp/pull/486,0,6,0,6,36,9,0,45,170.72666666666666,cla-not-required,False,True,normal,functional,"[{""filename"": ""pkg/kapp/clusterapply/add_or_update_change.go"", ""lines_added"": 9, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change.go"", ""lines_added"": 7, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change_factory.go"", ""lines_added"": 6, ""lines_deleted"": 3, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/cmd/app/delete.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/cmd/app/deploy.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/update_retry_on_conflict_test.go"", ""lines_added"": 12, ""lines_deleted"": 0, ""file_type"": ""app_code""}]",pkg,False
carvel-dev/kapp,426,Don't exit at first error but continues and summarizes all the found ones at the end of execution,"**Describe the problem/challenge you have** We want to use kapp to reconcile our e2e tests. We're trying to deploy multiple CRs that don't depend on each other, they can be deployed in parallel. There's no dependency between CRs, so if any of those fails it should not stop the other ones from being reconciled. Currently the kapp command is exiting as soon as it finds an error, with a message like this one:  ``` kapp: Error: waiting on reconcile customresource/resource-name namespace: default:   Finished unsuccessfully (Encountered failure condition Succeeded == False: Error (message: The job did not complete successfully)) ``` So we're not getting feedback about the success/failure of all the other CRs that were trying to get reconciled.  **Describe the solution you'd like** We would like a new CLI argument (e.g.: --ignore-errors or --no-abort) or a new configuration field to indicate to the kapp CLI to wait for all the possible results before exiting, indicating which ones failed in which steps of the reconciliation process.   Output would look like: ""Reconciled 10/20. Finished unsuccessfully: 5. Finished successfully: 5. Not deployed: 10.""  **Anything else you would like to add:** The configuration file we're using to replicate this issue looks something like this:  ``` apiVersion: kapp.k14s.io/v1alpha1 kind: Config  waitRules:   - supportsObservedGeneration: true     conditionMatchers:       - type: Succeeded         status: ""True""         success: true       - type: Succeeded         status: ""False""         failure: true     resourceMatchers:       - andMatcher:           matchers:             - apiVersionKindMatcher: {apiVersion: my-custom-resource-definition.com/v1beta1, kind: CustomResource}             - hasAnnotationMatcher: { keys: [kapp-matcher-success] }   - supportsObservedGeneration: true     conditionMatchers:       - type: Succeeded         status: ""False""         success: true       - type: Succeeded         status: ""True""         failure: true     resourceMatchers:       - andMatcher:           matchers:             - apiVersionKindMatcher: {apiVersion: my-custom-resource-definition.com/v1beta1, kind: CustomResource}             - hasAnnotationMatcher: { keys: [kapp-matcher-failure] } ```  --- Vote on this request  This is an invitation to the community to vote on issues, to help us prioritize our backlog. Use the ""smiley face"" up to the right of this comment to vote.  👍 ""I would like to see this addressed as soon as possible"" 👎 ""There are other more important things to focus on right now""  We are also happy to receive and review Pull Requests if you want to help working on this issue. ",2022-01-27T16:36:40+00:00,2022-09-19T07:54:56+00:00,5,https://github.com/carvel-dev/kapp/issues/426,532.0,2022-09-19T07:54:55+00:00,https://github.com/carvel-dev/kapp/pull/532,0,7,0,7,317,23,0,340,5631.304166666667,enhancement;discussion;carvel accepted;priority/important-soon,False,True,major,configuration,"[{""filename"": ""pkg/kapp/clusterapply/applying_changes.go"", ""lines_added"": 16, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change_set.go"", ""lines_added"": 23, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/waiting_changes.go"", ""lines_added"": 19, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/cmd/app/apply_flags.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/apply_wait_error_test.go"", ""lines_added"": 253, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/formatted_error_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",e2e;pkg,True
carvel-dev/kapp,532,Apply as many changes as possible with error summary at end,"<!--  Thanks for sending a pull request!  Here are some tips for you:  If this is your first time, please read our contributor guidelines: https://github.com/vmware-tanzu/carvel-kapp/blob/develop/CONTRIBUTING.md and developer guide https://github.com/vmware-tanzu/carvel-kapp/blob/develop/docs/dev.md -->  #### What this PR does / why we need it: Currently kapp errors out as soon as it detects an error while apply a resource or waiting for a resource, but we would want to apply as many changes as possible and return all the errors at end. If a change doesn't depend on a failed change then it should get applied.  - Changes that depend on failing changes shouldn't be applied - `exit-early-on-apply-error` flag can be used to exit as soon as an error is encountered while applying changes (default true) - `exit-early-on-wait-error` flag can be used to exit as soon as an error is encountered while waiting for changes (default true)  Sample gist for testing: https://gist.github.com/praveenrewar/2746318c5a273d13ed26497f34755638  TODO: - [x] Add e2e test - [x] Do more manual testing  #### Which issue(s) this PR fixes: <!-- If no issue exists for this change, please create an issue and link it here. --> Fixes #426   #### Does this PR introduce a user-facing change? <!-- If no, just write ""NONE"" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below.   --> ```release-note  ```  #### Additional Notes for your reviewer:  ##### Review Checklist:  - [x] Follows the [developer guidelines](https://carvel.dev/shared/docs/latest/development_guidelines/) - [x] Relevant tests are added or updated - [ ] Relevant docs in this repo added or updated - [ ] Relevant carvel.dev docs added or updated in a separate PR and there's   a link to that PR - [ ] Code is at least as readable and maintainable as it was before this   change  #### Additional documentation e.g., Proposal, usage docs, etc.:  ```docs  ``` ",2022-06-18T10:41:41+00:00,2022-09-19T07:54:55+00:00,13,https://github.com/carvel-dev/kapp/pull/532,532.0,2022-09-19T07:54:55+00:00,https://github.com/carvel-dev/kapp/pull/532,0,7,0,7,317,23,0,340,2229.220555555556,cla-not-required,False,True,normal,ui,"[{""filename"": ""pkg/kapp/clusterapply/applying_changes.go"", ""lines_added"": 16, ""lines_deleted"": 9, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/cluster_change_set.go"", ""lines_added"": 23, ""lines_deleted"": 4, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/clusterapply/waiting_changes.go"", ""lines_added"": 19, ""lines_deleted"": 8, ""file_type"": ""app_code""}, {""filename"": ""pkg/kapp/cmd/app/apply_flags.go"", ""lines_added"": 4, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/apply_wait_error_test.go"", ""lines_added"": 253, ""lines_deleted"": 0, ""file_type"": ""app_code""}, {""filename"": ""test/e2e/formatted_error_test.go"", ""lines_added"": 1, ""lines_deleted"": 1, ""file_type"": ""app_code""}]",e2e;pkg,True
